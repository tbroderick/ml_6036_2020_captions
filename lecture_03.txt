Okay good morning and welcome to lecture three. So just a reminder as usual, make sure to put all of your questions up at the discourse, we have our “lecture 3” category there for today and so you can start that just immediately. So recall that in the past couple of lectures, in addition to the labs and everything else you've been working on, we've been covering things like linear classifiers, understanding what is a linear classifier, and some algorithms to come up with linear classifiers. So in particular, at this point, we now have the perceptron algorithm. We saw that when our data was linearly separable, there were super cool things that we could do with the perceptron algorithm. In fact, we had this theorem that gave us a bound on, you know, how many mistakes or sort of how many updates we would make before we were done and we found this sort of perfect linear separator. And so today now that we have, sort of, some algorithms and the background to be able to talk about it, we're gonna put together a sort of more complete machine learning analysis. So we're going to think about starting from the very beginning, gathering data, and going to interpretation and evaluation and things like that and so as part of that we're going to talk about choosing good features. Now this won't be an absolutely complete discussion—so I say more complete rather than fully complete—but I think we're going to be going in the right direction towards, you know, you really being able to have a collection of data and run your machine learning algorithm on that data. So we'll talk about all of that today.

Okay so let's just briefly recap. So remember as we said, we've been talking about these linear classifiers, we've been calling them h, for instance, and so in general we have some features,  maybe x_1, x_2, x_3, x_4. Remember that we're only drawing things in two dimensions because of our fundamental limitations and vision as humans, the reality is our feature space is definitely, in general, going to be higher than two-dimensional and we'll even talk about that today but our visualization here is in two dimensions. And so in general we've been seeing that we find these linear classifiers, they're defined by a theta which is giving us the normal to the hyperplane, that's doing the classification. It tells us on what side we predict minus and on what side we predict a plus. Sort of on one side, we predict minus plus one and on one side we predict -1, and we have our theta naught which is our offset and again in general this will be a hyperplane not just a line. Now when we want to ask, you know, how well is this linear classifier doing on some data, we have to talk about some kind of loss and classification. We've been focusing on the 0-1 loss although, again, that's not the only loss that you would use in classification, we talked about the asymmetric loss. Here we're saying that if our guess is equal to the actual value then we get a loss of 0, which is the best, and if we're wrong then we get a loss of one. And so we talked about how we can look at training error which tells us sort of the average loss over our training data points as one way to look at how well our classifier is doing and actually it's something we've been using in our classification algorithm. Okay so this is sort of, you know, a bunch of things that we have in our toolkit at this point and so let's ask ourselves: what would a machine learning analysis look like at this point? Could you run a machine learning analysis on some data? Well first you kind of have to establish a goal and find some data and for that matter check that that goal is in fact something that you could accomplish with your machine learning algorithm. So we'll see an example of that in a moment:  we're gonna have sort of a cartoon where we consider diagnosing whether people have heart disease based on some available information about those people. I will emphasize this is very much a cartoon: if you really care about heart disease then I encourage you to check out like a paper about heart disease, there's actually some cool data sets out there that you can play with. But I still think this will get us a lot of insight into this process. Okay, so once we have some data, it will often be the case as we're going to see that you can't just immediately apply the machine learning algorithms that we've established, that you have to do some transformations of that data to put it into a useful form, and so we're going to see that process, a lot of that process, today. Once you've done that, now you can run that machine learning algorithm. So it could be, for instance, one of the ones that we've talked about before: we talked about choosing the best classifier from some finite list of classifiers, we've talked about using the perceptron. Now you in your explorations and homeworks and labs and such have also encountered the average perceptron, so you have a lot of machine learning algorithms in your toolkit and you're going to have more coming up. We're going to see that there are, you know, even cooler things that we can do, but you're going to run some machine learning algorithm at this point and return a classifier and an important thing that we want to do is also interpret and evaluate what you get out at the end, so we'll talk about that a little bit today as well. And it's also something that you've been exploring again in labs and other exercises. Okay so let's launch into this analysis again on this example that we're going for with heart disease. So in particular, we need to start off with our goal and our data and and it's worth noting that these are very intertwined in some way, you know, typically you'd want to gather data with some question in mind being able to, you know, answer that question but also gathering data is very difficult and expensive. Probably that's most of the work that goes into, you know, any of these steps, so you might find that you have some data that's available to you and you can ask a question of that data. So to make this concrete, again we're going to talk about diagnosing whether people have heart disease based on some information available about those individuals and, you know, let's imagine that thankfully there's actually a bunch of data already available to us. So somebody has measured, you know, do a bunch of individuals have heart disease? So here we're looking at four, data about four individuals, but we imagine that our data set is actually much bigger and it's just hard for me to fit on a slide, so we're going to look at the first four people. Okay and this person has also measured or this, you know, group of people has measured the resting heart rate of these individuals in beats per minute, they've measured if they've been experiencing pain in some time period, maybe the last month. Maybe we're looking at individuals who work at a hospital and so we could ask what's their job within the hospital, maybe that has some bearing on this diagnosis, we can look at what medicines they're taking, we can look at their age, maybe just roughly which decade they're in, and maybe the income of their entire family, not just their own personal income. Okay so let's say we've gathered this information and we would like to predict do these people have heart disease and so, you know, well one: we want to check is this something we could even turn into a two-class classification problem, right? And so this seems like something that has two labels: people have a heart, have heart disease, or they do not, so that that sounds good. We need labels: well we seem to have them, it seems like somebody has labeled this data at least for us. We need features: that's going to be basically everything except for the labels here. We need training data: basically labeled data with features. That's what we have here. That's great and we'd eventually like to apply that to new predictions. So on the face of it, it looks like this is something that we should be able to apply the learning algorithms that we've developed, these two class classification learning algorithms, and now let's ask ourselves: can we just put this into our learning algorithms? Well, not quite yet, right. So we need to identify a y, we need to identify an x, and that x is something we need to be able to take dot products with. And so you'll notice that some of the entries here are strings, like they're not even numbers, and so we couldn't possibly take a dot product yet, and that's what our algorithms so far have required, and so let's ask ourselves, you know, can we break this down? Can we put it into a form that we actually could even apply our learning algorithms? Okay, so we'll notice again that there's already this breakdown into labels and features, but let's make that explicit. And so that'll be our first order of business, let's first isolate our labels and make sure that we can put those into our algorithm. Okay so we want to encode this data in a usable form by the algorithms that we have, and we've sort of already talked about this with labels, but let's just make this explicit. We want to identify them and encode them as real numbers and so, in particular, the way that we've been doing that so far as we've been saying: we'd like to turn this into +1s and -1s, and so we might say that, let's say here, that yes is going to be +1 and no is going to be -1 and so we can turn this set of labels now into +1s and -1s. And so in particular here, the label for our first data point which we've been calling y^(1) will be -1 in this particular case. Now a couple of notes about this: one there's no reason that you have to use +1 and -1. In fact, we're gonna see pretty soon some algorithms that actually use a different set of two labels: 0 and 1. The reason that we've chosen +1 and -1 so far is that it's been very convenient,  it's been a really convenient choice of two labels, but we're gonna see that, again, that can change, but for today we're gonna go, we're gonna keep going, with +1 and -1 because that works for the algorithms that we have so far. And two, you definitely wanna save this mapping,  right. So you have chosen that +1 represents yes here and -1 represents no, and so you wanna make sure to save that somehow, so that when you get out predictions in the future and you say: “ah this person is -1”, well does that mean no or yes? So definitely make sure to save that mapping, so you can go back and find out, you know, what was the actual string that that corresponded to? Okay so now we've got our labels, we've got our y's and so we want to go back and get those features. Okay so now, let's do the same thing but we're going to identify the features rather than the labels and encode them as real numbers. Now just to be explicit here, a feature just means any function of the data and typically if we're doing something like supervised learning, which we are in this case (remember classification is a form of supervised learning), in that case, we typically exclude the labels from that. So it's a function of the data, except for the labels, and I mean that as there is a bunch of data that is not the labels and it can be any function of what that is. In fact, it can just be the data itself, that's a perfectly fine set of features: that would just be the identity mapping from the data to the data, but we're going to look at some other functions today. And in particular today, what we're going to do, is we're going to start with some data, let's call it x. You could think of those as our old features, sort of the original features. In some sense, like it's worth noticing that those were feature engineered to begin with, somebody chose what to measure about these patients, somebody chose what exactly was going to be in this table, and so it's not like, you know, those are pure and perfect in every way, like those were choices as well, but we're going to make some further choices and turn them into new features: let's call them phi and sometimes in this lecture I'll be sort of, just for convenience, not writing phi as a function of x, I'll just write phi, but it's worth noticing that we're just getting these new features from our old features. Okay so let's look at our data and see why are we even bothering to do this? Why don't we just keep our data exactly as it is? Okay and one thing I just want to note before we go on is that I'm slightly changing the direction of what we're looking at. So typically, we've been thinking of each data point as a column vector and so when you look at a data point here it's like I've transposed that, so just in the notation that we've been using for this class and the setup we've been using for this class, notice that a data point here I'm showing as a row even though we've been writing it as a column vector. Okay, so let's look at this first dimension: our first of our old features, the resting heart rate. Well, that's already a number, that's something that we could take things like dot products with and so maybe we don't have to worry about this one at least for the moment. Let's go to another feature, you know, have these individuals been experiencing pain, and notice: hey this is not a number, we can't take a dot product with the string, “no” or “yes”, and so we have to turn this into some kind of number. Well in this case, we kind of already have an idea of how we might do this based on the labels. Remember with the labels we just turn things into +1 or -1, or 0 or 1, and we can do the same thing here. So let's maybe make the no's into 0s and the yeses into 1s. Great and then of course again remember we definitely need to remember the mapping back if we want to interpret this in the end. Okay, so now we're getting into something a little bit trickier: let's look at jobs. So with jobs there are a bunch of different jobs, you know, and we're imagining here for the moment that these are jobs within the hospital, so we might be looking at things like nurse, administrative assistant, doctor, pharmacist and so on. So we're gonna explore some ideas for how we might turn this into numbers and I just want to note a general principle of our lecture is that when I put out an idea it's not necessarily a good idea. You know, sometimes we're going to put out ideas and we're going to ask ourselves about whether they're good ideas. I mean I think in reality when you go out into the world, you're going to have some ideas, some of them will be good, some of them will be bad, but more important is your ability to evaluate those ideas and so let's put out some ideas right now. So here's an idea: let's turn each category into a number, that seems really natural right. So I'm going to say that nurse is 1, doctor is 2, administrative assistant is 3, pharmacist is 4, and social worker is 5, and let's say those are, you know, the set of jobs that we see. And now, just for illustrative purposes let's imagine that, you know, let's just plot our data points with both occupation and resting heart rate. And so maybe we find in general that people with a higher resting heart rate, maybe we see, you tend to see, them as having heart disease and people with a lower resting heart rate maybe we don't see as much heart disease. And we see that, you know, things really maybe do change by occupation and we'd like to pick that up with our classifier, that you have some predictive power in occupation, that it can help you predict. Okay well here's an issue: if I use a linear classifier, it's pretty clear that the ordering matters, you know, so I'm assuming pretty explicitly here that nurse has a lower chance or a higher cut off for having heart disease than doctor does, and doctor has a higher cutoff for having heart disease than admin does, and admin has a higher cutoff for having heart disease than a pharmacist does. Or alternatively, the other way around, but that it's monotonic, that it's either all increasing across these numbers or all decreasing across these numbers and going in I just don't think there's really an ordering on nurse, doctor, admin, pharmacist, social worker. I don't think that that has some particular linear relationship that I want to codify and I'm forcing them to have that by making this codification into the natural numbers. An extreme version of this is, suppose that in fact my classification really just came down to which occupation people have: it turns out that some occupations, you know, are just super predictive of heart disease and some are not predictive and I would like to codify that and here I can only do a breakpoint between two numbers that are next to each other and so I'm allowed to group nurse, doctor, and admin but I'm not allowed to group nurse and admin without doctor. And because these are categories that don't inherently have an ordering, that's just not something that I want to do. Okay I think I have a question: what was, can I take that now? Yeah a question about mapping the data to features: does this mapping have to be invertible? Yeah great question. So it doesn't have to be invertible, you could make some function of your data that you plan to learn on and in some sense, you see that in the original data. Right so let's say that I, you know, I think about these individuals. Clearly I am not putting all of their information into my data, like there's some information about them, it could even be super important information for this problem, that I'm not encoding in the original data, you know, it could be the amount of plaque in their heart that would be really great probably if I'm looking at, you know, some kind of heart disease or some kind of issue, but the reality is that's very expensive or difficult or impossible to measure and so that's a part of that individual's data in some sense, that's information about them, but it's not encoded in the features. Now, that being said, it's important to understand the encoding that you're using so that you can interpret it, so when you have something like a one-to-one mapping, it's really easy to do that interpretation because you can go back and say: oh what string did this mean, what did this mean? If you don't do a one-to-one mapping, if you do some kind of function that says, oh all of these values get the same value, you just want to know what were all of the input values that gave you that output value so you can interpret it later.

Okay cool, so at this point we understand that we probably don't want to turn our categories into unique natural numbers. It seems like that's basically a bad idea when there's no ordering because we're basically implicitly assuming that this order matters and so let's think about what are other things that we could do in this case.

Why don't we turn each category into a binary number? Again, let's just throw out some ideas it's a, you know, let's brainstorm. Okay well first of all let's think about what do we mean by turning each category into a binary number? I mean in some sense, you know, we can just encode every natural number as a binary number so there's no difference in what we just did. I think what we really mean here is that we want to introduce actually, instead of one new feature, three new features, and these three new features essentially end up being binary. So we'll say that a nurse has encoding 0, 0, 0. Let's just call our new features phi_d, phi_(d +1) and phi_(d + 2). So the nurse will be 0 in all of them, the admin will be 0, 0, 1, the pharmacist will be 0, 1, 0, and so on. And so let's think about, you know, what happens when we use this encoding? Well let's again, let's try plotting. I think it's just one of the best things you can possibly do with your data and so let's do that here. So what are we looking at here? We're seeing that, we're looking in particular, the two end features. We're not looking actually at all the features but because we can only plot two, so let's plot two of them the end features phi_(d +1) and phi_(d + 2) and so we see here that 0, 0 which is the nurse or the social worker has a minus label. 0, 1 which is the admin has a minus label. 1, 0 which is the pharmacist has a plus label and 1, 1, the doctor has a plus label. And so this is nice, this is something where we see we can get a division between, in this case, the pharmacist and the doctor and the nurse and the admin and that's by having a cut, having a linear classifier and essentially that second feature. Okay but something I could totally imagine, it's totally plausible, is that maybe 0, 0, the nurse say, and 1,1, the doctor, maybe they have a lot in common, a lot of predictive power in common, maybe they tend to suffer heart disease more. At least this seems plausible a priori before we've learned anything, it's something that we want to say is possible. And it's possible that the admin and the pharmacists maybe don't have heart disease and so the issue here is that, well as you've seen in your reading for this week, we just can't have a linear classifier that classifies this correctly. And this doesn't seem that hard. Again it seems like if we just have a few categories, we want to be able to say that some of the categories go together and some don't and the problem with binary encoding is that we're enforcing those divisions again. You are allowed to do a break on the first feature, that means you're allowed to group nurse, admin, pharmacist, and doctor separately from social worker. You are allowed to do a break on the second feature: you are allowed to group pharmacist and doctor away from everybody else. You're allowed to do a break on the third feature: you're allowed to group admin and doctor together and nobody else, but those aren't the only breaks that we want, we want other breaks, we want to be able do, we want to be able to group any, you know, few of these jobs together and not group the other jobs with that. And so how could we get the ability to put together arbitrary groups of jobs? That's what we want in our encoding here. And so here is yet another idea that perhaps will finally let us do this, you know, how could we possibly do this. We essentially want to say for each job we can either put it in the group or out of the group and so that's the idea of giving each category its own feature essentially. So we're going to say there is a feature that is “are you a nurse” and so the nurse has that feature and they don't have the other features: they have a 1 and a 0 for the rest. There is a feature that is “are you an admin”, “are you an administrative assistant,” so the admin has a 1 for that feature and a 0 for the rest. There is a feature that says “are you a pharmacist,” so the pharmacist has a 1 for that feature and a 0 for the rest and so on. And so what's great about this is, let's again, let's look at plotting this. So if I plot any one of these features, well if I have any individual there'll be a 1 in that feature and 0 and everything else and if I plop it against another feature, well any other individual job, we'll have a 1 in that feature and a 0 everywhere else and so we're always going to be able to find a nice dividing line between these.

Okay so this has a special name: it's called “one-hot encoding.” The reason for this is that you can think of the 1 (and there's only one 1) as being “on,” as being “hot.” So there's one bit that's “hot,” that's “on,” and all the other bits are “cold”, they’re 0. And so this is really the standard for categorical class data, when we're doing some kind of feature encoding and we want to encode our categorical data. And hopefully at this point you sort of see why it's the standard at least relative to the other ideas that we've explored because we're assuming this ordering, if we use just a single number, you know, natural number for each one of these, we are assuming a grouping. If we assume the binary coding and here we don't have that, we have the ability to sort of break at any point that we want for categorical data. Okay so now what we can do is we can take our jobs, remember there were five of them, and so what we're going to do is we're going to replace that original set of job labels that were all strings with five new features, maybe we can call them jobs 1 through jobs 5 or j1 through j5 and we're going to put our 0s and 1s in there.

Okay so now we have this nice breakdown for jobs, let's go to medicines. Hey this looks like what we just did in some sense, right? I mean there's like four different types of medicines, you could be taking pain medicines, you could be taking beta blockers which turn out to be this type of medicine that I believe helps with slowing down your heart and might be useful in heart disease, you could be taking only beta blockers, you could be taking no medicines and so that's a set of categories, right? That's four categories and so should we do the same thing that we just did? Actually this is a question that I'll ask you: please respond in the private chat. Should should we just encode these in this nice one hot encoding that we just saw? Would this be four categories for the one hot encoding?

Okay I'm seeing a lot of no's and I'm seeing some people say no because they have overlap, that the categories aren't exclusive. Exactly. Okay so let's explore that idea.

Okay so here's what would happen if we did one hot encoding, should we use one hot encoding here. Well we would make our four categories or four new features and we'd have a feature for each category, one for pain, one for pain and beta blockers, one for beta blockers, one for no medications. Okay but as many of you are observing, there's something a little bit unnatural about this because, in some sense, what's really happening here is there are two features and we have just squished them together, right? One feature is “are you taking a pain medication” and one feature is “are you taking a beta blocker medication” and yes we can squish them together like we could squish any two features together. Like you could say, “hey my new feature is resting heart rate times income” but maybe I just don't think that that's a very useful feature, you know, maybe I think it's more useful to ask “what am I going to do with resting heart rate, or how does that affect my prediction, how does income affect my prediction?” And same thing here: you can think about this in terms of interpretability, you know, at the end of the day I'm going to ask well what was really predictive and how is it predictive and it might be more useful for me to really look at was pain medication predictive, was beta blocker medication predictive, rather than each individual combination. So here's an idea: let's use factored encoding and all that is saying exactly what we just said, I think a very common sense kind of thing, which is let's break this into two features. So let's say there is a feature which is “are you taking pain medication,” there is a feature which is “are you taking this beta blocker medication” and that's it, that's all that these represent. Now here, one of the benefits of this is also something that I think is a little bit less apparent from this small dimensional problem but if you could imagine that if we had like a lot more medications, you're going to get this combinatorial explosion of combinations of medications, you know, you're just getting a huge number if you can have each one on and off and you had k medications you could have 2^k combinations of medications, whereas you're just going to have k features of “are you taking a medication or not” and so that's going to be a lot simpler to deal with and potentially better for algorithms that perform poorly in really high dimensions. You're going to see here, you know, we have this a similar thing to before. If I have some effect that really changes based on having pain medication or not having pain medication, I have the ability to make a linear divisor there. If I have some effect that really, you know, or something that's very predictive when I have beta blockers versus not beta blockers then I'm able to make a linear separator here. Something that's worth noting is that this looks a lot like binary encoding. Right, I mean technically it is a binary encoding: we have a 0, 0, we have a 0, 1, we have a 1, 0, we have a 1, 1, but it's not quite binary encoding in the sense that we didn't just use any binary encoding. We were very careful to say that the 1s represented having a medication and the 0s represented not having it. If we weren't careful about that, we could get some weird situation where again, even though pain medication was the predictive thing, it could just be weird in the 0 and 1s and you get this issue where you can't do a linear separation and so we want to be again careful that we're not just saying oh let's do a binary encoding, it's really what that means here, what are the 0s and ones mean, whether I'm taking the medication or not. It's also worth noting, you know, there are a lot of tasks in life and just because one approach is good for one task doesn't mean it's good for all the tasks. And so binary encoding is great when you care about compression of information and that is just not our task here, we are not trying to compress the information that we have in some super efficient way, we're trying to make it easy for our algorithm to learn the things that are in the data and we're trying to make it easy for ourselves to interpret what went on and those could overlap with compression but they don't necessarily need to and it's always better to start from the goal that you have rather than some other goal.

Okay so here we have our set of medicines and so now we've seen that it would be useful to break this down into a feature that encodes “do I have this particular medicine,” let's say pain medication in my, is this individual taking this pain medication, and that'll be a 1 or a 0, and is this individual taking a beta blocker and that'll be a 0 or a 1. Now I also want to mention that this factoring, which really just means sort of breaking off into into, you know, sort of sub features as it were, doesn't have to be into 0s and 1s, and there's an application in the reading or an example of this in the reading where you can see that in fact there are different levels of the factoring and you'll see more examples of this I think as as we go forward in like the labs and the homeworks and so on and so just keep that in mind. I believe there's another question, yes. Yes the question is: if we have a one hot encoding with k labels or k categories can you use k -1 features with one label represented by all 0s? Yeah absolutely so this is totally true and this is a great observation that, you know, once, you know, something doesn't belong to the other categories then clearly it, you know, doesn't belong to the other categories. That could be something that you pick out. It is often useful though, to still say that that other category is explicitly represented by a feature. One for symmetry, because there's nothing special usually about that last category and so you might not want to treat it asymmetrically. Two, for interpretability because what we're going to do at the end of the day is look at our thetas and see, you know, which what things are predictive for what and, you know, what goes into what and so it would be nice to have a theta that corresponds to that category. And this kind of relates again to this discussion of, you know, do we care about being efficient versus do we care about making our predictions, making our algorithms run well, and making our predictions interpretable and things like that. And so that's typically the kind of trade-off that you're facing and so this might be one set of reasons why you would perhaps encode every category. Cool. By the way I should say too, you know, there's some sense in which the real answer to all of these questions is: it depends on what you're doing, and so every time you come across a new data analysis, I think it's worth asking yourself: how are things going to perform differently based on the choices that I make and it is totally possible that there are, and in fact it's probably almost always certainly true, that if we come up with some rule that there's going to be some data analysis where you want to do something differently just because that's a different type of data analysis and so just every choice that you make in a data analysis it's worth: asking how does that affect what we're doing?

Okay cool let's go to age. So here in age, we see that we have a few different decades represented. Like maybe age wasn't actually recorded very specifically, we just got decades maybe for privacy reasons. So people said, you know, I'm in my 40s or I'm in my 20s or I'm in my 50s in response to some kind of survey and so there's some interesting choices that we can make here. So one, and certainly a choice that you would commonly see in practice, is to use some representative h like maybe, you know, 45 for the 40s, 25 for the 20s, 55 for the 50s, and so on. I've also seen things like 15 for the middle of a month if you don't know what day something is in the month, you know, some kind of representative number. It's just worth noting that there are some potential pitfalls to this. One is that whenever you introduce a level of detail, it can be treated as meaningful by you or by others who use the data and so, you know, just as you should comment your code for the future people who are going to be reading your code, it's good to think about your data engineering for future people who might be using your engine, your data, the features that you make down the road. You might be using somebody else's features that they made and so you might want to ask what's going on there. So for instance, you know, if you saw that a lot of things were happening on the 15th of the month, then you might think there's something really important about the 15th and it turns out that was just the day people put in when they didn't know what day of the month something was happening. There's actually a really extreme version of this that I think is just really fascinating. So there's this just like fantastic piece of tech journalism which you, if you've never read it, I strongly recommend reading it. Basically so Kashmir Hill is this amazing tech journalist and she dug into this really weird thing that was going on, where a family and their renters were getting a huge amount of harassment, so they were getting all these sort of horrible things happening to them, they were getting visited by FBI agents, federal marshals, IRS collectors, ambulances, police officers, like you name it, weird people just lurking around their place and it was happening for a decade and they had no idea what was happening. And so it turned out that there is a tech company that provides physical location information for IP addresses and they often can't do it precisely, like often they can map an IP address to a physical location, but sometimes they can't. And so when they can't, they have default locations at the city level and if they don't know where it is at the city level, they put it at the state level, if they don't know where it is at the state level they put it at some default location at the country level. Well these people lived at the default location at the country level and that just happens to be in Kansas and so everybody where they didn't know where that IP address mapped, suddenly it mapped to these people and so it was just horrible. They got all of this, you know, horrible stuff happening to them and again the amazing thing is nobody knew. So the family certainly didn't know, I mean they had no idea why anything was happening, the local police and anybody they talked to didn't know, but conversely the company that was doing this had no idea, you know, they had this default value but they didn't sort of realize what that meant in terms of these people's lives and it was only when this journalist came in and sort of asked “hey, what are some locations with an unusual number of IP addresses mapped to them?” and it turned out this one has like something like 600 million IP addresses mapped to it, that she was able to figure out why this is going on. Of course this isn't even the only location that has this issue, it's just a particularly egregious form of this issue at this location and so, you know, they were able to identify other locations that had a lot of IP addresses mapped to them and weird stuff happening, but I think it's just worth noting a few things. One, the seemingly innocuous choices that you make in tech and computer science can have really, really big impacts on people's lives and so it's worth thinking them through and being very careful about them, even when you think oh, you know, I'm working in IP address mapping, it's not healthcare, you know, what's the difference? It can make a big difference. And then two, there's a great way to have diagnosed this problem. You didn't even have to anticipate this problem, you didn't have to know that problem could exist, but if you plotted your data, you would notice that something weird is going on and I think that that's, if you take maybe one metapoint from this lecture, it's that just plot your data to see if something weird is going on. If you plotted like a histogram of this data, you'd see that there's this one IP address that's just getting everything, like absolutely everything, or this one location that's getting all the IP addresses. Okay so maybe in this particular case we might not use a representative number, although I don't think anything is as intense as this is going to happen if we do. Maybe instead we'll do something like decade. We'll just say, hey what decade are these individuals in? Because that conveys exactly the amount of significant information that we have, you know, you remember hopefully from high school this notion of sig figs that you want to say for, you know, whatever information you have you want to convey the significant figures that, you know, but you don't want to suggest that you have more information that you have that that, you know, something to a greater significance than you really know it and so this lets us do that. Okay, so that's decade. Now I'm just going to take a little bit of a detour for something that we don't have in our data here but could come up and that's something called ordinal data. Okay so we talked about a few different types of data at this point: we've talked about numerical data where you have an order on your data values and the differences in value are meaningful, this is sort of the default type of data that we've been talking about in this class, it's the data that we assumed x was in to begin with, it's sort of the easiest type of data in some sense. But now we've also talked about categorical data where there's no ordering on the data values. And there's a different type of data that's kind of in between these, where there's an ordering on the data values but the differences are not necessarily meaningful. So some examples of this, if you've ever seen anything in the social sciences, an example of this is the Likert scale. So somebody asks: “hey I have, you know, some maybe political opinion, do you strongly disagree, do you disagree, are you neutral, are you agreeing, or are you strongly agreeing?” There's an ordering there, there's nothing that tells you the difference between strongly agree and, you know, strongly disagree and disagree is the same as the difference between disagree and neutral. They're ordered but that difference isn't meaningful. Same thing with, you'll often get asked in a doctor, your pain scale: do you have a pain at a level of one, do you have pain at a level of two, do you have pain at a level of three, and that just doesn't have the same meaning as what is your beats per minute of your heart. And so what could go wrong here? Well let's again plot some data. So here suppose that our doctor has measured our resting heart rate and maybe again with a higher resting heart rate we tend to see more heart disease and maybe they've also asked us what's our degree of agreement that, you know, we're not in pain. And so here we have an ordering and so very naturally maybe these are already encoded as the numbers one through five and so we might think “gosh these are numbers I don't have to do anything else with my data, I'm all set” but we might also notice that, actually, again these are different kinds of numbers, that they aren't meaningful in the way that beats per minute are meaningful. On particular, again, the difference between four and five might not really be the same as three and four here, maybe it's actually quite humongous and that really affects how we use linear classifiers. So if I have a linear classifier, I'm not just assuming an ordering on my data, I'm assuming a sort of rate of change: I'm assuming that the difference between one and two is the same as the difference between two and three and so on. And so if that's not true I could get some really different linear classifiers depending on what that difference is. This is actually pretty different from this.

And so, for that reason, even though this actually is already numerical data, I might consider encoding it differently when it truly represents something that we would call ordinal rather than numerical. Now an idea here is something that is called unary or thermometer code and so that's exactly the following: so I've replaced 1, 2, 3, 4, and 5 with 1 followed by all 0s, 1, 1 followed by all 0s, 1, 1, 1 followed by all 0s and so on. Now the idea here is that whenever I split, I split in order. I can split such that strongly disagree, disagree, and neutral are together and agree and strongly are grouped together. Unlike categorical data, I don't just take out a particular item when I split. I do a split that is in order but there's no implicit ordering on the data,  if I do a split it is just between those two adjacent elements. Now why is this called thermometer code? You could think of those 1s as they go along as filling up this thermometer with mercury. So it's sort of like very low temperature at 1, 0, 0, 0, and it's very high temperature at 1, 1, 1, 1 so that's that's the origin of that term. Okay so we've looked at a lot of different types of data at this point, we've seen our numerical data, our categorical data, our ordinal data, and in fact now the last thing we have here looks to be numerical data so, you know, we can keep it as it is and we're all set. So the thing to notice at this point is that we have numbers. These are numbers that we can take dot products with and so from purely the perspective of our algorithm will run and will not give us bugs, we're all set. But now let's ask: are there still potential problems or things that we could do better by again having some feature transformation of our data, transforming to a new set of features? So in order to talk about that let's actually talk about numerical data which we thought was the easiest type of data and, you know, in some sense it is but there are still some potential issues. So let's look at the output of our linear classifier and let's first look at, you know, something that we might do when we're interpreting our classifier after we've done our classification. So suppose that there are a bunch of people on the internet who are just saying “hey I really think that the number of garlic cloves that you eat in a week is super predictive of your heart disease” and we want to check is that true, is that something that is absolutely true especially when, you know, all these other things about individuals. And so we might check “hey I am interested in looking at this weekly number of garlic cloves eaten and resting heart rate.” By the way I should say the only thing we can talk about here is predictivity. We can't talk about causality because that's not what we're measuring, that's not what we're doing. We can't say is this causing uh, you know, something like the number of or the prevalence of heart disease or whether you have heart disease, you have to do some real science to check that or be much more careful, but we can check whether this is predictive given all the other information that we know. And so we might do this by the following: let's run our linear classifier and let's find our theta and we see in this particular case that our theta and our linear classifier just doesn't really depend on the number of garlic cloves that you're eating and one way you can see this is that, if you look at theta, it's change in the garlic clove direction is 0 or roughly 0 and its change in the BPM, the resting heart rate direction, is non-zero, it's highly non-zero, and so something that sometimes people do is they'll look at their theta after the fact and they'll say “ah, you know, it's really small in some of these directions, maybe those features weren't so helpful in our prediction.” And in this case, you know, that seems to be true. Okay so now that we've talked about this thing that people might do in terms of interpretability. let's see how numerical data and our coding of a numerical data enters into this.

Okay so now let's look at resting heart rate and income. So suppose I'm interested in heart disease, protecting heart disease, from these two features and I run my linear classifier and this is what I get. And now suppose that my friend actually encoded resting heart rate on a slightly different scale than I did, you're going to notice that they actually get out a different linear classifier. Now it kind of means the same thing in terms of prediction but if I were to look at the theta values and I tried to interpret the theta values, I'd say something different because I get these different theta values. Now there's an extreme version of this where I notice that actually income is on the order of tens of thousands of us dollars typically and resting heart rate is on the order of tens and so if I just plotted them on the scales that they are typically, you know, recorded at, you know, we just said “hey you should plot your data to see what's going on” so I plotted my data and this is the scale that it's at I would say “wow, you know, there's really nothing going on with resting heart rate here, this is nothing too interesting.” And if I looked at the theta, if I just looked at the theta without, you know, plotting my data or thinking about this, I would say gosh look there's, you know, you know there's almost 0 in the income direction and it's really highly non-zero in the resting heart rate direction, so it might be misled in various ways by doing this. And so there's something that we can do which I think you often just do without thinking, or more to the point your plotting program does without thinking, which is let's just make sure that we plot our data on a scale that we can see the differences.

And this is called standardization. So one way you can do this, I mean really again the main idea is just let's plot our data on a scale where we can see the differences, but one way you can do that that's very sort of automatic is to say, for each dimension where I'm interested in standardizing, I'm going to take the empirical mean of my data across that dimension, across that feature, I'm going to take the empirical standard deviation across that feature and I'm going to do the following transformation: so first by subtracting the mean I move everything around 0,  and then by dividing by the standard deviation I'm basically zooming in or zooming out as appropriate and so I'll end up plotting something that looks like this once I do standardization. Again I think we take this for granted because our plotting programs are typically doing this or something like it—they're finding what are the extreme points and then plotting those—but that's similar in spirit, you know, we're just doing something so that we can actually see the variation in the data and that's what this is accomplishing. This also makes it the case that when you look at a value of theta, when you do your linear classifier and you get some theta out, that it has some kind of meaning, as opposed to being totally determined by the scale that you just happen to be plotting.

Okay. Okay, so we've seen a lot of benefits of plotting our data at this point, again as a metapoint, I think that's just a really important point: you can't go wrong plotting your data. But let's just talk about one other benefit and also a really important benefit of talking to experts. You know, we just made this observation that maybe what you're going to do is you're going to interpret your theta and if you see that theta's kind of 0 in one direction that maybe not too much is going on there. Well let's suppose I do that. Suppose I look at this data and it's, you know, it's basically on the right scale, like certainly we can see any variation in this data, there's nothing where there's an issue of scaling that's going on at this data. So we're plotting people's resting heart rate versus where they seen at a particular hospital, and so it turns out that which hospital they were seen at is super predictive of what is going on, of whether they have heart disease, and resting heart rate just doesn't really matter and so if we weren't thinking about our data, if we weren't plotting it and we weren't talking to experts, we would say “yeah okay I have a great classifier, all you do is ask are these people, were these people seen at a particular hospital and then I know if they have heart disease. Great yeah it's perfect.” And so it turns out if I talk to experts here, maybe it's the case that actually everybody with heart disease goes to one hospital, there's like the hospital where you treat the people with heart disease and anybody who, you know, doesn't have heart disease doesn't have to go there, so they'll be at other hospitals. And so it's not the case that this is a useful prediction, I mean it's a great prediction— basically the doctors did the predicting already for us—and then we just, you know, copy their answer but if we're trying to come up with something that's going to be helpful to doctors, like a new useful tool for them, this is not going to be useful. It's like saying “hey you already made your decision and now we're just going to piggyback on that” and predict based on that. And so this is the kind of thing that, you know, we might not know going in, there's no reason that we would know all of these details especially if, you know, somebody gave us this data and then we're analyzing it for them, but by a quick talk with some expert about the data, we would be able to establish this issue and then maybe get rid of that feature because it's not so useful for what we're trying to do. Conversely, if you are an expert who is running machine learning for your problem, talk with yourself in the sense of plot your data, interpret what's going on with that classifier, and then think about is this actually what I'm trying to accomplish or is it something else and I need to rethink my features and rethink my problem.

Okay, so now we have all of our features, we've encoded them in these different ways, we're going to standardize our numerical features: so we're going to take them and we're going to perform the standardization that we just talked about and so we've got all this fantastic data that not only can we now apply our machine learning algorithms too, we don't have any issues with taking dot products, and we know that these thetas that we get out are going to be somehow meaningful and there's going to be some interpretability and so let's ask ourselves: okay we're ready to go, we're going to run our algorithm and, you know, I just I learned today that I should plot my data and so let's go ahead and do that and I plot my data and let's say here's my data. Okay so something that's very common in health and medicine is that there's like a sweet spot where the human body exists. Right so if your blood pressure is way too high or way too low, you might have some kind of health problem. If your breathing rate is way too high or way too low, you might have some kind of health problem. If your BMI is way too high or way too low, you know, there's there's all kinds of points where there's a sweet spot and so we might not be surprised to see data that looks like the following where somehow, you know, we want there to be, you know, we want to recognize that there is some typical range where things are happening and that we might predict some health bad health outcome outside of that area. And of course, something that you'll just immediately recognize about this data given all of the work that you've been doing, is that this is not a linear classifier and this is not linearly separable data. And yet it's so perfectly separable, you know, like it's separable in some other way that there's some simple function that somehow separates the places where we want to predict -1 and the places where we want to predict +1 and so we might ask ourselves is there something that we can do, is there something that we can do to deal with this, to approach this? 

And because today we're talking about how to deal with this or how to do pre-processing, we'll ask if there's pre-processing that we can do. Sorry I missed that there were some backlogged questions, if we could just handle those now that would be great. Going back to thermometer encoding, why is thermometer coding more meaningful than just encoding from like 1 through 5 for example? Yeah so I guess I wouldn't necessarily describe thermometer coding as more meaningful, just to say that it encodes a different set of information. So what's going on with the encoding of 1 through 5 is that you're not just saying ordering, you're saying that the difference between 1 and 2 is the same as the difference between 2 and 3 and so maybe I have a pain scale which says “I'm fine”, “I'm in pain”, and “I'm in excruciating pain.” So we could call that 1, 2, and 3 but maybe excruciating pain is just really different from a little bit of pain and it could be that when I do classifications that that really matters, you know, especially as we saw for linear classifiers that a linear classifier assumes that the difference between 1 and 2 is the same as the difference between 2 and 3 and so when we have a situation where our actual, the correct interpretation of the data that we've got, does not include that—so for instance in this pain example I don't know that the difference between 1 and 2 is the same as the difference between 2 and 3—then I really want to make sure that my encoding reflects that. So in some sense it's like I'm getting rid of meaning, I'm taking away some meaning, I'm trying to take away information that I had. So in the 1, 2, 3 it's like I'm trying to encode that there's more information , that these differences are the same. With thermometer encoding, I'm encoding just that there is an ordering but I'm not encoding that the differences are the same, I'm saying essentially that there's a dimension for each difference and that lets me have a different sort of slope or theta in each of those dimensions to encode that different difference instead of having the same theta,  the same change in everyone. Cool was there anything else that I had missed?

Yeah a couple questions on standardization: so does standardization always like, does standardizing data actually affect the results of our machine learning algorithm or is it just mostly used for picturing? Yeah great question. So it depends on your algorithm and so for instance we're gonna see algorithms like decision trees where it really just does not matter because you're just splitting on particular values. You can think through what happens in the perceptron whether it would matter or not and I will leave that as an exercise to you to think about right now and I think that you can figure this out. But we're going to see examples of algorithms where it actually matters to the algorithm. So in particular we're going to see in neural nets, for instance, that it matters and so that's not something that we've talked about so far and so I'm not going to go too deep into that but keep it in mind as we go forward to these other machine learning algorithms because there are algorithms where it actually matters for performance and it's not just an issue of interpreting and how we view it.

And from standardizing, do we get the mean and standard deviation from the data itself, or some sort of general statistics? Yeah so let me give two answers to that. So when I described it and what we describe in the notes, you get it from the data itself. So certainly a typical way to standardize is that you take the empirical mean, which is to say you add up all of the values in that particular dimension, in that particular feature, and then you divide by the number of values you've added. You take the empirical standard deviation, which is to say you get the standard deviation just by calculating it from the values you have. Now that being said, again I want to emphasize that if our goal is really just to get the data on sort of the same scale across our different features, this is not the only way to accomplish it and so if you had other information about some, you know, generic mean and standard deviation about your data, you could potentially use that as well or you could use quantiles as another thing that people can use in practice but again I want to emphasize this difference between, you know, typically when people talk about standardization—and what we're doing in the notes is exactly taking the empirical mean and empirical standard deviation—just that there are other things you can do that are perfectly valid and often very useful. Cool okay great, so let's come back here, and we've got this problem where, you know, we have this data, very reasonable realistic data, where we want to find some classification that works for this data and it's clearly just not going to be a linear classifier and so we want to ask ourselves is there something that we could do here and in particular, because we're sort of talking about pre-processing the data today, we're going to ask is there some kind of pre-processing of our data that would help with this because there are certainly other things you can do and we'll explore many of them later in the class but for the moment let's ask, you know, is there some pre-processing that would help. Okay in order to ask this question about sort of non-linear boundaries or answer this question about non-linear boundaries, it'll help us to revisit thinking about classification boundaries for the linear classifier. So here's, again, our cartoon of our linear classifier. We have some data we want to find a classifier that says “hey we're going to predict +1 on one side of some hyperplane and -1 on some other side.” So let me just recall first, you know, this is our linear classifier, this is the side we're predicting +1, this is the side we're predicting -1 and then on the line itself we predict -1 by convention, but let's think about a different way that we could visualize what's going on. So what's happening here is that I've introduced a new dimension, let's call that dimension z, so on the horizontal axis we have x_1. x_2 is the axis that's kind of hidden behind this colorful hyperplane and then z is popping out of the page.

This hyperplane is actually theta transpose x plus theta naught equals z.

This plane that I've just highlighted in gray is where our original data lies. It lies with x_1 and x_2 varying but z equals 0.

The line that defines the linear classifier is where theta transpose x plus theta naught equals 0.

That's exactly how we defined our line before. And if we want to look at the two sides of the line, there’s one side where this colorful hyperplane is below the z = 0 plane, the colorful hyperplane that theta transpose x plus theta naught is less than 0 and so we predict -1. On the other side, theta transpose x plus theta naught is greater than 0 so we predict +1. Why is this useful? Because we could have other functions, we don't just have to have a hyperplane, we could have some more general function then we could ask where is it greater than 0 and where is it less than 0. So for instance, let's say we cared about this data we were talking about before with the minuses and pluses.

What if we can make a function like this?

Let's just call it f(x). Here's z = f(x). Well in this region, the function is below z = 0, the function is less than 0, so we might predict -1 for the set of x for the function is less than 0.

And then if we look on the rest of this plane where the function is greater than 0, that's where we're going to predict +1 for that area. So if we have the ability to use very general functions instead of just hyperplanes we could get some really funky different regions where we do our classification.

Now really really general functions, that's asking a lot. Is there some way we could, sort of, simplify this and so there's a familiar idea, hopefully familiar to you from your earlier courses of a Taylor expansion, you know, I can get pretty close to a very general function by just looking at a polynomial up to a certain order. In fact it turns out this f(x) is actually just a quadratic polynomial.

And so how can we do this? Well let's look at polynomials.

So we just said if I want a really general function, general smooth function, I can approximate that pretty well especially within a particular bounded region with a k-th order polynomial, kth order Taylor polynomial. For instance, let's say it's around 0.

So if, in particular, I was looking in one dimension, if my features only existed in one dimension, then just recall that the sort of 0th order Taylor expansion is basically a constant or a constant times one.

If I take the first order Taylor expansion of a function, I'm going to get a constant times 1 and a term that looks like a constant times x_1, where x_1 is our only variable, our only feature. If I look at the second order Taylor expansion, I'm gonna get a constant times one and a term: I'm gonna add a term that looks like a constant times x_1 and I'm gonna add a term that looks like a constant times (x_1)^2. If I look at the third order Taylor expansion, I'm gonna add a term that looks like a constant times one, and a term that looks like a constant times x_1, and a term that looks like a constant times (x_1)^2, and a term that looks like a constant times (x_1)^3, and hopefully you get the pattern that's happening here. And so an observation is that if I had one dimension and I wanted some really flexible boundaries in my classification, I could have an expanded feature space where maybe I take my original feature, x_1, and now I look at new features 1, x_1, and (x_1)^2 or if I wanted even more flexible boundaries I could look at 1, x_1 (x_1)^2 and (x_1)^3 as my new features. Now, there's a version of all this that exists in higher dimensions. So the 0th dimension is just the same, it's just a constant. If I did a first order Taylor expansion in my d dimensions but only first order then I'm going to pick up x_1 up to x_d terms.

If I'm looking at second order, I'm going to get all of the quadratic terms: x_1 * x_2, x_2 * x_3, (x_1)^2, (x_d)^2, all that and it just gets really messy after that, we get a lot of terms. But the idea here is that we can get some really flexible boundaries by expanding our set of features using these polynomials and this will often be called something like a polynomial basis. It's not the only way we can come up with flexible functions, there are definitely other ways, but this one's kind of convenient and certainly an easy thing to do: all you have to do is take whatever my original, you know, feature was and make some new features 1, x_1 and (x_1)^2, for instance, and then I can get these super flexible boundaries that we saw before. So again, just to look at what this looks like, you know, if I use this quadratic function—so this is something I could get by using all the polynomials up to degree two—then I could classify all of the area where my function is less than 0 as being -1 and all of the area where my function is greater than 0 as being 1.

What if I had some funky data like this? Well if I use up to third order polynomials and I can look at all of the area where this function is less than 0, and that's exactly this weird shape—so blue is the z, it's the one that's sort of coming out the middle here, so hopefully you can see that we get sort of this weird shape by looking at where the function is less than 0 and then if we look at where the function is greater than 0 we get this weird shape. This is all just to say that if you use this polynomial basis, if you do this feature change, you change your features by expanding them with these polynomials, you can get some really flexible and different boundaries, certainly highly non-linear boundaries and the reality is that a lot of life is is pretty non-linear, you know, we saw this example in health, again, there are a lot of health examples where there's some sweet spot for the human body, there's a temperature that's a good temperature. If you're really far away from that temperature you might be experiencing a health issue and we'd like to be able to detect that and so we'd like to be able to say hey, you know, not everything is a linear boundary and so how can we do that and we're starting to see some examples.

Okay so we have this ability now to fit some very flexible models with some very flexible boundaries and so imagine that here is my data and something we often also really see in real data is this kind of overlap, you know, maybe if my weekly exercise amount goes up I tend to, you know, not have heart disease as much. If you look at people whose resting heart rate is going out maybe they tend to have more prevalence of heart disease but there's often this sort of wishy-washy in between, you know, some people who exercise a certain amount and have a certain resting heart rate have heart disease and some people don't and so I could fit my super flexible polynomial model, I took a super high degree polynomial, and I get this like really great classifier: so here I predict +1, here I predict -1, and the training error is 0. So this is fantastic,  right? This is a question for you in the chat: is this a fantastic classifier?

Great, lots of no's, “no way,” “it's overfitting.” Exactly, yes the training error is 0 and it's super flexible but sometimes flexibility can be a little bit too much as we're seeing here that, you know, maybe I have the ability to get so much nuance in my data that I'm worried how it's going to perform on new data and that's exactly the concern with overfitting that, you know, we have to ask ourselves again what's the point of doing any of this machine learning? Well the point is we'd like for somebody else to come into the hospital or to be at home and to be able to say, you know, something about their diagnosis, you know, do they have a particular health problem and our intuition here is telling us that, you know, it's probably not the case that if you exercise down to this decimal amount of, you know, exercising in terms of minutes of exercising and if your heart rate is exactly 88 but if it's 87 then you definitely have heart disease—that just doesn't seem physically meaningful. In some sense, you know, we have some expertise about medical problems, you know, it's not maybe as much as, of course, a doctor would have or somebody who's really in that area, although it's quite possible that people in this audience and maybe our doctors or EMTs, but even from what we know, we're consulting our expertise and we're saying this doesn't really seem like what's going on: it seems like we're overfitting here, it seems like this is not going to perform well on future data, and so we want to ask ourselves how can we detect overfitting and how can we avoid overfitting? And so for the rest of this class we're just going to talk about how can we detect overfitting, but next class we're going to start talking about how can we avoid overfitting. And of course we can start thinking about it right now, but let's go a little bit into detecting overfitting. Now this is something that you've been exploring again in the labs and the reading but let's just sort of cement some of those ideas here.

So in order to detect it, I mean in some sense what we're asking is how does our learning algorithm do, you know, is it the type of thing that in general if I apply it am I going to get some super overfitting, you know, if I apply a super, you know, high degree polynomial it seems like I could probably fit things that are maybe a little bit too flexible and I'd like to be able to know if that's true. So how good, you want to ask, how good is our learning algorithm on data like ours?

Okay so in some sense the issue that we have and this is an issue that you've been exploring in various problems is that we only have the data that we have. It would be great if we had sort of an infinite amount of data and we could try out all kinds of different things, but we only have the data that we have and so perhaps we could use all of that for training and then report the training error. This sounds so silly and we've talked about it a million times but it is surprisingly common, you read a news article and somebody reports accuracy and then it just turns out it was training error and you have to be really skeptical of that and so I, you know, it's one of those things that feels at this point like belabored but I just hope that the next time you read a news article about machine learning you ask “hey what exactly are they reporting?” Okay so we know that this is a bad idea, we know that we should not use the full data for training and then report the training error. We could have—exactly the issue that we just saw on the previous slide that that was, you know—0 training error. Great but not so great really. Okay so one idea is to reserve some data for testing. So just as a little bit of a cartoon, let's imagine that this bar represents all of our data from the first to the nth data point and so maybe we could use 75% of it for training and 25% of it for testing, or maybe we could use 50 of it for training and 50 of it for testing. The reality is that there's a real trade-off here. If we have more training data, that's closer to training on the full data that we actually have available, so we have a better sense of how our algorithm performs on data sets of size n. On the other hand, if we have more testing data, we have a better estimate of performance for what it is we're looking at. In the extreme version of this is, suppose I test on one data point, I can only get a testing error of 1 or a testing error of 0 and so that's just very noisy. I don't really get a sense of what really is the testing error of my method.

Also an issue here, and certainly one that you've been exploring, is that if I just train on some part of my data and test on some part of my data, that's just looking at one classifier and that might not be representative of my algorithm, you know, maybe I got unlucky, maybe I got some unlucky training data, and some unlucky testing data, or one or the other. Now there are, of course, ways to help yourself be less likely to get unlucky: one of those is shuffling your data so, for instance, a very realistic thing that could have happened, you know, we've been talking about this health example. Suppose that somebody went into the hospital where all the heart disease patients are not, and they went through all of them and they asked them all their data and then they went to the hospital where all the heart disease patients are and they got all of their data and so if I just took my first bunch of data as my training data and my last bunch of data as my testing data, I'd have this awkward thing where I train on all of my negative examples and then I test on all of my positive examples or something that looks like that. And so you just, you probably, want to, in general, shuffle your data and this comes back to an actually a sort of interesting point that came up in some of the discourse questions after lecture two, you know, when we were showing this demo of the perceptron, we were looking at data point one and then we did the updates for the perceptron for data point one, and then we looked at data point two and we did the updates for the perceptron at the data point two, but something some people noticed being very paying close attention which is great is that data point one was kind of in the middle of everything, and data point two was kind of in the middle of everything, and then data point three was kind of in the middle of everything, and what's important to notice here is that unless you know something about your data, you just can't assume anything about the data ordering. It could be all over the place which happened to be the case there, it could be very structured and in order which happened to be the case here. You want to be very cognizant and very conscious about what's going on with your data and make choices appropriately. And so on the chance that it could be very much an order, it actually helps many algorithms, as well as your ability to evaluate, to shuffle and that's certainly what we would want to do here. Okay so that all being said, we've seen there are some potential downsides to this, namely that we could get this noisy estimator performance if we have, you know, too little testing data and only one classifier might not be representative and so there's an algorithm that helps us with this and it's in, I believe, chapter one of your notes, certainly one of the first two chapters, and that's cross validation.

And so let's just go briefly through cross-validation. So the idea of cross-validation is that I'm going to have some way to evaluate my learning algorithm and in particular it's going to take in a dataset, let's call it D_n, and k, which tells us how much we break down our data, so we'll see that in just a second. So in particular, we're going to take our data and divide it into k chunks, these are often called folds, so we call this k-fold cross validation. So here we have our data and let's say k is 10, so we would divide it into these k, these 10 chunks. So this would be the first one, this would be the second one, the third one and so on and we're going to do this for loop over all of these different chunks. So let's just take one of the ones from the middle, let's say this is i, the i that we're on right now.

And what we're going to do is we're going to take our classifier—we have, in particular, our learning algorithm—we're going to take our learning algorithm and as input to that learning algorithm—remember our learning algorithm takes in a dataset, a training dataset, and it outputs a classifier—and so here the input to our learning algorithm is going to be all of the data except for the i-th chunk. So that's what this notation means, the D_n \ D_n,i means we're going to take all of the D_n except we're going to remove the i-th chunk. And so we can think of that in this illustration here at the top as being all of the orange data, so we're basically going to take, you know, one through five chunks and six through ten chunks or sorry seven through ten chunks and we're gonna leave out the sixth one and then what we're gonna get out from that is what we're gonna call h_i, so that'll be the classifier returned by our learning algorithm. Now what we're going to do, is we're going to compute test error. So we say I'm putting tests in quotes here because we had talked about test errors being on some new totally different data that we had never seen before. Well our training algorithm hasn't seen this data and so we can think of it as being new to our learning algorithm, so we're going to compute the error on this new data and the new data here is D_n,i. So the important thing is just that it was not seen by our learning algorithm and here I'm just using this notation E, it's a little bit different from how we had defined test error before, but what I mean by this is let's take our classifier h_i and look at its error on D_n,i. So we can look at its loss on each point in D_n,i and then average that loss over all the points in D_n,i.

Okay so we can think of that as a sort of test error. Again it's for data that wasn't used by this particular learning algorithm or sorry by this particular instance of the learning algorithm and then finally what we're going to do is we're going to take all of these different errors and average them. And so just as a sort of check on what we're going to get out here, well we're going to get a value between 0 and 1 for each test error, and so when we take this average over the different folds over the different chunks of the test error we're going to get another value between 0 and 1. So it's sort of how we've been thinking about error, it's on sort of the same scale and that's again because we're using 0-1 loss. If we used a different loss we might get something different, but if we use 0-1 loss and then we average over that loss, over the data, and then we average over all the folds and we can get something between 0 and 1. And so let's think about, you know, how does this relate to what we had on the previous slide? Well we said that we would like more training data because that gets closer to our full dataset size and so here we're able to get closer the bigger k is in some sense—or sorry, the yeah, the bigger k is—because we're basically training on everything except for that little case sliver. Now we said that a big issue is that we could have noisy test error if we have a very small thing that we test on and a way that we're getting around that here is that, even though individually one of these folds might give us kind of noisy test error, we're averaging over all the folds and so we get rid of some of that noise. And then another issue that we said previously was that we were looking at just one instance of the classifier and so if we do that we, you know, we might be worried hey we just got unlucky this one time. Here we're increasing the number of instances that we're looking at, by making k different classifiers, we're able to average over more versions. Now it's not perfect, it's not as great as if we had actually k different data sets, because these are clearly very related data sets, but it's better than just having one classifier, we're at least getting the ability to average over different instances and so if something really unlucky or weird happened, we might be able to isolate it a little bit better. Now that being said, it's still a good idea to shuffle the order of your data. you could still have this issue where, you know, maybe somebody looked at all the healthy patients first and then all of the patients who are having some health issue or vice versa. Now here it would only affect maybe a fewer number of folds, but it still might have a big effect and so it's still probably a pretty important idea to to shuffle your data to be sure that's not what's going on. Okay so today we've talked about more of a full ML analysis. We've talked about, you know, how you can get your data, how you can transform your data into a way that's usable by your algorithm that gets, you know, better performance from your algorithm, and then talk about evaluation but also interpretation at the end. Now one issue that we didn't resolve that was a cliffhanger from last time was this issue of “yes, maybe the reason that a linear classifier is bad is that you just have some weird shape,” but another reason you might not want perfect linear separability is what we saw in another example today is that, maybe your data is just a little noisy and so what do you do then? That's not something that—we've done a little bit to talk about this with average perceptron—but there's still some open questions there so we have a few cliffhangers including, you know, how do we deal with overfitting that hopefully we'll talk about next lecture. Okay have a good one everybody catch you later. And one small thing I noticed that a number of people have been writing a thank you at the end of our lecture in the chat and I just wanted to say I see that and I appreciate it and that's a very kind gesture. Okay see you next time.

Bye.
