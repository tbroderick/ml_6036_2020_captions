Okay, I think it's MIT time so let's go ahead and get started for today. A lot of the information at the top of this slide is basically some of the logistical stuff that we've covered in previous lectures but I wanna especially highlight the Discourse site which Crystal has also helpfully just put in the chat. Remember today's category for the live lecture is “Lecture 2” and so there's a nice example question in the Discourse with “Lecture 2” and so just make sure to use that category for your questions today. Okay so last time we talked about, you know, why are we even talking about machine learning and some basic setup questions and sort of, you know, did the notation for setup—what's the data we're talking about, etc—and started in on an example type of machine learning problem where we're interested in linear classifiers and we're interested in learning algorithms to find a really good linear classifier and so we really just got very much started and we're gonna, you know, continue on that journey today by talking about: “hey, what are some problems with the linear classifier?” You know, we found a learning algorithm last time. We talked about a learning algorithm, so why are we still talking, you know? What is problematic about that learning algorithm? And we're gonna see that it's not perfect and that we can do better perhaps with something like a perceptron algorithm so we're gonna introduce this alternative learning algorithm, the perceptron. We're gonna talk about different types of linear classification, how a particular data set might represent a harder or easier problem in these cases and that will help us talk about how well the perceptron performs in fact in a mathematical theorem. Okay so let's go ahead and just recall some key facts that we're going to be using throughout the day today about classifiers. So in particular, last time again we focused on linear classifiers. So we have our classifier h and recall that a classifier takes in a set of features x and it outputs a label y and here it has some parameters theta and theta naught that sort of determine its shape, so in this case recall that our linear classifier basically predicts a value of +1, a label of +1, in some part of the space and a label of -1 in another part of the space. In particular, we saw that we have this theta which is sort of a normal vector to our hyperplane, we have a theta naught which is the offset of our hyperplane, and together they define a hyperplane and so on one side that hyperplane we predict +1, on the other side of that hyperplane we predict -1, and so in two dimensions, you know, if our features were x_1 and x_2. For instance, last time we talked about, perhaps, we have a problem or we're interested in predicting whether newborns might have a seizure. That would be an important medical piece of information and we might have information about maybe, you know, sort of their oxygen uptake and whether they're moving a lot and we might want to predict that, and so the types of classifiers we might be interested in are these linear classifiers. At least certainly that's true for the moment and so we have our normal, our theta, we have our offset or theta naught and that's going to define a line. That's the set of x's that satisfy this equation. Now you know we said this line on its own isn't enough. It's together with this direction, theta, that defines the linear classifier. In particular, on one side of this line we're going to get a positive label, a +1 label, on the other side of this line we're going to get a negative label, a -1 label. Now, something that you'll have noticed is that we don't always have to put theta naught, or sorry, we don't always have to put this vector theta directly at the origin. So, we can move it around this normal vector,  so that's what I'm doing here, I'm moving it. In fact we could move it around to all kinds of different places, the key thing is it's always normal to the hyperplane, it's always perpendicular to the hyperplane, and what really defines it is going from its base to its top. So what's the difference there? That's the thing that is going to define theta. Okay, now in general this won't be a line, this dividing thing, this, you know, divider. It'll be a hyperplane. So if my data exists, my features exist in two dimensions, like here, it'll be one dimensional lower. In fact, it will be a line, but if my features existed in three dimensions, it would be one dimensional lower: it would be a plane. If my data features existed in four dimensions, it would be one dimension lower: it would be a three dimensional, you know, object and so on and so forth. So, you know, again it's worth keeping in mind that we as humans can only see in two dimensions, our illustrations are necessarily two-dimensional, but it's worth thinking what would happen in higher dimensions and so it's, you know, at least worth trying to visualize the three-dimensional version of this and convince yourself what that would look like. Okay, so we said: “hey, now that we have this notion of a linear classifier, we can look at the hypothesis class of all linear classifiers.” This is just, you know, the set of them all and then we're going to try to choose a good one for some data is our plan. How are we going to define what it means to have a good classifier? Well we're going to look at a loss. In particular, a very common one for classification is this 0/1 loss. So remember, g is our guess and a is the actual value and so what we're saying here is that we'll have a loss of 0, which is the best loss, if we guess the actual value, if we're exactly right, and we'll have a loss of 1 if we're wrong in any way. And so now, once we have a notion of a loss, we can look at training error. So in particular, for our classification problem with the 0/1 loss, what this training error says is we're gonna count up ones for every time we get something wrong and then we're gonna divide by n, the total number of data points. So the training error is exactly the fraction of data we get wrong, the percent of data that we get it's prediction wrong on the training data because this is the training error. Okay so last time we proposed a learning algorithm. So remember the idea of a learning algorithm is that we take in a set of data and we return a hypothesis and ideally there should be a good hypothesis for that data, that's sort of the goal. You know you can have bad learning algorithms, that's still a learning algorithm as we saw last time, but we'd like to have something where this hypothesis is going to be a good classifier for data and, you know, we feel like this is something we'd like to use and so for our example learning algorithm from last time we said “hey my friend went out and generated a whole bunch of hypotheses like totally randomly, just made some random hypotheses in a list, and then gave them to me and then walked away.” And then I'm going to do the following with those hypotheses:  I'm going to say I'm going to take in my data, so this is the first time I'm even seeing my data. Here, I have a hyper parameter k which tells me how many of these hypotheses to look at. I'm going to look at the training error on each one of these hypotheses, I'm going to figure out which one of them has the minimum training error among those k hypotheses, and then I'm going to return that. Now another way that I could do this is I could say: “okay let's let me look at my first hypothesis. What's its training error?” Okay that's my sort of putative hypothesis, the one I'm probably going to return at the end. So far now I'm going to look at the next hypothesis: does it improve the training error? If so, that's my new best hypothesis so far, if not I stick with the one I already had and then I just keep doing this. I say “ okay, for the third one, is it better than the ones I've seen so far?” If so, it's my new best hypothesis and I keep going and I can keep increasing the number of hypotheses. So let's actually let's try this out, let's see it in a demo, and so that is what I'm going to do right now.

So okay great. So what you're seeing here are two different plots. Let me just explain the top one first and then we'll get to the bottom one. So in the top plot, first of all, you'll notice that the axes are x_1 and x_2. So this is a plot of our data and so we have our data at its feature vector x and then you have a label, so it's either plus in case it was labeled +1, or minus if it was labeled -1. So particularly, this is our training data, we know all the labels and we're interested in finding some classifier. Now I hope you'll just take a moment and look at this training data and ask yourself, you know, “is there a classifier here that you might think would be a decent classifier?” I hope that you see that there indeed should be such a classifier. What if I had the best classifier here in the sense of training data, like if I minimize the training data. Now this is going to be a question that I'm going to ask you to respond to me privately in the chat: what would be the best training data that I could, with the best training error that I could achieve on this data set with a linear classifier? Great lots of folks are saying 0. Indeed, it looks like there should be a line that perfectly tells you which side is plus and which side is minus so that's exactly right. Okay so you know that and I know that, but let's find out if our algorithm is gonna find that, and so what's going on with this line here, so the line that we're seeing here is the first hypothesis proposal. So it was randomly generated by my friend, my friend didn't see this data at all, they just gave me this random line and it's my first hypothesis and so what's happening in the title is I'm saying “well before I saw this hypothesis, I had no idea what the training error was and so let's just say it's 1, because that's the worst training error that I could get for this.” The training error we saw before is between 0 and 1 because it's a fraction wrong, and this new hypothesis has a proposal, is a proposal, it has a training error of 0.47. So let's just take a moment to see is that a reasonable thing. Okay well this hypothesis is this line here and it says predict plus on this sided line, so it's getting these points right and it's getting these points right, but it's getting all of these points wrong and so it seems plausible that that could be in fact the training error for this proposal. Okay so now what we're also going to do is, because we're going to go through a few different proposals as we go along, we're going to plot the best training error so far down here. So here, we've only seen one proposal so far. We've just looked at k = 1 and that training error is 0.47. So that's why we're seeing a little dot here. Okay so now I'm going to go one step forward. Okay so what's happening here is the best proposal I saw before this step is in blue. That's just the last proposal because there was only one proposal before this step. Now my new proposal is that dashed black line and so, hey, this is a better proposal. You'll see that so far, my best training error, which is just the training error of the previous line, was 0.47 but my new proposal has a training error of 0.2 and, you know, just looking at this proposal it looks like a better proposal: it's closer to saying the pluses are on one side and the minuses are on the other side, and so let's accept that. Let's say that you know that's the best proposal we've seen so far, so that's going to be the classifier, that's our best classifier so far and in fact it has a lower training error. Okay let's go one step forward. So now we're getting another new proposal, so again my friend just generated these before we saw anything. They're not based on the data in any way, it's just another new proposal and that's in black and our best proposal so far was in blue and now you can just see this is a worse one. So even though our current training error of the best proposal is 0.2, the new proposal has a training error of 0.3. That's not better, so we're not going to use that, we're just going to throw it away, and so far our best proposal still has a training error of 0.2. Okay let's see what was my friend's next proposal, it was this line in black. This has a training error of 0.7. This is just a horrible classifier, I mean you could do better by guessing at random than 0.7. That's, you know, getting 70 percent basically of the labels wrong, and so we're definitely not gonna keep that. Okay so we can do this again, we can see, okay, what's the next proposal? It's some other literally random line and it's got a training error of 0.45 which also isn't good. Okay so there's this pattern emerging where, remember, these lines were generated before we saw any of the data: they're not based on the data, and so they're not, you know, systematically getting better in any way. We're really just hoping to get lucky and hoping that one of these lines is just going to happen to fall in the right spot, and so we can keep doing this, we can keep, you know, looking at these proposals that our friend generated for us before and they just keep looking bad and we're not accepting them because they're not in the particular place that we wanted them to be, and this can just keep going on and on. In fact, let's like step, you know, 100 steps into the future. We've looked at 100 steps and our training error still isn't zero. It's like we got very close, we got very close to zero, but it doesn't help us in the sense that, well, we now have to wait, we have to keep waiting until something just happens to fall in that gap, and it's not like we've learned anything. It's not like we're proposing better hypotheses based on what we've done so far, we're still just proposing random hypotheses that somebody generated beforehand and so nothing's getting any better and in fact, you know, we can keep doing this, we can keep looking at 100 more hypotheses and 100 more hypotheses, you know, now we've looked at a total of 413 hypotheses and the training error still isn't zero, and this just seems like a pretty simple problem. It seems like we should be able to find a classifier that divides these two sides of the line and so that's really where we're going next. We'd like to find an algorithm, a learning algorithm, that's a bit smarter,  that somehow uses what we've done so far to try to propose a better hypothesis based on that information rather than just looking through basically every hypothesis and hoping for the best. Okay so before I go on, I just want to check if there were any clarifying questions or questions just understanding what's going on in this, in this graph.

Someone asked, “how do we ensure that we're generating different hypotheses each time and that we're not repeating the hypothesis?” So we don’t. This is, this is great. What's going on here, my friend just gave me a list of hypotheses. Maybe my friend’s hypotheses are actually going to be all the same at some point, I don't know that. This is not a good learning algorithm because there's, we're not generating hypotheses in a smart way. I agree that it would be good to make sure that all of our hypotheses are unique at the very least, but I think we can do even better than that. It would be nice if our hypotheses weren't just unique but somehow got better, you know, somehow tried to be closer to dividing the pluses and the minuses, and so these seem like things that we would like in our algorithm and let's try to think about whether we can go in that direction. Spoiler alert: we might not finish that quest today.

Another question is: “what is k in this?” Great, okay yeah. So k is the number of hypotheses we have tried so far. So you could think of it as the hyper parameter to the learning algorithm in the slide. Basically it's just the number of hypotheses we've tried so far. So we have this list of hypotheses for our friend and we're saying “what's the best of the first k?”, and so when k is one, we're just saying, “what's the best of the first hypothesis?” It's always going to be the first hypothesis, but at this point we've looked at 413 hypotheses and the best one still isn't actually dividing our data and we think that maybe after 413 steps of effort that we should be able to find something that actually classifies everything correctly here because you did it on the first try,  right, like you just looked at this and you said “I know a good classifier that can get training error 0.” And so I feel like we should be able to automate that kind of process. One more question: “and if the point is on the line does it count as positive or negative?” Great yeah, so let me use that question to go back to my slides.

So as we talked about before, this is a somewhat arbitrary decision, but for the purposes of this class, we're going to say that if a point is exactly on the line, then we're going to give it a negative label with this hypothesis.

Okay so unless there's anything pressing, I'm going to go forward. So this was our learning algorithm. We said, “hey, it is a learning algorithm, the training error you'll notice is going to always, it's never going to get worse.” We never add a new hypothesis and then get a worse error, but we'd like it to be faster. We'd like it to go down to a good error as quickly as possible and so in some sense this is what we're going to try to address now: can we get better performance in that sense? Okay so we looked at our demo, let's try to find a better learning algorithm and in particular we're going to introduce the perceptron algorithm and then we're going to ask ourselves is this a, is this what we want? Is this a better learning algorithm? Okay so the perceptron is also a learning algorithm. So it's going to take in data as its input and it's going to output a hypothesis as its output and it's going to have a hyperparameter tau which is much like k in the previous algorithm in that it sort of gives us an upper bound of number of things to look at: it'll be a number of iterations, really. Okay so we start by initializing our values of theta and theta naught. Here's just a really quick question, again this is going to be for the private chat, so if you could just think about how many zeros is theta going to have. Do you have any thoughts about how many zeros theta is going to have? Now remember the way that we're interpreting theta and theta naught is exactly as on the previous slide. We want to do a dot product between theta and x and add theta naught to it so it, this has to satisfy, you know, that dot product dimensionality analysis that we did last time. It has to be something we can take a dot product of with x. so remember x is a column vector of size d, and so theta has to be something we can take a dot product with size d and so yes, exactly. A number of you are saying it has to have d zeros because again, that's the only thing that we could take a dot product of x with when x is a column vector of size d. Okay great, so the answer here is d, it's going to have d zeros just so that we can even take a dot product as we were thinking of before. Now we're also going to initialize theta naught. Something that's a little bit tricky here that you might notice if you're paying extra attention is that this actually does not define a hyperplane, the set of x such that zero dot product x plus theta naught equals zero. Well that's just everything. Everything is going to equal zero there and so we're gonna have to quickly get to the point where we have a hyperplane and we have a classifier, so let's pay attention to that as we go forward in the algorithm. Okay so as I said, tau is the number of iterations in this algorithm, so we're going to step through these iterations one through tau. We’re gonna change something, and so here's just an indicator of whether we've changed it yet. So far we have not changed it, so “changed” is false. Now we're going to go through each of our data points. So remember we have n data points and we're going to go through the data indices i equals 1 to n and we're going to check something. Okay so let's take a moment to think what does this check tell us. Now if y^(i) is positive and this second term theta transpose x plus theta naught is negative? Then this statement will be true, this whole thing will be negative. Well what does that mean? Well if y^(i) is positive, our label is +1. If this second term is negative, that means our predicted label, our guess label is -1, and so that means the actual was +1, the guess was -1 and we were wrong. Now similarly if y^(i) is -1 and our predicted label is +1, then we're also going to get something negative, and this whole statement will be true. And so one way to think about this statement is roughly that it just says: did we get this classification wrong? Did we classify this particular point wrong given our current iteration of theta and theta naught, our current values of theta and theta naught? And that makes sense, if we get something wrong, that's when we want to make a change, because we want to correct it and so that's what this if statement is effectively doing. Now there's a little bit more nuance here that relates to a question that we just had too. So one, if this prediction is wrong and we're not on the line, which is the case we just discussed, then this if statement is going to be true. You'll also notice that if a point is on the line, this if statement will just be true, because if a point is on the line, this quantity is equal to 0 and so the whole thing is 0 and 0 is less than or equal to 0. Also this if statement will be true if we're on the very first step, because in the very first step theta is 0 and theta naught is 0 and so this whole thing is 0 and 0 is less than or equal to 0. And so this is nice because we said in the beginning, “hey this theta and theta naught, that's not really a linear, describing a linear classifier” and so what's nice is that we're going to immediately change them on the first step here. Okay so what's going to happen if we decide to change, if we find that there's an error and we want to make a change? Well we're going to update theta and theta naught and I'll explain in a moment what these updates do and why they might be sensible, but for the moment let's just say we're going to make some kind of update to theta and theta naught and we're going to say that we in fact changed theta and theta naught, so we're going to make our indicator say we did make a change on this round.

Okay so if we went through every data point and we didn't change theta and theta naught at this point we're gonna break out of the algorithm and we're gonna finish up and once we go through all the iterations or we hit that breakpoint we're gonna return. Now remember we said that the output of a learning algorithm is a hypothesis and here we're returning theta and theta naught and the idea being that this will describe our linear classifier hypothesis.

Okay so let's just briefly say what's going on with these updates. Why not just add five to everything, you know, why are we adding these particular values? In order to get at that question, what we really want to do is we want to sort of look at, what was the thing that led to the update, the update being this change in theta and theta naught? Well the reason that we made this update is that this quantity was less than or equal to zero and we said that roughly that expresses that somehow we're not classifying this point correctly, you know, that we're guessing the wrong label for this point, and so let's see if that changes when we make this update. So what we're going to do is we're going to look at the same quantity but with the updated value of theta and the updated value of theta naught.

Okay so all we're doing is we're saying we had the original value of theta and theta naught, we made the update, and now we're going to look at the updated values. And so what I'm going to do here is I'm going to substitute for this updated value of theta the actual updated value, and for the updated value of theta naught, I'm going to substitute the actual updated value. So I'm just taking those equations, you know, where we say we set theta to a new value, we set theta naught to a new value, and I'm putting them in to this equation.

Okay so now what's happening is I'm just rearranging this slightly. So let's go through this term by term. So the first term here is this one, y^(i) * theta * x^(i) and that just goes down here. The second term in that first equation is (y^(i))^2 * x^(i) transpose * x^(i), so that just goes down here. The third term in the first equation is y^(i) * theta naught, so that just goes down here, and the fourth and final term is (y^(i))^2, which is equivalent to (y^(i))^2 * 1. Okay so this was just some algebra, we just sort of rearranged things in this equation. Not too much happened, but why are we rearranging things this way? Well now you'll notice that this equation takes the form of our original thing that we were comparing to zero. You know, if we look at that if statement, this is the equation in that if statement, the thing that we were saying: “hey, if this is less than zero, then we've made an incorrect classification and we've added something to it.” So you can think of the second term as being the change, what changed by changing this theta and this theta naught, what changed about this if statement. Okay well let's simplify this second part a little bit. The first part we already know, it's sort of, you know, what we were originally comparing to zero but what's going on in the second part? Well first, we have (y^(i))^2. y^(i) is +1 or -1, and so actually we can just evaluate this, it's just one. Whether I square +1 or -1, it's always going to be one. Okay now we have x^(i) transpose * x^(i), that's just equal to the length of (x^(i))^2. Okay so let's put that down here. So again we just have the thing that we were originally comparing to zero, that told us in the old theta and the old theta naught if we were misclassifying, and we have this second term which is: x^(i), its distance, its magnitude squared + 1. And the key thing to notice here is this second term is strictly positive because (x^(i))^2, this magnitude squared has to be zero or above, and then we're adding 1 to it, so we're adding this strictly positive term to our old value that we were using in the if statement and remember in the if statement we're checking is this value less than or equal to zero and so now we've added something strictly positive to it and by doing that we think that we should be more likely to get something that this if statement does not satisfy. In particular we should be more likely to get a correct classification because again this if statement represents sort of did I get a correct or incorrect classification and so that's. This is just sort of a way to think about why are we doing this update to theta, this update to theta naught, it's sort of moving us in the direction of a more correct classification for this particular point.

Okay so that's the if statement, that's what's going on there. We're saying again if I’m making an incorrect classification, let's try to move to a more correct one and then let me just briefly remind you of this “break”, and I think it's worth here verifying for yourself, this is a question in the reading as well, but I just want to point it out. You want to verify for yourself that putting in this break doesn't change anything about the output of this algorithm, that if we got rid of this break, the output would be literally exactly the same, nothing would change. The only purpose of this break is to save us some extra effort and not bother evaluating if statements where we already know that we're not going to get it true for those if statements and so you just want to make sure that you can convince yourself that that is the case.

Okay so let's look at this and practice with a demo. Oh, there's a question before I go on. One quick question is how do you decide on what's n and what's tau? Great okay, so how do I decide on what's n and what's tau? So for n that's an easy one, that's just the size of my data and so whatever training data I have, I just say how many data points there are and that's n and there's sort of no decision involved, I'm just given n. Now of course, in reality, sometimes you can go out and collect more data and then it's sort of a question of like cost benefit trade-off, but for our purposes here, and honestly in a lot of this course, we're going to be thinking of n as just being something that's given to us, in the sense that the training data is given to us. Now tau is a trickier question, because in some sense, we don't know when we're going to hit the end of this, although we're going to get some insights into that with our perceptron theorem. So in some sense, the perceptron theorem will actually give us some really great insight into how to set tau, but until then let me just say that one way you could think about tau is just being that there's some point at which you don't want to keep going. Like suppose I had an algorithm that could keep going forever and that's actually true of our previous algorithm. There is some chance that it could just keep going ad infinitum and never reach, for instance, zero training error and at some point I just have to stop. Right, like I have other things to do with my life, I have other projects to go to, and so you can think of tau as representing how much time you're willing to spend on this and sort of just making an upper bound. It's saying: “okay at a certain point, I'm just going to cut off no matter where I am and I'm going to stop and that's tau.” Now again, we're going to complicate that question a little bit more, we're going to get a better answer to what is tau going forward, but I think that's a perfectly reasonable way to think about it right now. Great.

Okay cool and just a reminder to everybody if you have any questions, adding them to the Discourse is the right thing to do. Great. Okay, so let's go to that demo.

Okay so again, let's just talk through what we're seeing in this demo. So here, what's happening is we have our exact same data set from before. This is actually the exact same data set that we ran our previous learning algorithm on, and that'll become important in a moment. Now remember that the way that the perceptron algorithm works is that it starts from setting theta to zero and theta naught to zero, and that doesn't really mean anything, but at the very first step we're going to pick out a point and we know for certain that we're going to update theta and theta naught on that first step. And so here we've picked out a point, that's that circled point with that black circle around it. We updated and now we have a hypothesis, so that's what's in blue, this is our hypothesis and you can see that it predicts plus on this side and minus on this side and so it's going to get all of these right, it's going to get all of these points wrong. Okay so that's our hypothesis on the step one of this learning algorithm. Now let's talk about what's going on down here in the second plot. So this tells us the number of steps we've taken so far, that is to say the number of times we've checked that if statement basically. So, so far we've only done it once. The number of updates so far is the number of times that if statement was true and we actually made a change to theta and theta naught. And so we did change it once so far, we started from theta being 0 and theta naught being 0. We updated it, we made a change, and now we have this hypothesis up here in blue. Now we can ask ourselves this is going to be the training error on whatever the latest hypothesis is because that's the way that this algorithm works, is it just keeps updating the hypothesis and that returns whatever is, you know, out there at the end, and so that's what we're looking at. The training error here is 0.38 and that's also plotted in blue as a blue dot down here. Now we just ran this other learning algorithm and so let's plot its training error too, why not. Let's compare them. So after that had taken one step, its training error was here in red, it was a little bit worse. Okay, so now what we're going to do is we're going to take a new step of the perceptron. So we're going to find a new data point and so I’m now turning our old classifier, old hypothesis, to this dashed line because we're going to propose something new. Potentially we're going to propose something new if this point is misclassified, right, and so first let's ask ourselves: under the current hypothesis, what do you think about this point? Is it misclassified? This is a great one to put in the chat.

Great, lots of great answers here. Yes, misclassified, bad. This is all accurate. It's points up here in the current hypothesis that'll be classified as plus, points down here will be classified as minus, and so this point is misclassified, and so the perceptron algorithm tells us we're going to make an update. Okay so let's go ahead and make that update, and so we get a different linear classifier, and whoa this one went straight to the right answer. So here we're seeing that after two steps and two updates the perceptron has zero training error. Now we're going to look at another example in just a moment to see if this is typical behavior. Spoiler alert: it doesn't always do this. But in this particular case that's what happened. It said, “hey let's use this new data point and it's misclassification to find a better classifier,” and so the training error is zero now. And so let's ask ourselves, here's another point, let's just, you know, keep going with this algorithm. Here's another point. Is this point misclassified according to the blue line, the blue classifier, which is our latest classifier? Is this point now misclassified? Keep the answers coming in the private chat these are all great.

Okay perfect, everybody's doing well. The answer is no, it is not misclassified, it is correctly classified because this blue line is saying that everything over here is going to be a plus and everything over here is going to be minus. In fact, it's correctly classifying every point and so because this point is correctly classified, we're not going to make an update, we're not going to go into that if statement. Okay the next point also is, of course, going to be correctly classified because we said they all are, so we're not going to go into that update, we're not going to go into that if statement and we're going to stay where we are. And the next point is correctly classified, so we're not going to go into that update, we're not going to go into that if statement, we're going to stay exactly where we are. And I think you're hopefully seeing a pattern here that every one of these points is going to be correctly classified, so when we evaluate that if statement, we're not going to go into it. It's going to be just fine and we're not going to make any updates and so this is it, we've reached the end. Basically this is the hypothesis that this perceptron algorithm is going to return. Okay now this is one particular data set and so I think you know it's always hard to read too much into one particular data set when you're thinking about an algorithm. Let's try out another data set, even if it's a very similar data set we might get some pretty different behavior and so that's what we're going to look at now.

So we're just going to do the exact same thing in the sense of looking at a demo, but it's a very slightly different data set. Now this is a very similar looking dataset, you know, it's got a lot of the same sort of things going on in it and yet it's not going to be exactly the same. So here let's again think about how the perceptron is going to go. By the way without us looking I also ran that that other learning algorithm from the beginning of lecture so we can compare training error down here. Okay so the first step in the perceptron again is going to be: let's look at a data point and then let's find our first classifier. So here we found this data point and so now we can plot our first classifier and it's going to say everything over here should be classified minus and everything over here should be classified plus, so it's got a training error in this case of 0.43 and that's what we're plotting here in blue. So now, that's our old classifier and we're going to update to a new classifier, potentially by looking at this data point. So this data point is on the plus side and it's already labeled plus and so actually when we state we're not going to make any updates here, we're just going to stay where we are and we're going to keep the old classifier. Okay let's go to the next data point. This data point is classified as minus, its actual true label is minus, and it's being classified as minus by our existing classifier, and so again we're not going to make an update, we're not going to do anything, we're just going to stay where we are. This classifier,  this point, is also correctly classified and we're just going to keep going until we hit a point that isn't correctly classified. So this point, also correctly classified, so nothing changes. Now this point is incorrectly classified, it's on the minus side of the line, but it's a plus, and so now we're actually going to make an update, the perceptron is going to make an update. And so we get a new classifier that's in blue and now we start from that when we play the same game: what about this point? It's correctly classified, so no update. What about this point? It's correctly classified, so no update. What about this point? It's correctly classified, so no update. Just keeps going until we hit something that's incorrectly classified and of course as we go farther along, more things are correctly classified and so we're not making too many updates, so let's let's skip ahead a little bit, let's skip a few steps ahead, let's say 10 steps. Now here's a really interesting thing.

You'll notice the error went up. So we went from a training error that was lower to a training error that was higher. That's a little weird, but it's a thing that can happen here because we haven't specifically said that we have to have a decrease in training error. Let's go a few more steps into the future.

Okay it's still up where it was. How about a few more steps? And now we're down to training error of 0 and you'll notice that this other learning algorithm that we tried out at the beginning still isn't at a training error of 0, it's still something that's non-zero. It's up here, but the perceptron, you know, around this iteration got to a training error 0, and as we saw before, once it hits the training error of 0, it's done, it's not going to be making any more updates, you know. It might check that if statement but that if statement will always, you know, be false. It will not be having any incorrect things and so it won't be making any changes. Okay so again I'll just pause for a second and see if there are any questions or clarifications or things that might be helpful to cover about this demo we've been going through.

I think you covered most of the questions for now, just yeah I guess a question could be: “how does the perceptron geometrically move based off of a given misclassification?” Yeah, yeah, and so okay. Let me, let me again use that question to come back to our slides for a second. So how does it move when we have a misclassification? And so let's head back to our slides here.

Oops let me share my slides and then head back to my slides. Okay.

Okay so this is what we had in the slides as the perceptron algorithm and honestly I think something that's useful, because you're going to be making your own perceptron algorithm in our problems, is to be looking at this at the same time that you're looking at it running and seeing, you know, what what exactly is happening here and making sure that everything makes sense there. But what you'll see in this algorithm is that when we make roughly a misclassification, when that if statement is true, then the update that we're going to make is exactly this theta update and this theta naught update, and so what's happening there in some sense is that we're trying to get the angles right between the thetas and the x's such that we're more likely to correctly classify this particular data point. Right, so this is what we saw in this “what does an update do” that we're making it so the left-hand side of the inequality in this if statement is more positive, and so we're hoping that it'll be positive enough that it'll actually be positive, that it'll be something where in the future when we hit this if statement, it will be false and so we won't have to make an update because this will be correct. So we're trying to move in that direction effectively. Now that being said, something we've already seen is that it doesn't have to strictly decrease the error. We saw this in the example that, you know, by moving in this direction we're hoping it's decreasing error, we're hoping that it's going to make a better judgment call in this data point. But one, this particular data point doesn't tell us about the other ones, we sort of have to deal with those in separate steps. But two, even in this data point that doesn't tell us it has to get it correctly, it just sort of tells us it's moving in that direction. And I think these are all things that we're thinking about, you know, the perceptron algorithm we're seeing has benefits over the existing learning algorithm that we talked about and we're going to see that even more concretely with a theorem, but it also might have minuses and we might try to do even better and so you know you keep both of them in mind. Great.

Okay so with that, unless there's anything else, I'm just going to go ahead and move on. And so in particular, what we're going to do now is we're going to try to answer some of these questions more concretely. So there's questions about how do we set tau, you know, how do we know about how the perceptron is performing, and in order to answer those questions, it'll help us to have a little bit of ability to talk about what makes sort of an easier and a harder learning problem, a data set that's easier and harder to find a linear classifier for. So let's talk a little bit about that, let's talk about classifier quality. Now the first idea that we're going to talk about is something that came up implicitly in these data sets that we were just looking at. You'll notice that they all had the setup such that you could find a line where all the pluses were on one side and all the minuses were on the other side. It seems to me that if you could find such a line if that exists, that that's probably an easier data set to find a linear classifier that's good, rather than if that weren't the case. And so let's give that a name. Let's say that a training set’s—this is going to be a property of a training set—we're going to say that training set is linearly separable if there exists a line (so a line is defined, our hyperplane is defined by theta and theta naught) such that for every point, we have this equation. Well, what does this equation mean? Remember we just sort of talked through this. If both of its components, both of its factors are positive, strictly positive, that means that the actual label was plus and the actual guess, or the guess, was plus and so they agree. Or another way to get the whole thing being positive is that they're both strictly negative: that the actual label is minus and our guess was minus. And so basically this, you know, equation is saying that we made a correct prediction and nothing was exactly on the line, and so that's why we have this strict greater than rather than the greater than or equal to. Okay, so let's look at an example. Here's an example, and so my question for you, for the private chat: is this a linearly separable data set that I am showing right here?

Getting lots of great answers here, really fantastic, keep them coming. Okay great, you're all you're all looking fantastic here. Yes, it is a linearly separable data set, and the way to show something is linearly separable is to just provide a proof of concept. Here is a line that linearly separates the pluses and the minuses. Once I have such a line, I have proved that this data set is linearly separable. Okay how about now, this data set: is this data set, this new data set on the right, is it linearly separable?

Great lots of fantastic answers here. I see some no’s, I see some nahs. These are all correct. A no with a frowny face. Yes this is, this is not a linearly separable data set. You can see that if I had a line that separated the pluses from the minuses, it would have to have, for these two points (this plus on one side and the minus on the other side), and there's just no line for which that's true, that doesn't also misclassify something else, and so this is not going to be a linearly separable data set. And again something that seems true about these is that somehow a linearly separable data set we think, you know, might be a bit easier for a learning algorithm to deal with, and a not linearly separable data set might be a little harder. Now in fact we can take this a bit further and just describe sort of a notion of like how linearly separable our data is in particular, you know: how much of a gap is there between the two sides of the data when it exists? And so that's what we're going to do on the next slide. Okay so it's time for some more math facts. Some math facts, which hopefully you enjoyed last time, and hopefully will enjoy this time. So the math facts that we're going to address today are: suppose I have a point x in my feature space and suppose I have a line, a hyperplane, in my feature space. I want to know what's the distance between that point and that line, and in particular what's the signed distance? So the sign is going to come from the fact that usually when we're dealing with hyperplanes, we're dealing with a direction. Because remember we're thinking about classifiers, we're thinking about direction as defined by theta. And so when that's true, what we want is effectively, if we just put our theta vector from our line to our x, we want to know how long it's going to be from that line to the point x. Okay and here's where we're going to use our math facts from last time. Now I'm going to do a little derivation here. In some sense all you could do is just take the end of the derivation out and use that for your own purposes and that's what we're going to do. But hopefully it's a little bit illuminating to see where this comes from. So another way that we can write this distance which I've now marked in green is: if just like before we put our theta vector at the origin and now we're looking at the same distance in green over here. Now one way to think about this signed distance in green is that it's the signed distance in purple minus the signed distance in orange and we know these distances. In particular, the purple value is exactly the projection of x* onto theta, which is something we talked about last time. The orange value is the signed distance of the line to the origin which is also something that we talked about last time. It's the projection of all of the points in that line on theta. Okay so I'm just going to cite some results from last time to fill this in. The projection of x* onto theta is exactly the scalar projection that again we talked about last time. So you can go back to the math facts from lecture one if this is if this is feeling new and then the signed distance of the line to the origin if you just play around with that b quantity from lecture one you can check that this is going to be negative theta naught over the distance in theta, or the size of theta. Now if we put these all together, we can say: what is the signed distance from a hyperplane to a point? It's exactly this formula. Again you could just use this formula out in the end if you're interested in the sign distance from a point to a line, but this is one way that you can think about getting it that uses the scalar projection ideas that we talked about in the last lecture.

Okay so now that we have this signed distance, again, what we want to do with this is we want to say a notion of how big of a gap is there, somehow, in our data when we have this linear separability. And so here, we're going to introduce another new concept, another new idea: the margin of a labeled point with respect to a hyperplane. So a labeled point is just a particular point, just a particular point in the feature space, but now with a label. So let's say that that feature value is x* and its label is y* and our hyperplane is defined by theta and theta naught and the margin is going to be the signed distance from the hyperplane to the point but times the label y*. Okay, so why is this important? Well this contains the information of this thing we've been talking about where we multiply y* by theta transpose x plus theta naught. So if y* is positive and theta transpose x plus theta naught is positive, our actual is positive and our guess is positive, and everything agrees and we're getting our guess right. If y* is negative and theta transpose x plus theta naught is negative, then our actual is negative and our guess is negative and so we're still getting our guess right. We're still predicting right and so the sign of the margin is telling us “is our guess right,” and the size of the margin is telling us the distance between the hyperplane and the point.

Okay so let's look at an example. Here's a big data set. It seems like there's a big gap between the pluses and the minuses and so in this case we think there should be a big margin, but let's let's first look at the margin between a particular, between a particular line and a particular point. So that's what we're looking at here. So this is a particular line defined by theta and theta naught and let's look at a particular point. Here's a point, it's just a point that I've circled. What's the margin? Well the point is labeled minus and our prediction is minus and so the margin is going to be positive and this is the distance and so it's going to be the positive value of the distance. Okay so now we still want to define this notion of a gap in the data and so now let's talk about the margin of the whole training set with respect to a hyperplane,not just one particular point, but the whole training set all of the different points. And so what we're going to do for that is we're going to say: “let's take all of the margins of the individual points and find which one is smallest.” So what we're doing here is we're saying, for each point, we calculate its margin. So that's from point one, point two, point three, up to point n, because remember i indexes the points. And then we take the minimum value. So this is different from the argmin. When we talked about the argmin we said: “which value of i indexes the minimum value?” When we take the min, we're taking the actual value, the actual smallest value. Okay so let's think about what's going on here. So if any of these points are misclassified, any at all, then the margin for that particular point will be negative. Right? So the minimum, then, over the whole data set must be negative. So if we get absolutely any point wrong at all, the margin of the training set with respect to this hyperplane is going to be negative. But, if absolutely every point is correctly classified, then the margin will be positive. And so we can see that here, so if we look at this particular data set again over under the “math facts”. On this example data set, in x_1 and x_2, you can see that for this particular line, every data point is correctly classified: all the pluses are on the side in the direction of the normal, all the minuses are on the other side, and so the margin is going to be a positive number, and it's going to be the distance to the closest point. This looks like it's the closest point to me, and so this is going to be our margin, the size of that distance. Okay, so this is sort of a useful way to describe, again, sort of how difficult a problem is. So if the margin is positive, we have this, you know, linear separability as shown by this particular theta, theta naught, and we're able to say, sort of, how far away the points are from that line. And so now that we have this ability to talk about margin and linear separability and things like that, we can have a theorem about perceptron performance. So let me just maybe pause here for a second to see if there's anything we should discuss and then I'll go on to this theorem. Yeah just a quick question about the perceptron update: why do we go through the data points one at a time and not necessarily in batches, for example? Yeah that's an interesting idea. So there's two ways that you can answer this question. So one is, you know, that you would have a different algorithm if you went through the data points in batches. Which isn't to say that's a bad idea, it's just to say that it's not this particular algorithm, and so one way to notice that is to notice if you make an update on a particular point, then the next point will clearly depend on having made that update, and so if you did something with batches, you would change the sequential nature of those updates. So again one way to answer this question is just that if I used a batched version, it would just be a different algorithm than the perceptron. Another answer is why don't we do this, because maybe we think it's better. And I think that that's an interesting question that I'm not going to answer right now, but I'm hoping that we're developing the tools to help you answer, to start thinking about how would I know that an algorithm is better? How would I would evaluate that it's better and so you can try out different algorithms and maybe they are better, maybe they are things that you would, you know, like to do in general. I think I see a raised hand again. I want to encourage people to write all of your questions in the Discourse, and then some of them will make it back here. Great thanks. Okay another question. Oh yes, please. All right, another question is: why do we look at the signed distance from the hyperplane to x instead of the other way around? Oh you can do distance in, you know, either way. I think what really matters in all of this is that we want to ask: are we correctly classifying points? So this notion of the margin, and then how far away are those correctly classified points? And so I think it's not so important, you know, what we choose as the directional. Although note that you would then have to, if you, change sort of the sign in one direction, you'd have to think about what y matches with that. So one thing that's nice about this is the y being positive matches with theta transpose x* plus theta naught being positive. So the guess being positive matches with the actual being positive when those two are positive, that we're correctly classifying things. So we want those to agree for everything that we're setting up here, but you could imagine just flipping both signs simultaneously, you just want to make sure that they agree is the most important part. Cool.

Okay great so let's go on to this next point here. Now let's care about trying to figure out if we can say something about perceptron performance, something a bit more concrete, and we would hope that because we're, you know, we're making these steps and these hopefully good directions, can we, you know, say maybe how many steps is it going to take? And this is what this theorem is going to to point us in the direction of.

Okay so there are going to be some assumptions in this theorem and I think it's always worth asking, you know, are these reasonable assumptions? The first assumption seems pretty strong on the face of it. The first assumption is that our hypothesis class is not just all of the linear classifiers, it's all going to be all the linear classifiers with separating hyperplanes that pass through the origin. So we're only considering linear classifiers, that's what we've been doing so far, that's fine. But now, we're only considering the linear classifiers where theta naught equals zero, and so, for instance, you know, here are some examples. Here's an example, here's an example, here's an example. These are all examples that pass to the origin but that's more limiting. I mean there are some data sets that could be separated by a linear classifier that could not be separated by a linear classifier passing through the origin, and so on the face of it, it seems like this is a pretty strong assumption. Although, you know, if you've checked out the reading, and we'll talk about this, you know, momentarily, it turns out this is not actually a very strong assumption and that we can always reduce to this case and so this is actually okay, but I think you want to be convinced of that. Okay, B: what is assumption B? We're assuming that there exists a theta* and a gamma. So theta* because we're only looking at linear classifiers for which theta naught is zero, theta star completely defines our linear classifier. Now so we're essentially saying there exists some hyperplane, some directed hyperplane, and there exists some gamma greater than zero such that, for every data point, we have that this quantity is greater than gamma. So hopefully you remember from the previous slide that this quantity is exactly the margin. The reason that it's changed slightly, the only thing that's changed from the previous slide, is that theta naught equals to zero, but otherwise this is the margin between a hyperplane and a data point. And we're saying that that margin is always greater than gamma and in particular this tells us that the margin of the whole data set if we look over all the different data points is greater than gamma. So what we're saying here is that there must be a line such that the data is linearly separable, everything is correctly classified, and there's some sufficient gap in it, that's that gamma. So for instance, if we look at this data set down here, here is a line. I have found a line, it is given by my theta*, and here is a gamma. So that's all I have to provide. It doesn't have to be the best line, it doesn't have to be the best gamma, but I have to find a line and a positive gamma such that this assumption is satisfied and I have done that here for this data set. Okay our third assumption: there exists some R—so this is going to be another real value—there exists some R such that, for every data index i, essentially the magnitude of the data point is less than or equal to R. So again, it doesn't have to be the best R, it just has to be some R. So here I put all of my data in a ball of radius R, and here's my R, so I found some R for this data.

So somehow this is just telling me how big does, how far does the data get out from the origin. Okay so once I've made these assumptions, so namely I'm only looking at hypotheses that are linear classifiers to the origin, I'm looking at data that can be separated with a non-zero gap in between data points, and I'm looking at data that all fits into a ball somehow. Then I can conclude that there is a maximum number of steps, of updates, that the perceptron algorithm will make. It will make it most (R/gamma)^2 updates to theta. Now remember there's a difference between steps and updates. So the way we were talking about steps before it was like every time we visited that if statement we're looking at a step. An update is when we actually go into the if statement and change our theta and in this case, theta, because it's only, you know, through the origin, but in general theta and theta naught. And once it goes through a pass of all the data points without any changes, the training error of the final hypothesis is going to be zero.

Okay so here's something that's pretty cool about this, is it's telling us some notion of a maximum number of updates that we have to make. So we know that there is a bound on the number of updates if we can check these things, if we can check this margin, if we can check this R, and so that's kind of exciting. That kind of tells us, you know, “hey, there's only so far we can go, you know.” There's really this limit as opposed to that earlier learning algorithm with the random hypotheses or whatever hypotheses our friend gave us in which case, like you know, we don't really know that we would ever hit training error zero even when we can, even when things are really linearly separable, and we should be able to. Now, in general, it's worth noting that if I set—then this comes back to the question of the number of iterations—if I set my number of iterations such that I can't make all of these updates, if I said it's very small, I am not guaranteed that my training error will be zero. There's a really easy version of this: set your number of iterations to zero or one and you know, in general, you certainly can't guarantee that your training error will be zero at that point. So this does tell you some sense, you know, how that you're gonna have to have a sufficient number of iterations in order to be able to get to that point.

Okay so this is cool. We have we have a theorem about perceptron performance and again I'll just pause for just a second to see if there are any questions about this theorem.

Or anything so far.

I don't think so. Cool, great, awesome. Okay so now what I'd like to talk about is, you know, whether this theorem is everything, in some sense, and before I get to that actually, I should talk about why classifiers through the origin. So let me backtrack just a second. So remember this first assumption was that our hypothesis class were classifiers through the origin and an important point about this is, again, these are just not all the classifiers. I think you can easily imagine data sets that are linearly separable—in fact, I think we've seen them so far in this class—but are not linearly separable with classifiers through the origin. You know, they have pluses on one side and minuses on one side but that's not true strictly with a classifier through the origin. If that's not immediately clear, definitely meditate on it, maybe draw yourself a picture, but that's certainly true. Now that being said, what I'm about to go through is a derivation, a quick derivation, that you can always reduce to the case of classifiers to the origin. You can always reframe your problem such that you can look at classifiers to the origin, that's sort of a different observation. So we're not saying that everything is linearly separable through the origin but we're saying that there exists another space in which it is, but we have to go to that other space first. Okay so why is it okay to look at classifiers through the origin? Well the observation here is that if we're clever, if we do work in a different space, we don't lose any flexibility. Okay so let's think about this. So first let's think about a classifier with an offset. So remember we're thinking of theta as our normal vector for our hyperplane and theta naught as our offset and so in general this is the setup that we have for our linear classifiers: we have, we look at all of the x's in some feature space R^d, we look at all the thetas (they have to have the same dimension as we talked about really early in today's lecture so that we can take this dot product, so that it even makes sense to take this dot product), and then theta naught is going to be a scalar. And we said that our line that defines the linear classifier is defined by, it's the set of x, that satisfy the equation theta transpose x plus theta naught equals zero. So this is sort of how we defined a line, we talked about this in lecture one, and we talked about what this looks like. Okay so this is our classifier, it's defined by this line and the normal vector theta.

And now we want to say is there some way, some space in which we could work that would be equivalent that would involve classifiers without offsets? And so the answer is yes and so let's let's just double check what that would look like. So basically we're going to work in a new space, let's call it new and so for each x in our original space, the one that we sort of really care about, the one where, you know, maybe we actually measured things about newborns like, you know, how much oxygen they're getting and how much movement they're making and so on,  we're gonna make a new space and we're gonna change the original space very simply. We're just gonna take our x_new here, we're going to take our original data point x_1, x_2, up to x_d and we're just going to append “1”. That's it, it's always going to be one. Every data point is going to end with 1 at the end. So that's sort of our expanded feature vector, it's like every data point got a new feature and that feature was 1. That seems like incredibly simple, like there's not too much going on here, and that's kind of true, but it's still a really nice and clever trick here. And so now we're gonna have a new parameter vector, theta_new, and what theta_new is gonna be is it's going to be our original theta (theta_1, theta_2, up to theta_d) and then we add a theta_0 on at the end, and the thing to notice is that if you take the transpose of theta_new with x_new, you get back exactly theta transpose x plus theta naught. And so what I'm saying here is let's look at the set of x's in the new space, x_new, and let's look at their first d coordinates. So that's what I mean by 1:d. I mean the first d coordinates of this x.

I think these should all have transposes to be column vectors, but modulo that small thing, then then I think we're going right here. Okay so now, what I want to do is I want to look at the set of x_new and their first d coordinates such that x transpose new times x new equals 0. That's going to be exactly the same set of x that satisfy the first linear equation and so we can get out basically that set of x from this expanded space. And so something to notice, remember, is that it's not just a line that defines a classifier, it's the two sides of the line, you know, it tells us do we when do we classify plus and what do we classify minus, and so it's also true that the set of x that are less than zero in the first case or the set of x_new 1 through d that satisfy less than zero in the second case same thing if we look at greater than zero and so we're really going to get the same labels, the same plus and minus and all of that. And so the proposal here is that if, let's suppose, we want to apply our perceptron theorem which we've just developed, well first you can convert to this expanded feature space and so if something was linearly separable in the original feature space, it's going to be linearly separable in the expanded feature space, but now this expanded feature space we're really just looking at classifiers without offsets and so it's not only going to be linear separable, it's going to be linearly separable with classifiers without offsets, so that's something sort of stronger, but it will be in this expanded feature space and then we can apply the theorem. And so this is all to say that we didn't, that that first assumption wasn't so strong after all, that really we can still apply things, we can still do all this in sort of, you know, the full linear space. We just have to use this trick of using an expanded feature space which will turn out to be an extremely important trick that we're going to spend a lot more time on in more generality, but I want to mention it in this sort of simple version here.

Okay so now we know about the perceptron theorem, and now I want to talk about limitations of the perceptron theorem.

So here's a big problem: a lot of real data is not linearly separable. So we've been talking about, you know, here's what you can do and here's what you can say about linearly separable data, but what if your data isn't linearly separable? So here is a real data set about penguins, which is adorable. So here are three penguin species, these are penguin species. I'm not a penguin expert so I’m, you know, borrowing this graph from the link at the bottom, but something you'll notice is that individuals have measured the flipper length of individual penguins (so that's on the the horizontal axis) and body mass of the individual penguins (that's on the vertical axis) and then what you're seeing is that for each flipper length and body mass—so that's like our x—that you get a particular type of penguin plotted as a color, it's like a label, and so even if we were doing two class classification, just looking at two classes at a time, like maybe we're just comparing the chin strap of the gentoo penguins or you know any two penguins, you'll see that no class two classes of penguins are linearly separable here, there's always some overlap in all of these data sets. And so if we wanted to classify this it seems like we can't apply our theorem because things aren't linearly separable. And so in fact let's look at what happens to our, to our perceptron when things aren't linearly separable.

And again we'll just do that.

With our example.

Okay, so here we're looking at a data set, just like before, but now you can immediately see this data set is not linearly separable. Hopefully you can immediately see that. So we have our pluses, we have our minuses, and they're jumbled together, and yet, you kind of have a sense that you could get a reasonably good classifier, like something that doesn't misclassify too many points, like the pluses tend to be on one side, the minuses tend to be on the other side, and so we'd still like to find a good classifier for this. Now in this case, I'm not going to bother to calculate too many of the random training algorithm, although I think you can think about how that might perform, but I am going to focus on the perceptron. So here the perceptron’s still going to do the same thing as usual: on the first point it's going to just make an update, no matter what. The first time it always makes an update and we get a classifier, and now each time it goes to a point and it says “is there a mistake?” In this case there is and so it's going to make an update and it's going to go to a new point, it's going to say “is this incorrect for the current classifier?” In this case, it's actually correct for the current classifier, so it won't make an update and it'll just keep going forward and so now let's go a bunch of steps forward.

So I've gone a total of 352 steps forward, so we've made 352 steps. It's a lot of steps, we'd like to think that we're going to get to a pretty good classifier after that many steps and you can see that the error which started around, maybe you know, 0.4 or 0.3 or so on actually went up and then it went down, but then it went up again, and now we're actually at a pretty high error, 0.41, and it seems like we're actually really struggling on this data set, you know, that we're not doing too well. We think that, you know, we could do better than an error of 0.41. Yes some of the points are kind of interspersed but it still seems to me like we should be able to do better and also, in particular, that like maybe we should at least choose the best of the classifiers we've tried so far instead of just this last one which just happened to be not that great. And so something that we're seeing here is this can be a problem with something like the perceptron: that if I don't have linearly separable data, maybe I need to ask myself, “can I do better? Are there better things that I could do?” Okay, before I step forward, I'll just check briefly if there are any questions about this.

It's cool, great okay. Let's go back to the slides then.

Okay so we've seen that the perceptron has certain advantages over, you know, this not so great learning algorithm that we started with, but it has disadvantages too. It seems like maybe it struggles with this not linearly separable data that we do care about and so we're going to ask what can we do? And in some sense that is a question that you will be asking and answering on your upcoming problems, on our next few lectures, and so this is, if anything, the cliffhanger to get us to be thinking about this, to be saying, in some sense, you know, you always want to be asking, you know, why do we have any more lectures? Why don't we just stop at this lecture? Why isn't everything solved? And here we're seeing, you know, that we've stepped in some interesting directions where we're able to make some some better choices than we did before, like maybe update our classifiers, our hypotheses adaptively, and yet maybe there are yet better choices that we can make. Maybe there are things that we could do even better and we're starting to see that here.

Okay so before I go, before I finish up for today, I just want to kind of situate where we are in a sort of pantheon of machine learning tasks. So something that we've been focusing on so far is binary or two class classification. That's where we learn this mapping, this hypothesis that takes us from our features to a set of labels which are—there's just two of them, it's minus and plus. Now we've been looking at the example of linear classification but that's certainly not the only example that we might look at. I mean you can imagine data sets. So here, I'm imagining, you know, maybe my features are only one dimensional but it seems like a linear classifier isn't enough here, I'd like to do more than that. So linear classification is an example of binary or two-class classification, but it's not the only type of binary or two-class classification. Likewise, we just saw an example of multi-class classification. This is where I have more than two label values. In the example we just saw, there were more than two types of penguins. We'd like to probably classify all the different types of penguins, and so I might be interested in three or more label values. Now these are both examples of classification. So in classification, I'm learning a mapping to a discrete set that those labels, you know, the different types of penguins or, you know, whether a newborn is having a seizure or not, but sometimes I don't want my mapping to just be a discrete set. Sometimes I want it to be something continuous and that's where we look at something like regression. So here I want to learn a mapping to continuous values like maybe, you know, how long my car is going to last, that's a continuous value, it's a length of time. You know, maybe I want to predict the temperature outside, that's a continuous value. There are a lot of things that I might want to predict that aren't simply a discrete set and that would be a regression. Now both regression and classification are examples where we have labeled training data: we're learning a mapping from a set of features to a set of labels, those labels could be discrete, they could be a set of penguins, different types of penguins, or they could be continuous, but they're still labels. We're still taking each feature x and labeling it with some y but that's not the only type of learning that we might be interested in doing, there's also unsupervised learning. In this case, there are no labels and we're just trying to find patterns. So if you look back to the examples from lecture one, we actually talked about an example of unsupervised learning. This was in the Reuters analysis: they took a bunch of text documents, these were these petitions before the court and they said “oh what are the topics in these documents?” And there were no labels there, they just wanted to find out what are words that goes together, what are what are themes in the documents? They don't tell them what are the themes to begin with and so that would be an example of unsupervised learning. So this is all to say we're focusing on classification right now, we're focusing on two-class classification, we're focusing on linear classification, but that's part of a broader picture in machine learning and hopefully we'll spend some time on all of these other ideas, but first let's really, you know, nail classification and figure out what's going there. Okay great, so that's the end of lecture two today and we look forward to seeing you in labs and office hours and everything else and I’ll see you for the next lecture next Tuesday.
