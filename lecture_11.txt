Okay, good morning, it's about that time. So let's just recall what we've been doing in the past couple of lectures. So first, we developed these ideas of state machines and Markov decision processes to see and model how we interact with the world and how that can change the world and how, in particular, having different inputs can change states in the world and, really, in the past two lectures, we've been focusing on using that information to choose a best set of actions in the world. So on one hand, we said if we know everything about how the world operates (we think it's an MDP and we know it's transition model and we know its reward function), then we can just use value iteration to find the best set of steps that we would take in this world, the best policy. And if we don't know those things and we have to muddle around and figure out what's going on, we can use Q-learning. We learned about that last time and today we're going to see that we can use these ideas of state machines and states changing in ways that aren't just what we've done in the past few lectures which is basically reinforcement learning. We'll talk about this on the next slide a little bit more but we can actually use it in supervised learning as well and so we'll see how that can arise with sequential data. We'll look at an example with text prediction and we'll basically define a recurrent neural net. Okay, so first, though, let's again just spend a little time situating what we've done in the past couple of lectures. So, in particular, we didn't explicitly get to saying it, but now let's explicitly say it in the last lecture: we did some reinforcement learning and the idea of reinforcement learning is that we're learning to maximize rewards by interacting with the world. And so this is different from what we saw in supervised learning, which is basically what we've been looking at the whole time beforehand. There are some similarities: you can think of a negative loss as being a reward, so it's not like the concept of reward is totally not there in supervised learning, but in reinforcement learning our actions affect both the reward but also even our ability to observe the environment. Only by taking certain actions might we even be able to see certain parts of what's going on. Our action this round can affect future rewards via the state and so this idea of interacting while we're doing the learning is very different in reinforcement learning instead of unsupervised learning. Just as a few examples too, let's just talk about when you would use reinforcement learning. The idea is that we would learn by doing and so we had this extended example in the past couple of lectures: if we have a farm and by working on the farm we learn about the soil qualities, how we change them, and what rewards we get. In the beginning, we make a lot of mistakes, but over time we become farm experts and then we get really good at it. And this is the thing that you see in a lot of other applications, this kind of theme. A really, really big one in reinforcement learning is gameplay: you start playing a new game like chess or go or World of Warcraft or whatever is your favorite game or game you'd like to try out and in the beginning, you're going to flounder about—you're not going to be very good at it, you're going to make a lot of mistakes—but you learn by playing it and then you get really good at the game over time and so the idea here is to automate that process. You could also see this in digital marketing, for better or for worse depending on how you feel about digital marketing. So multiple interactions with a potential customer online, perhaps using cookies or something like that, you might ask is there a best policy to get them to do what you want if you're a marketer, to get them to sign up for an email list or buy a product or something else? There's a lot of excitement about using these ideas for other things like automating robot tasks in industrial settings or manufacturing settings, self-driving cars, etc and you can see a little bit why there would be a lot of research in this area because as soon as we start thinking about these other applications, maybe even medical applications, you don't want to make a lot of mistakes in the beginning, you don't want to learn about what happens when you crash your car by crashing your car or what happens when you nosedive your plane by nose diving your plane and so there are issues of safety: how do we adapt this to have a lot of safety? There are issues of making sure that you don't need so, so many training data points or like rounds of learning to get somewhere. So those are some challenges that arise there but I just wanted to take that time to contrast with supervised learning. Something we also saw in the last lecture was model-based reinforcement learning versus model-free reinforcement learning. So in model-based reinforcement learning, that's where we use an explicit conception of the next state and the reward given the current state and the action and so, in particular, we explicitly say we have this transition model and we have this reward function and we are estimating them, whereas in model-free reinforcement learning, we are not doing that, we are just saying, “hey, we'll just do this Q-learning thing, for instance, that we saw that did not explicitly put together a T or an R but was still able to learn a good policy.” Now I want to emphasize something here: this is not the usage of the word model that we would use, probably, in the rest of machine learning, certainly in supervised learning. So this usage of the word model representing the transition model and the reward function is pretty specific to reinforcement learning and so I'll just just point that out. Okay so model-based reinforcement learning and model-free reinforcement learning are examples of reinforcement learning and, in turn, Q-learning was an example of model-free reinforcement learning. So we didn't instantiate T, we didn't instantiate R, you could consider estimating them but that's not what happens in Q-learning, you don't explicitly do that and you are able to get this estimate of the Q* function. So contrast Q-learning where we're trying to get an estimate Q with the actual Q* function. So what the actual Q* function would be if we knew the transition model, if we knew the reward, then we could say what is the expected reward of starting at a state s, making an action a, and then making the best action ever after and we saw that we could get this optimal policy by having this and so what we're doing with Q-learning is we're doing this estimate Q instead so that we can estimate or come up with an idea of a best policy but it isn't necessarily going to be the best one so long as Q is not exactly equal to Q* and, in some sense, what we're trying to do is get as close to Q* as possible. And again, you want to contrast this, also, with value iteration which we had discussed in the previous class. There, we assumed we knew the transition model T, we assumed that we knew the reward function R, and we just wanted to find the best plan, the best policy, going forward and so we didn't have to muck around, we didn't have to try out different things and see how they worked, we could just start from the very beginning by doing the best thing possible and that was value iteration. And then separately from that, so that was just getting the exact Q*. Even before that, we talked about how to get V, the value of a policy, and that could be still really useful. So Q* can let us get the value of the optimal policy, but what if your friend comes along and says, “hey, I've got another policy and it's just an easier policy and so if it's almost as good, maybe we should just use my policy, maybe I've already got the equipment for it or something like that” and she'd still like to evaluate what is the value of that policy and if there's a big difference, then maybe you're going to keep going with the exact optimal policy and maybe if there's a small difference, there might be other reasons that you prefer a different policy. Okay so this situates what we've been doing the last couple of classes and this class, now, we're going to see that what distinguishes reinforcement learning from supervised learning is not just having a state transition model, one of these state machines, it's not having an MDP, it's that you're actually doing this learning by interacting with the world and so we're going to see an example today of supervised learning where you do, in fact, have a state based model and so we are going to see that you can have these recursive properties, you can have sequences, and not be exactly doing reinforcement. Okay so our motivating example today will be in text predictions. So you see this all the time in your lives: if I go to compose an email in Gmail these days, it will help me by suggesting some additional characters that I could append after what I've written so far and I could just accept those and that will save me seconds of my time. If you go into some kind of search engine, many search engines, this will be true. Here's just an example from Wikipedia: if I start typing out what I want to look up, then it will make some suggestions about what I might be looking up and some of those are literally just completions of what I've written so far. Interestingly, some of them are other things, but clearly the first one here is an autocomplete of the word autocomplete. Okay, now for us, this is a convenience, it's nice to save a couple of seconds. It's actually a big deal in some other individuals' lives. So famously, Stephen Hawking, especially towards the end of his life, communicated by pressing a button and then eventually just moving a cheek muscle and so for him, word prediction was a huge deal. If you could only type a few words at a time very, very slowly, having the ability to automate a few words in advance can be huge and of course it's not just Stephen Hawking, there are a number of people who face severe motor impairments: Dawn Faizey-Webster, pictured here, amazingly completed, much more recently, both undergraduate and graduate degrees with assistive technology and merely by blinking. So I think this is a super cool and interesting area, we actually do some research in my group on assistive technology for individuals who have these extreme motor impairments, but in general, this is a reason to really care about text prediction and getting it right. Okay, so we're gonna look at text prediction or at least a simplified version of it today. And here, this is essentially supervised learning. What we'd like to do is we'd like to say, “hey, I've seen some text that a person has written so far and I want to predict the next text that they would write.” And we're going to do a very simplified version of that: we're going to say, let's just for the moment, predict the next character and you'll see in in the lab and other places how you might get beyond just that next character. Okay so how do we get training data? Well luckily, nowadays, there's just a huge amount of text online. You could just take all the text from Wikipedia, you can take all the text from your favorite source, let's say we took some poetry text, and so here's an example of some text that we might use in our training data and how would this help us train? Well, we can take each set of texts that we might have seen and try to predict the next character. So in this training data, one bit that we would have seen if somebody was typing this in order would be “w” and then we'd have to predict the next character and the answer, the exact answer, the true label, would be “h”. We also see, at some point in the course of writing this, that somebody has written “wh” and then the next character which we would like to predict would be “a”. Also in the course of writing this, somebody would have written “wha” and then the next character that we would like to predict would be “t”. So we actually get quite a lot of training data, in some sense, from this one sentence, from this one string of characters and now we have to think a little bit harder. The devil's always in the details of these things. We have to think a little bit harder about how does this exactly fit into our frameworks that we've developed for supervised learning? Okay, well it looks like a classification problem: I'm trying to predict a character and here I have 26 different possible English language letters and maybe an underscore for the space, so I'm, in some sense, trying to do classification with 27 classes at least as described here. Okay, but now how do I featurize this? This is a little bit tricky. So we had this nice lecture way back in lecture three of “oh yeah, I get my data and I have to turn it into features that I feed to my algorithm” and so this is essentially the question that we're asking here: here's my data, how do I turn it into the features that I really feed to my algorithm? Well one idea is why don't I just use the things that are in this column labeled features? I mean that sounds so convenient. Am I done? So just use all the previous characters, you could call this the context for the next character that comes along. Okay well let's think about what do the algorithms that we've developed so far take? Well, they take real values but that's not so bad, we know how to turn characters into one hot encoding and then that's gonna be a set of real numbers so that's okay. Maybe a little bit different, though, is that they take a fixed collection of real values, a fixed dimension collection of real values. So far we've said that if I have some features to my algorithm, whether it's logistic regression or neural nets or whatever is my favorite classifier, that each of my feature vectors has to be in R^d. So we said R isn't so bad because I can turn these into real values with one hot encoding or whatever my favorite encoding is, but the fixed dimension d is challenging here because you can see that we change dimension quite a bit: we have one character in the beginning, then we have two characters, and we have three characters and so it seems like we're not really keeping that fixed and so we'll have to think about that.

Okay so here's another idea: I just use the last character. So I predict an “h” from a “w”, I predict the “a” from the “h”, I predict the “t” from the “a”, and you kind of see that I'm probably losing some information from that, predicting what is the next character after “happ”, you probably have a good idea that it's pretty likely to be “e”, somebody's probably spelling “happen” or “happening” or something like that. But if I just predict after “p”, I might have some pretty different predictions, it might not be quite the same so you can see that having more characters potentially helps you out. Okay, but we can't have all the characters: well, one, that's gonna be really hard to learn, you're not gonna have every possible sequence of characters, but you have this issue of fixed dimension as well. And so let's, for the moment, say that we're going to trade this off by looking at m characters where m is one or greater but we're not quite sure what it is just yet and then we'll see if there's anything else we can do that might help us with this fact that we don't know quite how much information we need to have. Okay so we're gonna use the last m characters as our features and we're still gonna have to keep digging a little bit deeper because there's still some details that we haven't quite ironed out here and how this is going to work.

Now something that'll, maybe, help us think about how to do these things is that we can express what we're doing as a state machine and so what I mean by that is there's something inherently recursive about the choices that we're making. So in our state machine, suppose that we're trying to predict the next letter here given that we've so far seen “wha”. So this context, these last three letters, so let's just say that our context here will be the last three letters, that could be our state. So at some point, we have our state: at this point, it's “wha”. Now maybe, the person on their phone or with their assistive technology or whatever, types another letter, maybe it happened to be “t” and so what happens is we're going to want to update the state. Now, the last three letters aren't “wha”, now the last three letters are “hat” and so we can think of that as the input “t” coming in. This is the letter “t”, not the index t in this particular case. And then we're updating the state to be “hat”. You can imagine this keeps going, so now the person types a space and we want to update the last three letters to be “at[space]” and so here our input is the space and we update our state to be “at[space]”. And you can imagine that this just keeps going and you can keep doing this as you go along, as you read this sentence. Okay so let's just double check that this fits everything that we know about state machines. It's been a couple of lectures now, so let's just review what was a state machine. Well in a state machine, we had to say what was the set of possible states. So in here, the set of possible states should be all m characters in a particular order. So here m is three, so any state that is three characters in a row is an acceptable state, that is a possible state. What are the set of possible inputs? Well my inputs are going to be whatever is the next character, so any character is an acceptable input among my vocabulary of characters. Okay initial state, this one's a little bit tricky: how do I initialize my state? Well one option is I could just initialize from the first three letters in this case. Another option is you can imagine somebody starts up their assistive technology program or they start their phone and you might want to also already start predicting things for them. Maybe they just want to type “the” and that's a pretty easy prediction to make and so you'd like to start from the very beginning and so for that reason, sometimes people like to include something called a start character. So it's a special character that just indicates somebody's about to be typing and you would see this in general, that somebody opens up a texting app or something like that, that's your indication that they're about to be typing. And so if we did include this start character in our sentence (in this case, I'm just using a carrot, it could be any character that's not what I'm already using), then a natural thing that we might do is we might have our initial state just be m start characters. And so what would this look like? Well we could start with our initial state being, in this case, three start characters. We're gonna update with the first character we see that's a start character and so at this point we're gonna make our first prediction and that's fine, because we want to predict what is somebody gonna type without having seen anything yet and this is what this represents. Okay next, we see a w and now we're saying, “hey, we want to predict, given that we have nothing typed so far, except a single ‘w’.”

Now we have our h that we've seen, and now we want to predict, given that we've seen nothing so far except “wh” and then going forward from here, we're going to start having actually three characters in the context that are not just the start character that we can predict from.

Okay so the next part—so let's just fill in now, this t is the time step, hopefully it's clear the difference between the uses of the character t and the time step t—now we're going to talk about our transition function. So suppose that we have a particular state that we're in and we make a new observation, we'll make a new input, we have a new input: how do we update the state? Well hopefully it's straightforward to see at this point that, basically, what we're doing is we're saying what were the last m characters and we're updating to the new last time characters. So we're taking whatever the x_t value was and putting it at the front or the back, I guess, depending on how you're thinking about this ordering but we're appending it and then we're getting rid of the older character that we don't need anymore. So we can clearly get this as a function from our existing state and the new character that we've gotten. Okay, now we have to talk about what are our outputs. So one output that we could have is just a pure prediction of what is the next letter, like I see a “w” and I think the next letter is going to be an “e” but you can see that if you have a “w”, maybe the next letter is an “e”, maybe it's an “h”, maybe it's an “a”, and so a really useful thing here would be to have a probability distribution over the different letters. And this is something we're familiar with from classification: think about soft max for multi-class logistic regression that gives you a probability distribution over the different classes that you're predicting and so this is something that we know how to do and so instead of just a single letter, something we might output is a vector and now I'm using abbreviations to fit everything in here, but what I mean is a vector of character probabilities. So for each possible character, for “a”, for “b”, for “d”, for “e”, for “f”, for the space, etc, we're going to say what's the probability that that is the next character? So again, this is something that we'd really naturally do with our multi-class logistic regression, using the soft max function instead of just the usual logistic sigmoid and so at least, in theory, we know how to do this and we'll nail it down in just a second. Okay and so our output function, g(s), will then be, exactly, a multi-class linear classifier. So s, you can think of the state as being the features to that linear classifier. So what's been useful about expressing this as a state machine is now we have a fixed dimensional set of features that are going into our classifier and those are exactly the elements of s. And so we can take those in as features and then we can output, again, this probability over all of our classes, all the potential classes, this vector of character probabilities and so this is our state machine, this is an almost totally well-defined state machine. I mean we're still talking at a high level, we're gonna get to some equations in a moment, but this is how we can fit things into a state machine and how state machine can let us come up with a nice notion or a nice particular way of getting our features for this problem, for specifying what are our features in each classification case that we have. Now, just a little note here: it's a little bit different from how we've talked about x when we were talking about classification problems before is, okay, well we have this input x, but then we have x_1 and x_2 and x_3 and x_4 as subscripts as we go along within a single sentence, within a single string, and so we might want to collect everything in that string as one string that we're feeding into this state machine and so we can say that this is our x^(1). So the superscript is denoting that we have this full string and then if we do a subscript, we're indexing individual characters within this string and so, of course, if we were doing some kind of supervised learning, some kind of training on this, and we were trying to learn predictions of these next characters, we wouldn't have just a single string, we'd have lots of strings and so here's just an example of three of them that we might have and of course we'd have so many more and we'd probably look at look at quite a lot of text data for this.

Okay so this is still a high level description of this state machine. Let's do one more high level description before we get into the exact equations that we might use to encode this. In particular, let's go back to our function graph representations. So can we do a function graph to express what's going on with this state machine? So it's going to be a little bit different from how we've looked at state machines before: before we were looking at states and going in between states, now we're going to say the function graph for how we apply f and g and f and g recursively.

Okay so what is this function graph going to look like? Well first, I have my input. Sorry first, I have my starting state, the initial state. This is, in this case, a bunch of starting characters before I've gotten anything else and now I have my first input, this is the first character I see. In this case, it's a starting character: it says somebody opened up their text application and they're about to start writing something. Okay so I'm going to put those in together into my transition function.

Incidentally, what I'm describing here, in some sense, is just a state machine, like I'm illustrating it with this particular application in predicting text like the next character, but in some sense, this illustration, this function graph is a pretty generic state machine. Okay so once I have these, once I have the input and my old state, this transition function updates me to the new state and then that new state, I can use in my output function to get my actual output. Now previously, when we talked about state machines, we called this output y, but here we're going to call it p for prediction because we want to reserve y for the actual ground truth label which is what we had done back when we were talking about classification and before we had gotten into all this state machine stuff and now we have this unfortunate overloading of the terminology, so let's use p for prediction, but it's the output of our state machine and then y will reserve for what is the true label here which is the thing the person actually wrote next. Okay so once I have this new state, I can put it, again, into my state machine with my next input. Those together will go into my transition function which will output the next state and then that will have some prediction for the letters I've seen so far: what is the letter that I want to say is next? So if the character that I want to say is next and then this goes on and on. As you can imagine, this just going on-off the page, hence the dot, dot, dot. That just means that we recurse again and again and again.

Okay and so here, a couple of numbers that are going to come up in our examples here. So we said m is going to be the number of characters in our context. So this is the size of s right now, it's the number of characters we're looking at when we want to predict the next character. And let's say v is the number of characters in our alphabet. So for the moment it's 28 because we have the starting character and we have our 26 English language characters and we have the underscore for space. Now in order to write down exact equations for what's going on here, we're going to choose a particularly simple version of this.

Okay so in particular, let's suppose we have basically the simplest alphabet you could have: it's just 0 and 1. So the number of characters in this alphabet is just two, it's gonna make our lives really easy. In particular, one thing that's really nice about this alphabet is that it takes exactly one character or one element to one hot encoding: I can either say it's zero or it's one. Once we go beyond that, we have to be a little bit more careful. And let's keep m = 3. Let's say we're still trying to predict the next character from our last three characters.

Okay so in this case, our state is going to have size m by 1 because we're predicting from the last m characters and each character we can one hot encode with just a single vector element. So for the moment, it's just n by 1. But question about notation please. Yes. Is x_n the same as x^(n)? Oh great questions. Yes, so no they're not. So let me just go back for a second to illustrate this. So x^(n), so the superscript denotes which string we're reading in. So our x^(1) here is “^what happens to a dream deferred”, x^(2) is “^if you can keep your head when all about you”, x^(3) is “^you may write me down in history”. Now x_n would tell us which character we were on in this string. So if I had x^(1)_1, that would be the start character, if I had subscript two that would be w, if I had subscript three that would be h. So probably a better way to write this and maybe I can update this in the offline lecture notes is to put a superscript one on this x_t in the table because we really are taking from exactly this first string here. Great.

Yeah and in some sense, you can think of this state machine as just going through a particular string and then we're just going to do it again with another string and again with another string.

Okay great.

So let's see where we were. Okay. So the state here, in this case, is the number of previous characters because we can encode each character with just a single vector entry. Our x is also just a single character and here we can code a character with a single entry: so it's 1 by 1. Our p here, again, is going to be a set of probabilities over the characters in our alphabet. So we're going to say “what's the probability of a? What's probability b? What’s probability c?” Here, we're going to say “what's the probability of 0? What's the probability of 1?”

So in general, it'll be v by 1 because we're going to have a probability for each of those.
Okay. So now what we're gonna do is we're gonna write an actual equation for f and for g. So remember we said f was the transition function that pushes out the last character and brings in the new character from x, from our observation x, and g is the thing that takes our states in as features and then outputs our classification, our probabilistic classification for the next character. Okay so now that we're thinking of s as m by 1, I want you to think of it as the latest character comes first. So if I had just written “wha” (so a is the latest thing and so that'll be on top and we have h then we have w), so if that's the case, let's see how we're going to construct that from our x and from our s_(t - 1). Well it would be nice if we could do it linearly, that's about the simplest type of update that we can do so we're going to try to do it linearly and see how things go.

So remember, we just said s has size m by 1. So here, that's 3 by 1 because we're saying that we're just using the last three characters. x_t has size 1 by 1 because it's just the next character that comes along and so my first question for you and this is for the chat is: what is this? We're going to be pre-multiplying something, this big... Darn, well. So I was going to ask you and maybe you can still think about it: we're going to be pre-multiplying this x_t by something, by a matrix, and we want to think what would be the size of that matrix if we pre-multiplied it to get s_t? I sort of just revealed it, but I think you can still say in chat what is going to be the size of that matrix? What is that going to look like? Great yeah. Okay you guys have got it, fantastic: it's 3 by 1 because you have to get a 3 by 1 at the end so multiply a 3 by 1 by a 1 by 1. Okay so now, and I see some of you are already thinking about this, let's ask what this question mark is going to be. And now I think some of you are getting the idea exactly right but for the moment let's assume that the latest character is on top. So in particular, I basically want to move x_t into the top position in my s_t vector and then I'm going to have the second latest character below it and the second latest character below it. So then my question for you is: what is the, we see now, the 3 by 1 matrix that we're going to pre-multiply x_t by to get that?

Great. Looking good. Okay. So got some nice answers here: [1, 0, 0]. So now, originally, some folks were thinking [0, 0, 1]. That would be potentially totally fine, it's just how you define your state and so if, in this particular case, we're defining our state as the latest character and then the second latest character and then the third latest character. So if I had written “wha”, we would have an “a” and then an “h” and then a “w”, then this is what we would get. If you define your state some other way, that's totally fine. It's just this is the particular way we're doing it right now. Okay and, of course, here our alphabet is {0, 1}, but that's why we would do this. Okay, so what we're doing here—so let's just recap—so what we've done so far is we've said our state is going to have x_t, the last character that we just observed, put up at the top, that's going to be its value in the top position. And now what we'd like to do is we'd like to have its next value be the first value in s_(t - 1) and its third value be the second value in s_(t - 1). So now we have to think about how to accomplish that with this remaining big question mark and I'm just gonna show you and then you want to think through why this is true.

Great and we actually had some people putting in the chat which is awesome. Okay but the most important thing is just to think through what's going on here. So in the first row here, we're multiplying this by s_(t - 1) and we're getting 0, so this has no effect on the first element of s_t, we're just adding 0. In the second row, we're picking out the first element of s_(t - 1) but this is the second row so we're putting it into the second element of s_t

In the third row, we're picking out the second element of s_(t - 1) but this is the third row of s_t so we're putting this into the third row of s_t.

And so the thing to observe here is that we're just shifting the elements of s_(t - 1). Again, you could choose your state to be represented in a different order. Here, this is just a particular order that we've chosen for the moment but we're gonna get this shift and then what we're gonna get is that we're just putting x into that first element, we're putting the first two elements of s_(t - 1) into the second, the last two elements of s_t and then we're throwing away the last element of s_(t - 1), it has no effect here, it's not coming in at all. And that makes sense because that is too far outside the context now, that's like the fourth character so we don't need it anymore, so we're throwing it away. This is just a way of saying exactly what we said on the previous slide but with an equation. So we're just expressing that same idea with an equation that we're just shifting the context, we're shifting what are the last three letters here.

Okay and then we also want to specify g with an equation. Now remember the idea of g is that now we have our features, our s_t. We have specified: here's the set of features that we're going to use for this problem. Our features are going to be s_t. It's going to be what are the last characters. If we want to do a classification with those last characters, we want to ask ourselves how would we do a classification where we have a linear classifier and we return the probabilities of the different characters in the alphabet? This should just be applying things from previously in the course but let's briefly review them. Okay so we're going to call our parameters W^o and W^o_0, where the o stands for observation because this is the output of the observation model. Now we're going to have some function, let's call it f_2. WWe haven't defined an f_1 yet, but this is kind of anticipating that we will define an f_1 shortly, so let's just go with it for now. Let's call it f_2. And hopefully this looks mostly familiar with a few small differences. Like, for instance, if we were doing two class logistic regression, we could just have regular logistic regression: we have the probability of one class pop out. In that case, we could have this be 1 by 3 and this be 1 by 1. That's what we did when we first introduced logistic regression way back in the day.

But in general, if we only have two options, we can't specify all those options with just a single probability and so, in general, we might have, instead, f_2 describing not just two-class logistic regression, not just a logistic sigmoid, but a v-class logistic regression.

And so now, if we did something like a soft max, we might have v different outputs. Now notably, here we have an alphabet of size 2 and so 2 is an awkward stage where you could do either: you could either do vanilla logistic regression where you have the probability of one outcome and the other outcome is 1 - p or you could do the more general soft max function and then you would have the probability of one class and the other and it just so happens they add to 1. And so it's just worth noting that that's why we're seeing that, even in v class logistic regression, you might have v by 1: that's when we're naming, we're writing out explicitly, all the probabilities.

Okay now, in general... So in this case, f_2 is going to be this general soft max function: we're going to be taking this v by 1 vector that we get out here from this whole thing, we're going to be putting into soft max, and we're getting out these v probabilities.

And now, in general, s_t doesn't have to just be three long: it could be m long and so we could replace m there.

Okay so what I want to emphasize at this point is that this, in some sense, isn't too much that's new, we're just combining a lot of ideas that we've seen elsewhere in the class. So the way that we came up with our features was by running our state machine forward, so that's a review of state machines, an application of state machines to this problem. Once we had our features, the last three characters, we just ran vanilla logistic regression or soft max v class logistic regression. So these are ideas that we've encountered previously in the class, there's something new in that we're combining them but we haven't really defined something totally new at this point. But at this point we can also say, well... Oh and actually before I go on, I do want to point out one thing that actually is different here, not substantively, but it looks different. So something that you're used to from our logistic regression lecture, our neural nets lecture, every lecture where we've done some form of logistic regression or softmax or some kind of classification, we've always had a transpose after our first parameter, the parameter that we're multiplying by the feature vector. And here you'll notice that there's no transpose, so that is different. It's not substantively different: you could always just have described your other vector, whatever you were using as you could call the vector the transpose or vice versa. The reason—it's just a convention—but the reason that we're changing the convention here is that this is the convention for recurrent neural nets and that is what we're building up to. And so it's just a convention: it's not meaningful, but it's worth looking out for that this is a change that has happened. And in particular, it highlights too that, in reality, we're going through all these topics pretty quickly in the class but there's a whole set of people in whole communities that work on reinforcement learning, there's a whole set of people and communities that work on recurrent neural nets, there's a whole set of people in communities that work on convolutional neural nets and things like that, and they develop these these ways of talking about things and these standards and this is just one thing that is different across them. Also, it's just worth noting that, in general, when you actually read papers about these things, you can't just assume—unfortunately, I wish—but you can't just assume that the notational choices that we've made are going to be the ones in those papers: it may well be different and it's just worth keeping an eye out for what may change. So I'm just highlighting one of the things that has changed here. Okay, so here I want to point out that there's a familiar pattern here and this is no different where we start by choosing how to predict a label given a set of features and parameters. This is like, literally, I'm just copying the things that I said in lecture eight again because it was a familiar pattern then, it's a familiar pattern now. So here we've chosen how to predict a label given a set of features and parameters. We said given these inputs, people writing text, we have a way to predict a label which, here, is what is going to be the next character that we see and we've said our parameters: our parameters are these W’s, W^o’s. Okay, now we have to choose a loss between our guess and the actual label and then the way that we actually learn anything is we have to try to find some parameters. Typically we might do that by trying to minimize the training loss. So we did this for logistic regression, we did this for linear regression, we did this for vanilla neural nets for classification and regression, we did this for convolutional neural nets. Basically everything we've done in supervised learning has fit into this familiar pattern and so now we have another example of that familiar pattern: we have a way to predict a label, so that's our p_t. For every single label, we have this p_t. We need to choose a loss between our guess and our actual label and then finally we would do something like gradient descent or stochastic gradient descent or something on that loss so long as everything were differentiable and if they weren't differentiable, then we'd have to think about something else. Okay, so what can we do here? So we have to choose the loss. Now if I look at a particular superscript i, so that's, again, one string of text like a line from a poem and a particular subscript t, so that's just a particular input letter, I can say I made some prediction for this letter and there was some actual letter that somebody typed, how do those compare? And again, because our predictions are probabilistic, a really natural loss here for this comparison would be something like negative log likelihood. In particular, the version of negative log likelihood where you can have multi-class classification. Okay, so that's for one particular character in one particular string and here p_i is the output, just to emphasize the dependence on the parameters W, is this output for a particular input x^(i). So that was the thing we were reading in and these parameters W. So we only get p_i by having the parameters W.

Okay so that's a particular character in the string but, of course, we want to say, “well remember, we can predict this first character, we can predict the second character, we can take the third character and so we want to talk about how we do over the whole string?” And so for that, we can sum up the losses over the whole string and so here, n^(i) is just going to be the length of this i-th string, so like the length of “what happens to a dream deferred,” what's the number of characters there, and so we're going to go over each of those characters, say “how do we do predicting them” and then report that loss and so we can say that L_elt is like the element loss for a particular character and L_seq was for the whole sequence, what's the loss of the entire sequence. And then finally we want to say, “okay, well that's one particular sequence, that's one particular line in a poetry book or what have you.” I want to know what's the loss over all the sequences. Typically I'll read a lot of different text strings and I want to train on a lot of text strings and so let's say we have q of those text strings and then finally that gives us an objective that depends via the predictions on the parameters W, W^o, and W^o_0.

Okay so these are slightly different than our usages in the past, so here we're saying that i ranges from 1 to q, q is the number of sequences. We're saying that the number of characters per sequence can change. I mean certainly that's true: if you look at even just the English text sequences that we saw on a previous page, but in general like sentences change length so we want to let them have different lengths and so we'll call the length of that sequence n^(i). There's something that's a little bit awkward here: there's nothing that exactly corresponds to our notion of number of data points from before because we have multiple observations within a sequence and we have multiple sequences and so there's no strict thing that's exactly like the n that we had from before. These both contribute to that.

Okay so now, we've chosen how to predict our label (we did that with this basically multi-class logistic regression), we chose a loss between our guess and our actual label and then finally we would choose parameters by trying to minimize the training loss and so if we made all the choices that we've talked about here we could totally just use gradient descent or stochastic gradient descent or anything like that because everything is differentiable and so we could go ahead with that and this would be just a typical problem like the ones we've seen before, just a little bit more sequential.

Now, an observation that we can make is, well, this is how we built up to neural nets: we started with a typical logistic regression problem or a typical regression problem but basically just a linear classifier and then we said, “hey, wouldn't it be nice if we took the features and we actually learned the features instead of just putting in different pre-composed features.” And so we might ask ourselves: could we do the same thing here? So here, we decided, we said, “how do we construct features?” Well we take in our inputs, we take in our existing features and we combine them in this way. But maybe we don't have to combine them in just this way, maybe we could combine them in a different way.

So something that we did, again, way back in neural nets is that we said, “hey, let's try out different combinations and let's let the weights in those combinations be learned.” Something that we did in convolutional neural nets was we said, “hey, here are filters, here are some useful filters that you might be interested in with zeros and ones in them and then what happens if we learn the filters? What happens if we put in different weights in those filters and then we learn, again, essentially learn the features that we're going to be using?” And so this is exactly the same story we can again say, “okay, we've defined a particular linear classification problem. You have a bunch of features, those are just the last few letters, and you have an output which is the linear classification based on those features. But maybe I want to learn these features. I mean, in particular, one of the things we said was, well, gosh I don't know if I want exactly m of the last few letters or a different number I'm not sure exactly how much information that I want to keep for my problem, what's the right thing to do there? Also, maybe, this isn't the best way to keep that information. Maybe I don't want to just keep it as here's just an enumeration of the last three features, maybe I want to do something more with that, maybe I could get better performance if I combine them in a different way. I won't know without trying so let's do it and so, in particular, what we're going to do now, again just like we did for neural nets, just like we did for convolutional neural nets, is we will, instead of having this strict set of zeros and ones, will allow these to be parameters. Another question.

Yes in your J(W^o, W^o_0), what's the meaning of q here? Yeah so again, the thing that's a little tricky here is that we have nothing exactly like number of data points from before. So in some sense, if you're reading a sentence on “what happened to a dream deferred” you kind of get a lot of data points for training from that because you want to predict after “w”, you want to predict after “wh”, you want to predict after “wha” and so on. So each one of those is kind of like a data point. So those are going to be indexed by t. Now each time you have a sentence or a string, that's going to have a bunch of these t's within it, so we'll index those by i. So each string is like an i here. Now the number of strings that you have that you're using for learning has to be some number, let's call it q, and the number of characters within the string has to have some number and that could change from string to string, so let's call it n^(i), and the reason you might think of neither of these as being exactly the same as n is because they're both determining the amount of training data that we have here and so previously we called the number of training data points “n” and here they're both contributing to the total number of trading data points. So you could think of q as being like the n that we had from before: it's kind of like the number of data points, but it's really the number of strings, it's really the number of sequences, and then n^(i) is the number of points within the number of time steps, within a particular sequence indexed by i. Great, okay cool. So what we've just done is, again, hopefully, somewhat familiar: this idea that we had these predetermined features and then we decided instead of just saying we're going to have this particular thing that we do with these features, that we can actually learn the features. What's different about it is how we're doing that: we're not just taking our x's and then putting a bunch of weights in front of them and then using that to find the features. That would be a more vanilla neural net that we've seen from before. Here what we're doing is we're learning the features from the new x and the old set of features, so this is much more sequential than anything we've done before: we're keeping those old features around and doing something with them. Also something that's interesting here is that, in some sense, the inputs to our feature construction, the x_t and s_(t - 1), together all add up to the same size as our features, our s_t. And that's because we have to keep this going, that the s_(t - 1) has to be the same dimension as s_t, has to be the same dimension as s_(t + 1) and so on and so forth. x_t doesn't technically have to exactly combine in this way, but the s_(t - 1) certainly have to have the same dimension over and over again, the s_t's, so it's a little bit different than things we've done before too. But essentially here what we're doing is we're saying, “hey, there was a useful setting of these W's, we just identified it. It told us the context but maybe it's not the most useful setting, maybe we could find another useful setting by setting these to be W's, by setting these to be parameters and then learning them together with everything else” and this is, again, the story that we had for neural nets: we thought about learning our features and so we introduced parameters there, we talked about this with convolutional neural net, so now we're just doing it for a different type of data, a different type of thing. Now this sequential data with recurrent neural nets which is what we're essentially defining now.

Okay so this superscript sx here is just to denote that these are the weights that go from x_t to s_t. So it's just telling you which set of weights because we've introduced two sets of weights at this point. So likewise, this ss is telling you that these are the weights that go from s_(t - 1) to s_t. Now if you think back to what we did before when we were doing neural nets, there are a couple things that are a little bit different here. We didn't just combine the inputs to get features, we can linearly combine them together with a potential offset. Same thing with our filters and convolutional neural nets: we said that there could be these weights but there could also be an offset. So let's just introduce the offset and so it's going to have this ss superscript in this case again and then just a 0, a subscript 0, to indicate that it's the offset. That's just notation, it could really be anything. It's just a different, it's a new set of parameters that are the offset. Now the other thing that we did in all these cases that we haven't yet done here is we had an activation function. So in particular, we would take our inputs, we do this linear combination. So here we have a linear combination. This is a particular linear combination, what we might have called a unit before, for instance, and then we applied an activation function. And so now, let's do the same thing, let's apply an activation function. This will let us get interesting nonlinear things going on and so that we can really learn what's going on in our data. We saw the power of that back when we talked about neural nets. We saw how when you have these non-linearities, you can get these really interesting classification boundaries and useful classification boundaries and so we'll do the same thing here. Now it's worth noting that the way we apply this is component wise. These are exactly the same as activation functions, essentially, that we had before.s So you have this 3 by 1 vector that's coming in, you apply the f_1 at each of those elements and you get a 3 by 1 vector coming out. I think it's not always obvious how these are being applied, so it's always worth asking, if you have a particular function, is it being applied component-wise or not? And so in this particular case, when we think of these activation functions that are being applied to each of you might think of the units.

Okay now, this of course, is this setup here is specific to our choice of m = 3 as the size of the state. It'll help us to write this in more general notation for the cases where it's not just m = 3, so let's write these as matrices. Rather than all the elements in those particular matrices, let's just write the matrices and so here, we have just our capital W^(sx) now, that's just exactly the matrix that we had before here. The W^(ss) is the matrix we had before, W^(ss)_0 is now this offset that we have. So this is just exactly the same thing we had right before I changed it, so this becomes this, but again two things: so one, now we're just writing it with a single symbol, but two, this will be more general because this will hold for things where we're not just setting m = 3.

Okay so in particular, we can expand our alphabet now to say have l characters, our state could be the last c characters. We have to be a little bit careful like what is the size of the state vector? Well, it's gonna change here, so x_t, if we have an alphabet of l characters, we might be able to encode a single character in one hot encoding with, maybe, let's say, l elements, that's one option. So if we did that, then our x_t would become l by 1 and our s_t should be the c previous characters and so, in particular, if we say that we encode each one of those with one hot encoding, then it might be that m = c * l here. Depends how you encode it, but that would be true. Now of course, this was before we learned our W's. Once we incorporate these W's and we don't just set them to be zeros and ones, we allow them to be learned. It could be that the state is not just the last c characters. It could be that the state is really something much more general. It could be that you choose m to be something that isn't just m = c * l, it's just a general m that encodes basically how many features you want and so this is a useful way to think about how we arrived at the state, just the same way we thought about filters we looked at some examples of useful filters, but the reality is you actually learn the filters because you're learning the parameters. It's the same thing here: this is a useful way to think about the state but once you put in those W's, you're actually going to learn a state. It's not going to have this interpretation necessarily anymore.

So that's, again, just a pure example of this broader idea.

Okay so now, the set of parameters that we said, our familiar pattern, we choose how to predict a label given features and parameters. So now we've chosen how to predict a label given features and parameters. We have a new, updated version of how we're going to make a prediction and what's changed from the previous time we talked about this familiar pattern mere minutes ago is that now these aren't our only parameters, we have these parameters as well, we have the parameters that are part of the state construction as well. And so now, we can just update this little sidebar about loss over here and we'll note that the prediction depends not just on the W^o’s, but in fact, all the W's. So maybe we could write it in this way but the thing that we're trying to emphasize here is that it depends on all the parameters and we have to learn all the parameters, we have to encode all the parameters here. So the loss our objective depends on all of the parameters and so now when we try to minimize the training loss, we're going to look at all those parameters.

And so here, what we have described now, is a recurrent neural network.

So when we just had the state machine and we had our logistic regression, we're getting there, but here, now, where we're learning these W's as well. We have a recurrent neural network, so it's like the neural networks we saw before, but there is this recurrence because we have this iterative procedure of going back to the state. Okay. So let's draw this in a way that is reminiscent, at least a little bit more reminiscent, of the way we've drawn some of our neural networks before. So again, the example that we saw, just a motivating example, was that we have this alphabet of l characters. We might hope that the state could represent something like the last c characters, but we might learn something else, we'll see. We do, in general, have these updates for the state and for the prediction and now we have both an f_1 and f_2 so it makes more sense why we called that second thing f_2 and this is what we're calling or when our output is from a recurrent neural network, it's that it came from these equations, these recursive equations on s_t and p_t. Okay, so let's make this drawing. So before we add the general, very general, state space model, now let's look specifically at this recurrent neural network model. So here, now, we have our starting state or x_1 and they go into some kind of summation, this kind of linear combination and get out what you might call a pre-activation. So here we're defining this input to f_1 to be z^1_t.

So this is what we used to do in neural networks with the hidden unit and the inputs: we would take all the inputs, we would do some kind of summations although we had we had more units depicted in the picture, but that's still going on here, it's just that z is a vector, a vector of the same size as s. Okay and so we can do the same thing over here: this can be, also, an activation. This is z_t^2. So here, the superscript 1 just means that we're in the first layer. We're gonna see that there's a conception of layers and the superscript two just means we're in the second layer. You can think of them as being like the f_1 and f_2. So the z gets a superscript from the f subscript.

Okay so once we have these activations, we put them into our activation or pre-activations. We put them into our activation function, get our activations. Those are exactly the states. Then those, themselves, go into a linear combination to get the pre-activations for f_2. We put them through the activation function and finally we get our output for our final activation p_1. And then, just like before, we recurse on this. So just like we saw in the state space model before, our old or sorry our new state becomes the state that we just got, it becomes our new input, it becomes the new state that's going into the next round and so we started all over again except now we've incremented the subscripts so we're one step forward in time. We get our outputs, we get our new state, we get our outputs and we just keep doing this.

Okay now I want to highlight how this is very similar and just a slightly different drawing to the regular vanilla neural nets that we described in lecture six. So this is like the hidden layer in our regular vanilla neural nets: we have some inputs. Before they were our x's, now they are s_0 and x_1 or, in general, our state and our new input.

We have some kind of sum that we're doing. When we drew this for neural nets, we separated into a bunch of units. You could still think of there being a bunch of units here, it's just that we've put them all in a vector together.

Now each of those units is going to give us a pre-activation. So it's worth noting this z_1^1 has the size of the state.

And now what we're going to do with these preactivations is we're going to turn them through an activation function. It could be any of our familiar sigmoids, that would be a typical choice and turn them into activations.

And those are our states now.

Okay so this bit behaves like our hidden layer in our two-layer vanilla neural network.

And this bit behaves like our output layer. So now we take our hidden units, these s_1’s, we do some kind of summation thing, we get our pre-activations, we put them through an activation function and we get out our p_1. It depends what you're doing whether f_2 is applied component wise. In the example that we gave, we're interested in doing this multi-class classification it would not be it would be a soft max and so it would be across all of the components so it depends what you're doing, but that's just like for regular neural nets, that's not some new thing here: it's actually just like you do for even a straight linear classifier, so here it's a similar deal. Okay but what's different is that now the state keeps changing. So instead of just having that bit there, we keep going. We have this thing that just keeps going off the slide.

Okay so let's make another way that we could represent this. So something that's a little bit annoying, potentially, about this representation that we have right here is that it just keeps going, like it’s going off on the slide off to the left and it's just it's ad infinitum. Is there a way that we could draw this in a finite way? Well yeah we could just wrap it in on itself. So here what was happening was we were taking all of these subscripts and incrementing by them by one each time and we would go forward and forward and we just have those appear right next to each other every time so we increment this to two, to time step three, to time step four, and that's how we would build this long piece of tape that's describing our recurrent neural network. And so what we can instead do is have a loop to represent this. So this is exactly the same thing that we see on the left here but on the right, we've gotten rid of all the subscripts: they're just going to naturally increment each time we go through this picture.

And then the other big thing is that we're seeing explicitly that the state that we come up with becomes the input to the next round. That's that big line there at the bottom.

So this is just another way of representing the exact same recurrent neural network but in this finite diagram.

Instead of imagining that it goes on forever but just off the slides. And again... Oh and we will call this the folded or rolled version because if you think about, maybe you wrote the really, really long one on like a long piece of tape, and if you folded it up, then that would be a way that you could see it going back into itself every time and so this is the folded or the rolled version and this is the unfolded or the unrolled version where we just repeat it again and again and again.

Okay so let's just, again, briefly compare to other things we've seen in this class, feed forward neural networks. Especially this folded, rolled version looks very, very similar to our feedforward neural networks: we have essentially something like the hidden layer, we have something like the output layer, and of course, the big difference is that we are not feeding forward anymore. So if you recall from our lecture on feedforward neural networks back in lecture six, we said that the nature of a feedforward neural network is that everything always goes to the inputs to the outputs. But here, we have this arrow that's going against this time ordering, that you can actually go back, you can now put that back in as an input again and so this isn't strictly feeding forward from that perspective.

This is also different from convolutional neural nets, although you can think of the reason for us doing it as being quite similar. So they're both designed, essentially, for a very particular type of data, they're all just examples of neural nets. Fundamentally we're all doing this similar deal with building up units on top of units, but they're constrained in particular ways, convolutional neural nets and recurrent neural nets. So with convolutional neural nets, we designed them essentially with pixels in mind. We said, “hey, we have pictures and these pictures are composed of pixels.” Then there are natural things that we're interested in there. As we said, we want shift in variants, we want these like spatiotemporal properties that we can encode with things like filters, and that was really natural for vision type data. And here what's really natural about text data, but also other types of data, is that it's sequential: you have it building up in order. The way that you type is that you type the first letters and then you type more letters and then eventually you get to the end. It is unusual for people to type from the outside of the sentence in or some other order, you almost always type from the beginning to the end. And so this is representing and respecting that kind of structure in the data and then trying to learn from it and so we're seeing that in these recurrent neural networks that they're designed for a certain type of structure in data. Just like feed forward neural networks, just like convolutional neural networks, they can be optimized so long as you choose all of your activation functions appropriately, that everything is basically differentiable, you can optimize with gradient descent, stochastic gradient descent and all these ideas. And just like convolutional neural networks presented a bit of a pain for actually doing that optimization and actually, really, this is all chapter seven with the feed forward neural networks, there are still, equally, like some pains to actually using these in practice. So it's a nice idea that you're just gonna specify this, you're gonna go and choose your loss and you're gonna optimize it and then, in practice, you have to do some tricks to really get that to work well but that's the overarching idea here. It's also worth noting in all these things that essentially what we did was we said, “hey, we have this existing idea of how to make a prediction and we noticed something that wasn't ideal about it for a certain type of data, like maybe we could make some extension that would make it better.” In convolutional neural networks, maybe we could use these filter ideas and that would help respect the structure of vision data. Maybe in recurrent neural networks, we could take advantage of the fact that we have this sequential nature to our data and so if you come along with some new data, this is not the end of the story. I mean, certainly, for both convolutional networks and recurrent neural networks, we see a lot of developments beyond these very basic architectures that we've covered in class that help deal with other things that we know about the data and that's very much something that you can do and hopefully you feel that that is something that that is within your power, that you could make further improvements on these. Like these are really just the start. Okay and then I also want to just again say this is a contrast with reinforcement learning. So here the data is just sequential. We're not treating our own decisions as changing the state of the system, not that you couldn't, but for the moment that's not what we're doing. We're just saying, “hey, we trained on a bunch of data and then we can predict on some other data. The text and the training data was written before we got here: we just scraped a bunch of poems from the internet or scraped some text from the internet and we're just going to predict the next character on the phone.” This is very typical supervised learning. Again, it's not to say that you can't combine these ideas. People combine things like convolutional neural nets and recurrent neural nets, they combine reinforcement learning with various neural nets, but as we've been showing them here, that's not what we've been doing. We've just been saying here's a supervised learning problem.

Okay so finally, we have to decide how to actually do that stochastic gradient descent or that gradient descent. And just as we did for CNNs, we're just going to get the various tastes, hopefully just to illustrate what is challenging here. And the real reason any of this is challenging (I mean to some extent all of these problems are solved by the fact that we have modern automatic differentiation tools and you can use them), but it's worth keeping in mind here why can't we just immediately use what we used in neural nets? Why can't we just use exactly vanilla back propagation in all of these cases? In the case of convolutional neural networks, it's because we had a lot of weights that were set to be the same, right? So we have this weight sharing where, when you apply the filter here and when you apply the filter here, you were using the same set of weights and so that's what created this slightly different look to the back propagation and it's the same thing here that every time you increment one as you go forward in time, you're applying the same set of weights and so again there's a form of weight sharing that's going on and that's going to make the back propagation just a little bit more involved basically. And so if you get anything from this, that's really just the key point. It's easy to get lost in the math and I just don't think that's as important. What's really important is to understand this high level idea that you form a way to make predictions, it depends on some parameters, you form a loss and then you try to optimize that loss. We're doing that here with something like gradient descent or stochastic gradient descent and that will just involve taking derivatives and it could get messy just because of these dependencies. That's the main idea and then everything beyond this is like icing on the cake and just getting into the details. Okay, so let's say we chose to do stochastic gradient descent here. So we would choose our index i uniformly at random from the data indices and, in particular here, that's the sequence we might choose a random sequence from our sequences.

Okay so once we have a sequence, we can look at the loss in that sequence. In particular, we can run our forward pass. That is to say, you just run the recurrent neural net as we've described it and you can get the loss. In particular, you can get the prediction which will give you the loss together with the label. And now, of course, what we'd like to do is we'd like to use the usual SGD story: we'd like to optimize with respect to our parameters. Now of course, we'd like to do that with all the parameters. I'm just going to choose, almost arbitrarily, a particular parameter here, but you would do this with, of course, each of your parameters that you're trying to learn. This is just one of our set of parameters.

Okay, so W^(sx). We're interested in learning this parameter, just like the other ones, and so we take these derivatives as part of SGD or GD, but here in SGD, we'll do this for each of these sequences, we'll take these derivatives and we'll use our usual SGD updates. And so, in some sense, we've reduced this problem to: we need to get these derivatives. If we know everything about SGD already, stochastic gradient descent, then we just need to get these derivatives. Okay, well remember, the sequence loss is the sum over the element losses for each of the t's, so each of the characters in our example and so we can just take that derivative out over each of the element wise losses and so now we've reduced the problem to: we need to get the derivative of the element wise classes with respect to W. We're just going to keep reducing this problem.

Okay, well now if I look at these element wise losses, there's two inputs to them: there's the prediction and there's the actual label. Only one of those depends on the parameters, that's that's the prediction, and so yes, we need these element-wise losses but what we really need is the derivatives of the predictions with respect to the parameters.

Okay why is it challenging to get the derivatives of the predictions with respect to the parameters?

So let's focus on getting the derivative of the prediction with respect to the parameters. Okay well where does this W^(sx) come in? Well it comes in here: it's what we multiply x_1 by before we do the summation to get the pre-activations. But it also comes in here: it's what we multiply x_2 by in order to put in the summation to get the pre-activations. And it also comes in here: it's what we multiply x_3 by to get the summation to get the pre-activations and it just keeps going, on and on and on. And so the challenge is that p_1 is a function of this W^(sx), but then p_2 is a function of it in two different places and p_3 is a function of it in three different places and p_4 is a function of it in four different places and so you have to be careful to make sure that you're doing that derivative with respect to all of those different places. And so, since we're running out of time, I'll just say this essentially involves making sure you do partial derivatives correctly. The idea is a relatively straightforward idea and just, in practice, it can get a little messy like a lot of things. This is why we love automatic differentiation these days to help out with these things, make sure we don't make mistakes, but this is essentially the problem or not the problem, but the thing that makes this a little bit tricky is that it's not like when we had previous parameters there was only this one dependence. And so what we could do is, if we had something was a function of a function of a function of the thing we cared about, we could just use the chain rule automatically. Now we have to be more careful with partial derivatives because there are these multiple places that the same weights come in and so that's what just makes back propagation a little bit more tricky here. Okay so to recap: we were doing a lot of reinforcement learning, we saw these cool reinforcement learning things before and now we've seen that we can actually use these ideas of state machines and sequences in, in fact, supervised learning as well. So if you have your classifier in your mind of whether or not you're doing reinforcement learning, it's not just a matter of “are you doing state machines are using MDPs?” You really want to know are you learning as you go along in the world and are you changing the states and is that something that you're trying to decide how to do and to learn more from. Now, we've seen recurrent neural nets. That gives us a sense of how we can learn with sequence data, how we can do this in this neural net type framework and so we're back, in some sense, to supervised learning but it's a different type of supervised learning for a different type of data than we've seen before. Okay I'll catch you next time, have a good week. Bye everybody.
