Okay it's MIT time, so let's go ahead and get started. So let's just recall: previously we've been talking really about classification in this class, you know, we had a little moment where we were talking about how this fits into broader machine learning, how classification is a particular type of supervised learning, and regression is a different type of supervised learning, but today we're actually going to really get into the details and actually do something with regression, which is new at this point. So again, we've been talking about classification, in particular, last time we really started talking about logistic regression which, despite the name, is a form of classifier, it's an algorithm for classification, so we also called it linear logistic classification and we talked about how we can get good parameters with this using gradient descent, which is a much more general algorithm: it can be applied to optimization in general, it doesn't just have to be classification and in fact, hopefully, we'll spend some time seeing that today. So today, we're going to focus on regression, just as in classification, within classification, we did a lot with linear classification, here we're going to do a lot with linear regression. We're going to see that we can add a penalty or a regularizer just as we did in classification, in particular, in logistic regression last time and we're going to get what's called ridge regression and then we're going to talk about how even though it looks like we can solve these problems exactly, we still actually care about things like gradient descent and hopefully we'll have some time to talk about stochastic gradient descent. Okay, so that's the plan for today, so let's just start by recalling some of the nomenclature and notation that we had around classification because we're going to find sort of the analogues in regression. So in classification, we had a bunch of data. So we could think of data as being plural and one particular data point you might call a datum and so let's look at our data as just an example. You know, last time we were talking about, you know, when would I wear a coat and I might decide that based on the temperature. You know, here maybe it's the other way: when will I wear, you know, a coat? As I get to a higher temperature, I'm not wearing a coat, as I'm in a lower temperature, I am wearing a coat and so we can think of each of these data points as having sort of two parts: there's the feature vector, the set of x's. In this particular case, it's a really simple feature vector, it's just one dimensional, d is one dimension, but in general, I could have a lot more features as part of my x and then each feature for my particular data point has a label. And in this case, it's -1 or it's +1: so -1, I'm not wearing a coat, +1, I am wearing a coat and so now we want to find a hypothesis, a hypothesis ideally that's a good hypothesis that we can use to predict, you know, when I'm going to wear a coat in the future. And so right now, there's no hypothesis in this plot, there's nothing that tells me, for every possible x value, what might I predict, why might I predict here. But I can add one: here's a hypothesis. Again, a hypothesis is just a function that tells me, for every possible x value, what's a prediction? It doesn't have to be a good function, it's just any function, although of course I'd like to find a good one, one that will perform well when I get test data in the future, when I get new data. Now, we talked about a lot of different losses, actually, that you could use for classification. So one is 0-1 loss: I incur a loss if I'm wrong, I don't incur a loss if I'm right. I might care about asymmetric loss: I might be wrong in different ways and that might matter to me. I might look at negative log likelihood loss, which is something that we looked at last time, and what's cool about this is it gives us a way to deal with data points that sort of overlap, that aren't perfectly linearly separable, and this notion of uncertainty that we talked about. And now, an example of classification, because classification, you know, could be pretty general, but an example of classification is linear classification. So in linear classification, we have our set of features and we find a hyperplane. So that's a pretty simple concept in just one dimension, it'll just be sort of a point and a direction, but so here we're looking now at a linear classifier that says on one side of my hyperplane I predict +1, on the other side of my hyperplane I predict -1. Now, something that we've certainly seen is that I could actually have a much higher dimensional complex feature space and so that when I plot things in a lower dimension maybe it doesn't look linear, but if I'm doing linear classification, there is some high dimensional space with all of my features where I'm ultimately going to have this linear classifier. Okay, so now let's start developing regression and linear regression sort of in comparison to this classification development that we've had. So here, let's think about regression. So in regression, you know, a lot of things are actually pretty similar: I'm still going to have a bunch of data points. So here, let's look at a regression problem now where I have a single feature and I have my label, y, and the big difference here is that my label can take more values, it doesn't have to be just -1 or +1 or just a discrete set of values which is usually what we're assuming in classification or, in particular, it doesn't have to be sort of this unordered discrete set of values, it can be more general. So here, our feature vectors are really kind of unchanged, you know, we still have a d-dimensional feature vector in this cartoon, it's just one-dimensional (d is 1), but our label is the thing that's really different. Again, we're letting it take more values essentially and really be, sort of, continuously valued. So as an example of this, suppose I am looking at the temperature. So again, let's assume x_1 is the temperature, but now I'm looking at my air conditioning bill, and so maybe if it gets really, really, really hot, I expect that if I have air conditioning, my bill will go up and so I might expect this kind of relationship between those and that's not something where it's just going to be “is my air conditioning on or not,” I actually might be spending more for air conditioning as the average temperature goes up. Okay, so again I'm going to want a hypothesis to express: how do I predict on new data? And just as in classification, I can have good hypotheses and I can have bad hypotheses. So a hypothesis is really just going to be some function from my features to my new set of labels, my labels here again being anything in the reals, but of course again I'd like a good one, I'd like one that helps me predict on new data points in a good way. So here's an example of a hypothesis: it is a function that goes from my features to my labels. Okay, but again, as we said, we want it to be good and just as in classification we came up with this notion of loss to help us decide, you know, what's good and what's bad, we're gonna have notions of loss here. Now just as in classification, there can be actually many losses that we might use, it's going to be the case here in regression that there are many losses that we can use but the loss that we're going to focus on is what's known as “squared error loss.” So remember the idea of a loss, as we've discussed it before, is that we have a guess, let's call it g, and an actual value, let's call it a, and we want to compare those two in some way. So for instance, let's look at this particular data point here in our graph. So there is an actual value for that data point, we can just look at its y value in particular. So here, when we say actual value, we're really focusing on the labels. Now we can also compare to our guess and our guess is where that regression line is at this particular set of x's and so if we drew a little segment from our guess to our actual here, it might look like this, this little red line. So it goes from our guess, which is in blue, to our actual, which is in black, and then of course to actually get the loss we're gonna square the length of that little line and then we could do this at all the other data points to get something like our training error that we've talked about and just sum up over all of that. And so again, the loss is just a way to ask about, you know, how well are we doing, how good is this hypothesis, what does it mean to do well in this problem? And it's worth reflecting, you know, why did we have to come up with a new loss? Why couldn't we just use all of these losses that we had already developed for classification? Well let's look at them. So 0-1 loss: well one version of this says that I have to predict 1 or -1 and that's definitely not going to be true in regression, but even if I think I'm asking “is my prediction exactly the same as my actual,” that's just not generally going to be true in regression. If I look at any of these points in the graph on the right hand side and the figure on the right hand side, none of the actuals are the same as the guesses but I could still actually have a really good hypothesis even if that's not exactly right. Like if I'm predicting my air conditioning bill and I'm off by one cent, I don't think that's so bad, and this squared error loss is expressing that if I'm off by one cent, that's not so bad, but if I'm off by a hundred dollars, that actually might be pretty bad and I might not be too happy about that. So I wanna express that. I also don't want asymmetric loss for the same reason, that's gonna have the same problems as 0-1 loss. And negative log likelihood assumes that my guesses are between 0 and 1, which again doesn't make sense for this regression problem. There are plenty of times that my air conditioning bill will not be between 0 and 1, I wouldn't generally expect it to. Okay. And again, just as we had this notion of a linear classifier, we will have a notion of a linear regressor and so in particular, in linear classification, we said “hey, there's this hyperplane, so that's sort of just one dimension lower than this full space of the x's and y's” and what we did with the hyperplane classification, was we used it to divide: to say on one side, we're going to have our our -1s, on the other side we're gonna have our +1s and then we actually used it in a slightly more involved way when we were talking about logistic regression, because actually, sort of, just even the magnitude of the theta could tell us something about our uncertainty. And so here, though what we're going to be doing with our hyperplane is we're going to be saying “hey we can use this hyperplane to actually make our predictions.” And so here, if we have a hyperplane for linear regression, that's actually our hypothesis and now we're going to say our guesses are along that hyperplane. Also a big difference here is that, at least for this type of loss, we're not really specifying a direction, we're just looking at that hyperplane itself. Okay, and we can also do this in higher dimensions so we, you know, we've certainly talked about this with classification and classification we could have, you know, two-dimensional features, higher dimensional features, and now our hyperplane is just dividing things in that higher dimensional space and same thing with regression. So in, regression we can have two-dimensional features or higher dimensional features, so here's x+1 and x_2, and then what's gonna happen is we're going to have a hyperplane prediction over this and that will give us our y predictions and something that's sort of worth pointing out here, and we'll come back to this too, is that the nature, even though we're calling it linear regression, we called it linear classification, there's not what we might think of as a line going on here, like always a one-dimensional thing, like this is definitely a hyperplane that we're using for our function, for our predictions and not just, you know, always one dimension. Okay, so that's classification, that's the general idea of regression, and this example of linear regression, and so let's dive deeper into this. So in particular, let's actually write out what is, what are the hypotheses that we're thinking of. So we have these hypotheses for linear classification and now we're talking about hypotheses for linear regression and hopefully this looks somewhat familiar, you know, like we're still talking about hyperplanes, we still have the same geometries that we've been working on this whole time, that we've developed, that hopefully you've gotten more and more intuition on as you've gone along in this course, and the big difference here is that we're not looking at the sign on one side or the other here, we're actually just looking at the values on this hyperplane and that's, you know, that's very different from classification. Okay, so again we want to talk about training error, just as we talked about training error for classification, and actually if you look back in your notes from either this lecture or just from the readings, you'll see that we actually defined training error very generally in the beginning of this course, we said that it was just this notion of average loss over the training data points and that's not specific to any particular thing that we might be doing, classification or regression or whatever. We're just saying for some supervised learning problem—so we have a bunch of features and a label—what's the loss between our guess, which comes from the hypothesis, and our actual, which comes from the label? And so this is very general, this is exactly what we saw before and what's going to be different here is the choice of loss and the choice of the hypothesis. And so if we put in exactly the hypothesis above, this linear regression hypothesis, and we put in this squared error loss that we just described, we're going to get out a new training error, something you might call mean squared error or average mean. A root mean squared error is sometimes you might take the root, but basically what's happening is we're taking a mean, we're taking an average (that's the 1 / n times the sum) and then there's this squared difference between our hypothesis and our y.

Okay so this is really going to be the loss that we're focusing on a lot today, the thing that we're trying to do something with and in particular, you know, just as in the past, we basically wanted to try to get this as low as possible. We've always wanted to try to get a loss and then choose some parameters that make it as low as possible, we're going to be doing that again today. Now, we're going to do some sort of mathematical trickery occasionally and this is one of those points and this is a mathematical trick that you have already seen in the context of classification but we're going to do it again here: if we augment our features with a feature that is just one, like the last x, we're just going to make the value 1, then that's equivalent to having had both our theta and our offset theta naught. So basically it's a way to get rid of theta naught, while still having the same expressive power. So you can look back to what we did with classification, we're going to be doing the same thing here, but effectively what this lets us do is it lets us get rid of that theta naught term while still expressing all of the same linear regressors as before, so this is just sort of convenient for math but it doesn't really change anything.

So we're just doing exactly the same thing we did in the line above but making this little trick where we have augmented our features with feature “1”.

Okay and we're going to be dealing with a lot of matrices and vectors that are all multiplying each other and I just think a really great unit test as you go along whenever you're doing this, when you're coding, when you're writing an equation, whatever, is to check the dimensions, you know, check that this makes sense. And so let's just start doing that: let's notice that theta, we've always said is a column vector, it's d by 1 so theta transpose is 1 by d. Same thing with x: it's a column vector, it's d by 1. And so these are two things that we can multiply by each other, we're going to get out a 1 by 1, a scalar. Now y^(i) itself is also a scalar, it's just a label, it's some number and so together when we can take, you know, we could subtract one of these from the other and we'll get a scalar and that's something we can take the square of. So this is all legit, this is all things that, you know, we can do within the realms of sort of matrix vector multiplication and so we're good. Now it's worth noting that this whole time, we never had to write theta transpose x^(i), we could have just written x^(i) transpose theta. You should definitely convince yourself that those are just exactly the same thing, they're two ways of writing the dot product, certainly the dimensions work out: x^(i) transpose is going to be 1 by d, theta is d by 1, y^(i) is still 1 by 1, and so this whole thing gives you a scalar, you take the square, and you're good. Okay so now, we're going to make this observation that what we're doing here is we're adding up a bunch of things squared. In fact, n different things, little n (those are the things in the sum). And so for adding up n different things and each one, we're squaring them, that's basically the norm of some vector, the square norm of some vector. And so we can ask ourselves: what is that vector? What is the vector that we're taking the norm of here? And in order to establish that, we're going to add in a little bit of new notation. Namely, we're going to collect all of our x's in a matrix. So let's call—so this is the vector we're interested in, the one whose i component is this thing I'm highlighting here—so let's call our our new matrix X tilde. So you can see that X tilde collects all of the information that we have on x, so it has the n data points as its rows and the d dimensions as its column, so the whole thing is going to be an n by d matrix. Why are we calling it X tilde with a tilde on top and not just X? Well to sort of emphasize that we've slightly changed things. You know, this whole time we've been talking about a data point as being a column vector so it goes from, you know, x_1 to x_d, and here we're slightly changing things by making it a row vector. So this is a really typical thing to do in linear regression, this is going to help us get out formulas that will look familiar if you look at for instance Wikipedia or, you know, another textbook, and so this is just a way to write all the x's together in a matrix. And we can do the same thing with y, so just as we collected all of the x's in this matrix and we had the each data point as essentially a row, that's what we're doing with y here too. So Y tilde has each data point as a row, each label as a row. And so now, the observation that you want to make is that, you know, we said we were interested—so this y is just down by one—we said we were interested in the vector whose elements, whose i element, was this guy, was this thing that we're highlighting, that's exactly this vector. If that's not immediately obvious to you, that's totally fine, but you should check it. You should check. I mean, everything we do with a matrix vector multiplication is just something you can write down, it's just effectively a quick way of writing a sum and so you can just double check that on your own. Now that is something that you can and should do, I'm just going to do a unit test on our understanding to make sure that this even seems plausible. So I'm not going to prove it right here, but I'm going to just make sure it seems plausible and in particular what I mean by that is I'm going to check the dimensions. So we just said that X tildes is n by d, we know that theta is d by 1, and so when I multiply them together I'm going to get an n by 1 vector. Y: We just said Y tilde is n by 1, so I can take an n by 1 vector and subtract another n by 1 vector, that's allowable, and I'm going to get out an n by 1 vector and that's exactly what we said we wanted. We wanted an n long vector whose elements we were going to square and add up. And so finally, again you should definitely check that you agree with this if this isn't immediately obvious and I don't mean you have to do it right now just sit down later and do it, this thing that we're trying to minimize, this function of theta that we're trying to minimize, can be expressed as the norm of this vector squared times 1 / n. And something that we've already seen and used in this class is that another way that you can write the norm of a vector is as a dot product with itself. Okay so here's a question for you: so here's a couple of vectors and I want to take their dot product, so I'm going to have to put a transpose on something, do I put it on the left term or do I put it on the right term? Where do I put the transpose? This is a question for the chat.

Lots of great answers coming in. I'm just going to give time for a few more so people have time to think about it.

Okay great, so totally nailing it. It's the first term, or the left term, and this actually is really important and let's see why. So first of all, let's notice that this—remember we just said that this X tilde theta minus Y tilde is an n by 1 vector, it's a column vector—and so X tilde theta minus Y tilde transpose is 1 by n and so when I take that whole thing together I get a dot product and I get a scalar. If I did it in the other direction, if I put the transpose on the second term—you should check for yourself, just do this kind of dimensionality analysis—you're going to get an n by n matrix. That's not a scalar, that's not a value that we can minimize, that's just something that's totally different. And here, what we're looking for is a single value, something that's a function of theta that returns a real value, that we interpret as a loss, that we interpret as sort of how poorly we're doing, and then we want to minimize that value. And so the direction here really matters, it's really important that that transpose be on the first term here. Now I also want to note that that's because each of these is an n by 1 vector. If we were working with row vectors instead of column vectors, the answer wouldn't be the same.

Okay, so basically what we did on the slide was just a bunch of math to write this loss in a fancy way but really it's really just the loss of the start, this training error, the squared error loss, across our data and what we want to do with it is we want to find some really good choices of theta that will make this as low as possible, that will give us a really good loss aka a really low loss and that, you know, hopefully the goal is that will perform well on future data as a result.

Okay so our goal is to minimize this loss. So all I'm doing here is writing one of the many ways that we wrote the loss on the previous slide, so this is just exactly this linear regression loss from the previous slide. And we want to find a theta that's going to minimize this, that's going to make a really nice low value of this loss because this tells us how poorly we're doing on our training data.

Now here's something that's really cool and really different from everything we've done so far. So everything we've done so far, we've kind of had to beat around the bush because we couldn't actually minimize our losses. So, when we talked about 0-1 loss, when we talked about perceptron, when we talked about that random algorithm from the very first lecture, we were sort of thinking about “here are some ways that we can try to get the loss low but, you know, they have pluses and minuses and we certainly can't guarantee that we'll get the lowest loss.” Even when we talked about logistic regression and gradient descent, we come close to the lowest possible loss. In fact, we could get guarantees we saw that we might get arbitrarily close, as close as we want, the lowest possible loss. And here's what's different in this case: in many cases we can just find the lowest possible loss. We don't have to do any of these sort of iterative procedures, we can just write it down, and that's pretty exciting. So let's do that. Now why can we do that? Why is this special in a way that those other cases weren't? And the reason is that this is a quadratic function of theta and quadratic functions are really nice. If I gave you this quadratic function, you can just sort of find, as we're going to see in a moment, the point that minimizes that function. Now, the caveat is: just because I have a quadratic function, doesn't mean that I can find a unique global minimum. So let's just see some quick examples of why that won't always be the case. Here is a quadratic function. There is no global minimum. You can just always get lower no matter where you are by just going farther out. So this would be a case where there would not be a unique global minimum and I would not be able to minimize this. Here's a slightly more nuanced case: there are global minima but there's just a lot of them and they're all equally good from the perspective of minimizing this function, but there's just not a unique one and this is going to turn out to have some really practical and important consequences in linear regression and so it's just important to keep in mind that, just because I have a quadratic function, doesn't mean that there definitely is always a unique global minimum, but when there is things are good and life is good as we'll see in a moment.

Okay so what we'd like is this first picture. We'd like to say, “hey when there is a unique global minimum, can we just find it? Can we just write it down?” and that's exactly what we're about to do.

Okay so I'm going to get rid of these pictures and in a moment we're going to find this unique minimizing point. And the thing I want to notice with these pictures before we do get rid of them is that: how will this happen? Well the function will be uniquely minimized at a point if the gradient at that point is zero and if the function sort of curves up everywhere. There's a precise way to talk about this if you take a linear algebra course. Basically the idea is that the matrix of second derivatives is positive definite but for the moment let's just say it curves up, that's basically the intuition anyway that if this function curves up everywhere, then there will be this unique global minimum and we can sort of see that from these pictures.

Okay so now let's do this, let's take the gradient and let's set it to 0. This is something that you're probably familiar with from your calculus courses and the big difference here is that we might be doing it in higher dimensions perhaps than you have worked on in the past. Okay so we're going to take the gradient. My first question to you, and again this is one for the chat. Oh before I asked my question, I missed that there was a question. Sorry let's cover that.

How can [unintelligible] change if the labels are multi-dimensional or a complex number? Oops sorry could you just say that again? And let me turn my volume up a little bit.

 [unintelligible] change if the labels are or if y are like common [unintelligible]. I'm sorry, I am just not hearing super well, I'm not sure if that's just me.

Are the other staff able to hear that well? Or is it… Okay, other people can't hear it as well either, maybe just typing the question would be okay? Feel free to like copy and paste or something. Sorry about that.

Okay great how will the loss function change if the labels are multi-dimensional or if they are complex numbers? Okay so the first one, the labels being multidimensional, let's just first think for a moment like how would this arise? Basically I would be trying to predict multiple things, right, so I would be trying to predict, you know, what is my air conditioning bill, what is my gas bill, and maybe a few other things. And so if, in fact, everything that I am trying to predict is like a bill, then what I might do is I might add up my losses because that'll be saying “hey these are all literally numerical losses, they are money that is out of my pocket” and so I want to add them up and I want to say, across all of them, the total loss is the amount that I spent this month or, you know, something like that. Now sometimes, though, what I'm predicting might be pretty different, you know, I'm predicting my air conditioning bill but I'm also predicting how many days a week I'm gonna get sick. This is more difficult because it's not immediately obvious how these compare, right, when I want to say what is my loss, well these both seem like losses: I don't want to get sick too many days of the week and I also don't want to spend too much on my air conditioning bill, but I'm going to have to do some real thought to think about how do I add them together and how do I put them on the same scale. And so I'll have to think about, you know, maybe there should be some factor that tells me how many days a week converts into units of dollars but it won't be as obvious how I make that loss comparison, but I can and it's something that I can do. For instance, by making that comparison in terms of y being complex numbers, I guess it would again depend and I think this is the real underlying answer here on what am I trying to do. Because really all the losses, at the end of the day, is saying “hey I have this practical problem, how much do I lose by getting things wrong?” and that doesn't have to be squared error loss, it doesn't have to be purely additive, it could be something else, but it's really how much do I lose and so you really want to ask yourself for any particular practical problem that comes along, what's a natural way to express how much I lose by getting things wrong and then once you've done that, that just tells you your loss and so this is sort of an example of how you might do that. But the real answer is: ask yourself for the problem that you have how to do that. Great. Okay, so now let's come back to my question here. Gradients, we've done them before. In theory, we know something about them. What is the dimension of this gradient if I—remember our theta is d dimensional, it's a d by 1 vector—and so if I'm going to take this gradient, what is its dimension? This is a question for the chat.

Getting some good answers here, keep them coming.

I see some people putting question marks but you're getting it right so well done.

Okay so the answer that many of you are getting here is d by 1. So remember the idea of the gradient is that I'm in the theta space and it's a vector that tells me where to point in that theta space and so it's got to be the same dimension as theta. So this is going to be a d by 1 vector and, in general, I think this is a good way to think about these things. Don't feel like you're just going in and you have all these, you know, quantities you're just memorizing their dimensions. I think a better way to think about it is what is it doing? What's the point? What are we trying to accomplish with these quantities? And then hopefully that will point you to what is the appropriate dimension. Okay so we should get a d by 1 vector out of doing this and so here is another case where what I have done is I have taken the gradient. You could interpret that in the way that we talked about last time, it's the, you know, for each element of theta (theta_1, theta_2, theta_3, etc.), it's the partial derivative in each of those elements. But it turns out there's a nice neat matrix form that I can write this in. Again, I'm going to ask you, you know, in your own time later on, if this is not immediately obvious to you, to work this out and I think there are at least two things that will be really helpful to do: one, check the case where n is 1 and d is 1. That's the case where everything here is a scalar. We're totally forgetting about all this matrix vector stuff that's going on, you just have an x times x theta minus y. Everything is a scalar. And you should find that this basically reduces to familiar concepts from calculus: you're just taking a regular derivative, you're just taking, you know, a regular square of a value and so you should find that this is something where you can just do the usual derivative calculations that you would do and this should look familiar once you've done that. The second piece of advice is you can check the vector elements. Again, all that this is expressing, all that these, you know, matrix vector multiplications, vector multiplications are expressing is this fast and easy way to express these sums and derivatives that are happening. And so you can just check from first principles, you know, what are all of the elements, the gradient? How can you express that in terms of sums? And then check that it's exactly what these vectors are expressing. And so again, if this is unclear to you, I think the first exercise, checking the n = 1, will give you intuition about why are we getting these values, you know, and help them seem to look familiar and if you just want to convince yourself that these are the right matrix vector operations, check those vector elements.

Okay again let's just do our own little unit testing to make sure that this even makes sense. So again, when we do this sort of dimensionality analysis, we're not proving that these two things are equal, you know, there are plenty of things that we could multiply that would satisfy these dimensions but would not be equal, but we're checking that this even makes sense and this is always a great idea, again, in your code and your math and everything to do this kind of double check. Okay so we know that X tilde is n by d, we know that theta is d by 1, and we know that Y tilde is n by 1 and actually, technically speaking, we've already sort of gone through this and we know that X tilde theta minus Y tilde is an n by 1 vector and here this is just reiterating that. 

So, X tilde transpose is d by n because it's just the transpose of X tilde. So we take a d by n matrix times an n by 1 vector, those n’s agree, so these are two things we can multiply. The outer values are d by 1 and so we get exactly a d by 1 vector out and so that looks good, that looks appropriate, that's a nice little check that this looks okay. Okay now the point here, though, wasn't just to find the gradient. So if we were doing gradient descent we would just, you know, find this gradient and then sort of move in the direction of this gradient. We're trying to find the point where the gradient is equal to zero and solve for that directly, so that's a little bit different. So here what we're gonna do is we're gonna set this equal to zero and solve for theta.

Okay so let's go ahead and do that. So all we're doing in this line is noticing that (2 / n) is just a constant and so we can multiply both sides by (n / 2) and the zero is not affected and that goes away. And then we're taking that X tilde transpose and just putting it into both terms. Okay, so now once we have this equation, well, we can just take one of the terms and put it on the other side. So all we did was we said we have minus something equals zero. So we can just bring that something over to the other side. It's like we added X tilde transpose Y tilde to both sides. And that's basically all we're doing in any of these manipulations is multiplying by one and, you know, adding zero and so that's what we've done here.

Now let's notice that if we multiply both sides by the same constant, this equality would not change. If we multiply both sides by the same matrix, this equality will not change, and so let's choose a particularly good choice of a matrix to multiply by. Here, we've multiplied both sides by X tilde transpose X tilde, all of that inverted. So we just took the equation that we already had and pre-multiplied it by the same thing on both sides and so that's not going to change that this equality is true.

Now why was this a good choice of matrix to multiply by? Well we ultimately want something of the form theta equals something, because we’re trying to solve for theta. The whole idea here was to find where is the gradient, what choice of theta is there at which the gradient is equal to zero? And so we're hoping to end up with an equation that looks like theta equals something and so we notice that now we have something of the form: a matrix inverse times a matrix. And the way that matrix inverses work is that when you multiply a matrix inverse times a matrix or a matrix times its matrix inverse, either direction, those are just going to cancel out. And so those cancel out on the left-hand side, we're just left with theta, and we're left with a formula on the right-hand side and crucially it doesn't have theta in it, so we've solved for theta at this point.

Okay now we said that this is a unique minimizer. If the gradient is equal to zero, so we check that.

It's only a unique minimizer if the function curves up though.

Now again, this is beyond the linear algebra we're going to get into in this class. So I'll just tell you: you can check this kind of thing, that it curves up, with what's known as the matrix of second derivatives. Do not feel that you have to derive this, just don't worry about this. The thing that I want to tell you though is is that, if the function curves up, this matrix is invertible. You'll notice this matrix, up to a constant, is exactly what we need to invert to get theta. And so if there is a unique minimizer, then that formula for theta we just talked about will be fine because you can take that inverse. If there's not, you won't be able to take that inverse.

And so this is how you see it in the formula is what happens with that inverse and we're going to talk in just a moment about what's actually going on, why would this ever not be invertible from a more intuitive perspective.

Okay let's, before we do that though, let's just back up for a second and assume for the moment that we have a problem where we actually can't invert that and we can solve for theta. So what is this going to look like? What it's going to look like is: you're going to start off with some data. So we said that for our data, we have a bunch of features and a bunch of labels. So here's an example of a one-dimensional feature space with a bunch of labels. Once I have this, this defines a loss function j and it's a quadratic in theta. Now here, remember we played this trick where we made the offset be actually the last element of the theta vector. So here we really do have a theta_1 and an offset, we're just calling it theta_2, which is a little bit confusing but again it's just because we did this trick where we said that our last feature is 1 and so our offset goes to the n. It's annoying but that's what's happening here. So really, theta is two dimensional. We've got our, you know, sort of slope effectively. I mean, you know, we saw it's not exactly the slope but the thing that's defining it and the offset and so that defines a j and now what's really cool is that we don't have to do gradient descent or anything in this, we can just go straight to the optimum with our formula that we just came up with, this function of theta. 

That defines our particular theta. That defines a particular line because it defines a particular theta and so now we can draw that line and, in particular, this is actually the line that we get by doing this minimization here. So in this particular case, I just randomly generated some data, I plugged in exactly this matrix, and this is the line I got out and it looks pretty good, right: it looks like it minimizes the loss in the sense that the predictions are near the training data points.

Okay so that's what this formula is doing, it's letting us pick out just a particular value of the theta which is effectively giving us a line and then we're plotting that line. Now let me just show the same thing but in a two-dimensional feature space.

So in a two-dimensional feature space, it's going to be much the same: we're going to start off with a bunch of data, but now that data has x_1 values and x_2 values and labels.

Now that gives us a J(theta), but in this case, because we have to say what's the direction in the x_1 space and the x_2 space and the offset, that's a three-dimensional theta vector and so I can't really draw this. I mean we could get clever and sort of try to find ways to look at it, but I can't really draw this function so I'm not going to but I'm going to note that it exists. And still, whether I can draw it or not, this formula is going to give us the formula for the particular theta that's going to optimize it and so now once I have that theta that corresponds to a hyperplane— this is the hyperplane it corresponds to—and I can just draw that hyperplane.

And so again, I just want to emphasize that despite the name linear regression, just as in classification, linear here really means hyperplane. So if we look at what does this prediction look like when we have this two-dimensional feature space, it's not a one-dimensional line, it's this higher dimensional hyperplane.

Okay so this is what everything looks like when, in fact, we have a nice function that curves up and this unique minimizer but now let's talk about what can go wrong. Why would I not have a nice function that curves up and a unique minimizer? And it turns out this is stuff that really happens in practice and so it's definitely worth being aware of.

Okay so before we say what goes wrong, let's just recap we just said “hey, what what we want to happen and what often happens is that I have a bunch of data, you know, it spans some nice values in x_1 and x_2 or whatever my feature space is, and it has some nice labels and then I can use that theta formula that we just came up with on the previous page to find this best hyperplane, that's one that minimizes the squared error loss.”

Okay when would that go wrong? What would go wrong is if there wasn't a best hyperplane. So here's an example: suppose I want to predict my air conditioning bill and so what I did was I measured the temperature outside for my first feature, so let's call that x_1, and then for my second feature I decided to also express the temperature outside but in Celsius instead of Fahrenheit. So x_1 is the temperature in Fahrenheit, x_2 is the temperature in Celsius. They encode basically exactly the same information and so no matter what my y's are, the x_1 and x_2 are going to be on this perfect line with each other and so now if I go in to fit a hyperplane, which we just said was sort of the whole thing that we're doing in this case and the whole thing that this theta solution is solving for, well this is a hyperplane that fits this. But this is a hyper plane that fits it just as well and in fact you can just imagine if you just take this hyperplane and sort of rotate it around this data, you're just going to keep getting hyper planes that all fit this data exactly as well from squared error laws.

And so there isn't a unique best hyperplane, in fact, there are infinity best hyperplanes that are all equally good from the perspective of squared error laws. When this happens, that inverse, that matrix inverse, will not exist. Okay, I think there's a question. Hoping my mic works now. Oh great. Yeah okay, so does theta have the same geometrical interpretation here as it has before? So is it still perpendicular to the hyperplane? Yes exactly, that's exactly right, and you're thinking about it absolutely perfectly. So theta is still perpendicular to the hyperplane, that is how it describes the hyperplane. So we've got our theta that is, sort of, again, exactly the normal vector to the hyperplane and our theta naught is still the offset, just as before. The only difference is what we do with the hyperplane. So the geometric intuition that this question has is absolutely perfect and absolutely worth enforcing and the only key difference is that, before, once we had the hyperplane, we said “oh we'll predict +1 on one side and -1 on the other side.” In this case, we're using the hyperplane directly to do our predictions. We're saying, “hey if I have a new x value, I'm going to look at what is the exact value of the hyperplane at that x value and then I'm going to say that's my prediction.” But yes, all of the geometric intuition is exactly the same and it's just what we do with the hyperplane. So perfect, great. Okay so and that's true of all these hyperplanes too. The only issue is just that all of these hyperplanes from the perspective of our squared error loss are equally good and because of that, that matrix won't be invertible and we won't have this unique sort of best hyperplane and so, you know, you couldn't just use this formula directly that we just derived because you would get some sort of error, you know, you would say your linear algebra system or whatever you're using and Python would say, you know, “hey there's an error, this isn't invertible.” Now, there are ways to get around that, but essentially what we're going to have to do, is we're going to have to decide “okay, between these infinity of hyperplanes, how do we decide which was the one that we're going to report? Which is the one that we want?” And that's the question that we'll be answering very shortly. Before we do that, before we find the answer, before we discuss an answer, let's also talk about more problems that can arise. Now a related problem is: yes, sometimes you might have a unique best hyper plane but on a technicality. In particular, imagine that you had this exact situation I just set up but my data was a little bit noisier. So here's an example: suppose that I'm interested in my air conditioning bill and so I have a thermometer that measures the temperature right outside my apartment and it says “hey here's the temperature, you know, say in Fahrenheit” and then I have another thermometer that's like one door down from that, you know, and it also measures the temperature. Now in this case, technically speaking, there just is going to be a little bit of noise, because no two thermometers are absolutely perfect, they're gonna, you know, have a little bit of noise and it'll be a little bit off and I think that you will find that actually you can invert this matrix, you will not get an error from your program that says “hey matrix not invertible” but the problem hasn't gone away, you know, yes technically your program will let you do it, it will let you find this best theta, but should it? You know, you still have so many of these planes that, yes technically, one of them is very slightly better than the other one, but they're all sort of effectively very, very close in how well they perform and we should think a little bit more about which one really is the one that we want to report, not just put it into the formula and go and not think. In general, I think that's a great meta point for machine learning and probably for life: never just put stuff in the formula and go and not think about it,  always think about it. Always think about what's going on and this is a case where I think you will benefit by doing that. Okay so we still have this question. Even if we could invert this matrix, we still kind of want to ask ourselves which hyperplane should we really report? Maybe we should think about it a little bit more and we will so no worries, we will address this problem shortly. Before I do that, again, I want to emphasize how much this can come up very easily. So back when we talked about features, this actually even came up, I think, in a student question, we had redundant features. If you think about one hot encoding, one of our features is exactly a function of our other features, and so this just immediately will come up then, right? You have that one of your features is exactly a function of your other features and so there won't be a unique hyper plane because of that. Another issue is that when you measure a lot of things in real life, they often tend to be correlated, so they often tend to have this problem. Just because if you measure enough things and they're all sort of around each other and you're thinking about the same problem, it's just very common that this would come up. Another way that this can arise is that if you just have a lot of feature dimensions. Now unfortunately, again, we can't really plot things above two dimensions so this is hard for me to plot, but I will give you a plot and then you're going to explore this so much more in the lab and so basically let me start with a very simple plot. Suppose that I have a one-dimensional feature and I have a single data point. I think you will all agree that there are many hyperplanes that go through this data point, here are some of them. Again, there's an infinity of hyperplanes that all go through this data point. If I had a two-dimensional feature and I had two data points, there would be an infinity of hyperplanes that would go through those two data points. If I had a thousand dimensions of features and a thousand data points, there would still be an infinity of hyperplanes that go through those data points. So why would I have a thousand features, why would I have more than a thousand features? Well this can often arise in biology, in genetics. It can also often arise, you know, if you think about the sort of tricks that we've been playing, right? Like think back to polynomial features. We just made a whole ton of features out of nowhere by using polynomial features so that's an easy way to get into a really high dimensional space and you're going to have exactly this problem that we just discussed that there's this infinity of hyperplanes and therefore no matrix inverse if you get too high dimensional. So again, you're going to be exploring that so much more in the lab and you're gonna get to see, you know, sort of what are the consequences, but it's really coming back down to this exact principle.

Okay so fundamentally, for all of these reasons, we need to think how do we choose among the hyperplanes. Well we've already developed a little intuition about this. So you actually had a problem and I believe it was homework three that asked you to say “oh, what are the most influential of your features” and those are the ones with sort of the biggest magnitude, right? And so something that we could say is, well, if we don't really know what's going on, if there's not really a good way to distinguish between different hyperplanes, maybe we should choose or we should prefer the theta values that are near zero unless there's a strong reason not to from our data. We shouldn't go and say yes, this feature really matters when we just don't have the evidence to support that. This is very much like what we did with logistic regression, right? With logistic regression, we said “hey, you know, if left to our own devices, if we didn't change anything, we make our thetas have humongous magnitudes” and that doesn't express the uncertainty that we really have. We wanted to instead choose thetas that were near zero unless we had a strong reason to believe that that shouldn't be the case. And so we're going to do exactly the same thing here: we're going to express a preference for the theta as being near zero unless we have a strong reason not to and we're actually going to do it in exactly the same way that we did for logistic regression.

Okay so we have all of these problems. We think that all of them are fundamentally about choosing a particular hyperplane among many that seem roughly equivalent. We're going to express a preference for our theta components being near zero, how can we do that? Well we can regularize.

So we're going to take linear regression, everything we just did, and we're going to add a penalty, a penalty to big thetas just like we did with logistic regression. When we do that, we get something that is called ridge regression if we do it in the exact same way. So here, this was our existing linear regression training loss, so I'm just rewriting it, it was something that we had on the previous slides. This is one of the many forms that we wrote it in, and now we're going to form the ridge regression optimization objective—so right now, what we have is just our regular linear regression optimization objective—by adding this square penalty. Okay, so a few points about this: one, this is not the only penalty that one could add that would penalize very large theta. This is a particular choice of a penalty that penalizes large theta. It's the same one that we had before, but actually there are people who absolutely use other versions, this is just going to be the one that we focus on. 

Two, notice that when lambda is equal to zero, we get out exactly the original, the original optimization objective for linear regression. Here, we're going to set lambda greater than equal to zero—or sorry, in particular, greater than zero—to make sure that there is some penalty associated with having a large lambda. So, sorry, with having a large theta. So basically when we have a large theta, we incur more loss, we say that that's bad, we have things that we add on that make that bad. Incidentally, and this is a question from the chat, for the chat, what happens if lambda is less than zero? Why, you know, should we sometimes set lambda less than zero, would that be a good idea?

Great okay, so everybody notices, one, that if I have lambda less than zero, then I can decrease my optimization objective by choosing thetas that are larger and larger, and so in fact, I could get the optimization objective arbitrarily small just by choosing larger and larger theta and it doesn't even matter what direction, just any direction, and so this is sort of useless because all it is is telling me “here is a penalty for really large theta, it doesn't matter how small lambda is in magnitude as long as it's negative, I'm just preferring an extremely large and arbitrarily large theta, one that sort of goes off without bound.” And so that's not really useful to us. What we really want to suggest is that there's really a trade-off here, that we get some loss from not being near the data. That's what's happening with the first part of this optimization objective and we get some loss from just having a really large theta and so that lambda is just controlling the trade-off and again, because we don't really know exactly how to make that trade-off, we'll often use something like cross-validation as we said before.

Okay so now, what's going to happen is, just as we just solved for the theta that is the unique global minimizer when that unique global minimizer exists, we're now going to do it here for the special case with no offset. The reason for that is entirely because it's easy to write down, you can absolutely do this for the case with an offset, that could be a fun exercise, but I just want to say we're only going to do it for this. Okay there's a question.

Yeah a couple questions about the regularization term. Yeah. So why would we want the theta magnitude to be near zero? Since if you have a greater magnitude that means the feature is more impactful in the prediction. Yeah exactly, so let's go back to the previous slide, so the question is, you know, why do we want that theta magnitude near zero? So on the previous slide, we said “hey, if in fact I have a super impactful feature and my data can really pick it up, then linear regression, forget about regularizers, is going to pick that up.” But here are some cases where I run linear regression and there are a bunch of totally equivalent things, there are a bunch of totally equivalent settings of theta, that all give me the exact same results, that all give me the exact same loss, or in the noisy case, they give me almost exactly the same loss. Like it's not exactly the same, but it's so small as to be, you know, not really something that's going on. I can't really tell, you know, I can't really tell. And so what we're saying is that, in these cases where there is literally basically nothing in the data to distinguish between different thetas, then I should probably set the thetas as close to zero as possible because I don't want to erroneously say “yeah these were super impactful features.” Because I totally agree with this question that if I had a really large theta, I would probably think to myself those are really impactful features, those really matter, but if I don't have evidence to support that, then I don't want to say it. And so when I don't have evidence to support that, I would prefer to choose the theta that is near zero and so that's what we're doing here. We're saying “hey, when there isn't evidence in my data to support choosing a really big theta, one that demonstrates that this future has a really big impact, then I would prefer to choose that theta near zero,” and so that's what's happening now in this penalty is we're saying there's a trade-off, you know, if there's a lot of evidence in my data to support there being a really impactful feature, then that should come out in the first part of this optimization objective, but if there's not a lot and everything's kind of the same, then the second part of this optimization objective kicks in and it says “hey let's prefer that theta is there near zero.” Great. Another question is: so, how does the ridge regression prevent the coefficients from essentially being noise in theta, it's like very small but not zero? Oh so it doesn't, yeah, so this is an interesting question. So let's say that I ran this. All it's gonna do is it's going to ask my theta values to move towards zero, in general, and it's going to prefer ones that are near zero when everything is equivalent, but it's not necessarily going to take them all the way to zero and so actually if that's something you're interested in—and people absolutely are, so there are some people who want to say “hey I want to pick a subset of my features that are the ones that matter and I would like something to very clearly tell me which ones are included and which ones aren't”. People often use what's known as an L1 penalty, instead of an L2 penalty. This is something called a “lasso” instead of the ridge regression, there are other ways to do it too and this will make some of the theta values just exactly equal to zero and then you can just look at the non-zero ones and do something with those. So absolutely, this is something that people do and ridge regression does not do it, it just tends to move things smaller, it won't set them exactly to zero.

Cool okay. So now we have this regularizer. We said that we're gonna take lambda greater than zero or equal to zero, if we're not using the regularizer, but we're gonna use the regularizer here and we're not gonna take lambda less than zero because we saw some things that could go really wrong there. And now we're just going to solve for theta in the same way that we did before: we're going to find the optimal theta and we're going to ignore the offset purely for mathematical convenience. Okay, so if we do that, this is our ridge regression, this is—I'm literally just rewriting the same thing but without the offset—so this is our whole optimization objective. We also saw that it could be written this way, completely equivalent, this is just sort of the manipulations we did on that previous slide.

And now we're going to optimize this. So in particular, we're going to say, “let's take the gradient and set it to zero.” Now something that I did on the previous slide, something that [unintelligible] do, and something that I'm doing here, is a little bit, I'm pulling a fast one on you. So when I say set to zero, what's the dimension of zero here? So this is one for the chat.

Good stuff. Okay great yes, you're all totally nailing it. It's d by 1. It's got to be the same as the gradient because we couldn't put this equality here, but it's a little bit of a fast one, right? I mean usually when we say set to zero, we think that we mean the number is zero and this is something that happened in the notes, that happened when we set it to zero on the other slide, and it's happening here. What we really mean is that we're setting it to the vector zero, that is to say we're saying each element of this gradient we're setting to zero separately and so that's worth keeping in mind that this might not be the zero that you're familiar with, this is actually the vector zero that is a column vector, that's a d by 1 column vector. So that's a little bit of a fast one, make sure you go back in the notes and and feel comfortable with what's really going on there. Okay, so we're going to set this gradient to zero.

We're not going to go through the derivation again this time, it's in the notes and also you can just redo everything that we did on the previous slide and you're going to get out the following. But I do want to talk through this. So the first thing to notice is, if lambda is equal to 0, this is exactly the solution we got last time, so that's a good check to begin with. There's this new matrix called I, let's just briefly say what is I. I is known as the identity matrix, it's one on the diagonal and zero everywhere else. Okay another quick question for the chat: what is the dimension of I here?

Okay we're getting a few different answers on this one, so I'm glad that we talked about it. So let's first do some unit tests on our thinking about this. One, I is always a square matrix, but you might not have known that. Here's how you could know: we're taking its inverse. We're typically going to do that for square matrices, so it should be something of dimension by dimension. Okay, let's figure out what is its dimension. Well it's got to be something that we could add to this x tilde transpose x tilde, so let's remember what are those dimensions. Well, x tilde transpose is n by d.

Sorry, x tilde is n by d so x tilde transpose is d by n and so we're adding I. Now n and lambda are just scalars so they're not changing anything about the dimensions here. We're adding I to what seems to be a d by d matrix and so I is going to have to be d by d in this case. Now here's a tricky thing: this I is much like zero: whenever I appears, it doesn't have to be a d by d I, it doesn't have to be an m by m I, it could be anything and you always have to figure it out from context and so this is another one to be careful about. Just like the zero, you have to ask yourself “okay what are the things that I'm doing with this, what am I adding and multiplying?” and therefore I'll know what the dimension is. And so in this particular case, it's going to be d by d.

Okay, now again, something that we're not getting into, but I'm just going to tell you, is that the matrix of derivatives, we can calculate it and it tells you whether this curves up and here's a fun fact: this particular matrix derivative, this matrix of second derivative, tells us that this always curves up and it is always invertible when lambda is greater than zero. So that's very different from what we just did when we were just doing this least squares this, you know, without the penalty, sometimes it's not invertible or sometimes something goes wrong there. In this case, we're safe: it's always invertible as long as lambda is greater than zero. There have been a few questions now. I'm not sure if that's a new question or not, is it? Yeah, should there be like sum from i equals 1 to n in the matrix form? Oh no that's totally a typo, thanks for catching that. Yes okay so I will correct this in the notes we post online. I like to think that these typos are just keeping you on your toes, so well done, but yes, so once we've taken the norm, there is no more sum from i equals 1 to n and so that should just not be there, that's totally a typo. Thanks.

Great. Okay cool. So this all sounds really good, I'll just again mention that you can also solve for the minimizing parameters in the case with an offset, it's just a bit more math, we're just not doing that math, but it's not like a deep thing that's happening here. Okay so you can do this, you can solve for this, you can get out, you know, again the whole idea is you solve this, you get your theta, you get out your hyperplane, you can plot your hyperplane, and that's great. So are we all done? You know, have we solved everything in linear regression? Okay, well, let's make a couple notes on this and particularly let's make a couple of notes on features. You know, we talked previously about, in classification, how it's not just a matter of, you know, somebody gives you some data and then you just immediately run a classification algorithm on it, that often somebody gives you some data and you have to be really careful, you have to be really conscious about the features and turning them into useful features for your algorithm and that's absolutely the case here as well. And so here I'm just showing again the linear regression with the squared penalty. This is exactly from the previous slide, we're going to take some notes on the features.

Now a big important point, and this is true for the last time we used a square penalty as well, for logistic regression, is that we are implicitly assuming that the features are on the same scale by using this penalty. If you use this penalty, you're saying that lambda times (theta_1)^2 is penalized just as much as lambda times (theta_2)^2, is penalized just as much as lambda times (theta_3)^2 squared. If you go back to our features lecture, so this is lecture 3, we saw that if you change the scale of the data, you change the scale of the theta. That was not a conscious rhyme, but it turned out to rhyme. But if you change the scale of the features, you know, if you just happen to put them in different units, you know, or something like that, you're going to change the scale that the thetas are on and, in particular, I mean the normal vector theta not the offset. And so here, and that's what we're looking at here anyway, and so here, you have to notice that if you don't do something to make sure that your x's, your features, are on the same scale, then implicitly your thetas will be on the same scale and they'll be sort of penalized differently and so if you want to make sure that these penalizations really make sense, you've got to do something like standardization and, you know, these general techniques to make sure that all of your features are on the same scale.

Now related note is features still matter. You know, we went through this whole lecture on featurization, on turning your data into useful features, and all of that is still true: you could equally apply basically everything in that lecture to the regression case and I will just point out that this matters and it's extremely important in so many aspects of life. Just as one example, among many: featurization for regression was actually a big news item this summer. So I don't know if anybody followed the Bolivian election, there was an extremely contentious and bitter election in Bolivia. There was an analysis of whether vote rigging had happened. There is a huge amount of nuance around this issue and I'm just not going to get into the whole issue, I encourage you to read up on it if you're interested, but if you look at this, one of the big issues in this analysis—so somebody did this analysis, it was a regression style analysis to decide whether vote rigging had occurred—and it turned out that they did not featurize correctly, the time stamps were sorted alphanumerically instead of chronologically and this made a huge difference. So in particular something like, you know, 1:01 pm was listed before 1:00am because of the alphanumeric rather than chronological and what they were trying to detect was if a big change had happened and so this might make it look like a change had happened. So this is all just to say featurization really matters and it comes up in very real life, this had huge consequences. Again, I don't want to suggest this was the only thing that was happening, that everything hinged on this one element, but this one element was very important and it did matter. Now a couple of notes about how this could come up in your life: first of all, I think that there is sometimes this idea, this fantasy, that what happens is that somebody who has applied gives you a data set and then you, as the machine learning researcher, do your machine learning magic and then you give it back with some machine learning magic output to that person and you can just see from a lot of the things we've looked at here, including this, that that's just not the case, that you can't just not look at your features, you can't just not think about your features and about your data and what your goal is. You have to give that thought, you have to be very careful and very conscious about it and you have to really engage with it and probably talk to people. Now a second meta point that I just want to make while we're on here is that this is not the end of the world to make a mistake, you're going to make mistakes in your analyses. Literally anybody who's done any kind of meaningful analysis has made a mistake, but what's really important is that it be detectable and so a real issue that arose in this particular case was that people did not share their code in their data and so the mistake was not known for a very long time and people couldn't correct it and, you know, the analysis wasn't replicable. If you share your code in your data and you communicate with other people, then together you can find any bugs or mistakes and it's just not the end of the world to make a mistake but by working with other people you can avoid that. Okay so those are just some sort of general machine learning life lessons but they very much relate to this issue of featurization.

Okay so great, we have this awesome way to optimize linear regression, you know, we don't have to have to worry about gradient descent, right, we talked about in the past how, you know, if we had an optimization objective like the one that we have here, we could use gradient descent: we could start from a point and we could progressively try to get closer and closer to the optimum but here we don't have to do that, you know, we have this what you might call a direct solution, a closed form solution, analytical solution. Basically what we mean by that is you can just write it down, you don't have to do this iterative procedure, you can just say “here's the solution. Boom there it is, that's the optimum” And yet, some people actually use gradient descent for linear regression, for ridge regression, for all of these things that we've been talking about. So let's ask ourselves why is that? Why would you use gradient descent here when we have this awesome, you know, analytical closed form direct solution, why not just go straight to the answer if you can do that?

The reason is that, even though you have a closed form solution, that doesn't mean that you can get it in zero time. So we can't talk about accuracy, about saying “hey this is such a better solution, we go straight to the optimum and everything is great.” You know, there's actually a few answers to this question, but this is one of them: that, you know, we can go straight to the optimum, we have to talk about running time and we have to say well what are the actual running times that we see in practice here? And it's going to turn out that this going straight to the optimum solution sometimes is very costly. It might not seem like that, it might seem like an iterative solution would always be less costly, but that's not really the case. So you have to talk, whenever you talk about accuracy, you have to talk about running time at the same time. So if I have a solution that I can go and get the perfect solution and it just takes me six months to run, I effectively don't have a solution for many problems because if I don't have six months, then I'm just not going to get that solution right. There are some problems in machine learning that turn out to effectively take the age of the universe. I'm not going to run them, so I effectively don't have the answer to that problem. This happens to go the other way. This is, again, just like machine learning life lesson: running time doesn't mean anything without accuracy. So if somebody tells you they have this cool new machine learning algorithm and it takes constant time or it's super duper fast, they have to also tell you about accuracy. It's very easy to come up with methods that have a fast running time. It's much harder to come up with methods that have a fast running time and give you a good result, especially a provably good result. Okay so we need to measure accuracy for the running time that we have. What is the issue with running time that arises here? Well let's look at our formula again. So this is the formula that we're saying, it’s a nice closed form formula, it takes us straight to the optimum. Why would this take a long time to run? The issue is the matrix inverse. If you are ever working on very large data sets, you should always be skeptical of matrix inverses and you should always ask yourself, you know, what's the size of the matrix inverse? In particular, we just said on a previous slide that this is a d by d matrix within the inverse and a fact of life is that matrix inversion is somewhere between quadratic and cubic in d, nearer to cubic, and that's just extremely expensive if d is large and actually there are some very practical, very real-life d's, you can get easily into the millions and billions and then this just can become prohibitive and it's not really something that you're going to run in practice. And so, if gradient descent can get super close to your optimum in a reasonable amount of time and this exact solution cannot converge, cannot even give you any kind of answer in a reasonable amount of time, you actually will prefer gradient descent.

Okay, so we can write out gradient descent. So it turns out we have this nice convex problem. Again, just as logistic regression was convex, linear regression is convex. And then once you add that penalty, you get an even nicer convex problem that's really going to have, you know, sort of a nice unique optimum as we saw and so we can apply gradient descent.

So I'm not going to spend too much time on this, it's really, in some sense, formulaic at this point. You want to take your gradient descent algorithm and just apply it to our objective, either our linear regression objective or our ridge regression objective, and as usual, it's going to take some initialization: it's going to take some step size parameter, it's going to take some kind of ending parameter. Here, let's just say it's a number of steps t. 

And so we initialize all of our thetas, we go through all of our steps, we update according to the gradient and just like logistic regression, there's a lot that's being written here, but I wouldn't worry about it. This is like you just apply linear regression, you know, you apply gradient descent to linear regression and this is what you get out. And hopefully that's actually true this time unlike logistic regression where we had a typo but, you know, great for checking and this is I think a good thing to, you know, go through an exercise and check yourself. And then we should sort of finish up and return. And so here, remember, we talked about there are a lot of ways that we could decide when to stop, here we're just stopping based on number of iterations, but an emphasis that I want to make, which is an emphasis that I made with logistic regression, is that this depends again on stepping through all of your data before you make any change in theta. So we just talked about how if you do this exact solution, you have this problem that you have a really slow algorithm in the dimension. Here you have a really slow algorithm in the number of data points, which can also be extremely large and is very large in a number of important machine learning problems and so that might be something that you don't want. And so there is an alternative. So again I'm going pretty quickly through this slide because we're already familiar with gradient descent and most of this is really just applying linear regression to gradient descent just so that I can talk about stochastic gradient descent very briefly. So with gradient descent, we had our objective and we would just find the gradient for it. With stochastic gradient descent, what we do is instead we notice that this objective has a particular form, that's just the sum of these individual terms, these f_i’s. So this is something that is true for linear regression but it's also true for logistic regression.

So basically, there's some lost term per data point. In fact, general training error loss as we've set it up would look like this.

And so the idea of stochastic gradient descent is to look at one of these at a time rather than the entire data set. It's going to be noisier, it's going to be, you know, not as good over a particular iteration, but because the iterations are so fast and so cheap, it can be better for the time that you have.

And so with stochastic gradient descent, we again have initialization, we can have a step size parameter and we again have some kind of stopping criteria.

We initialize our parameter, we stop after, in this case, just some number of iterations.

Now again, instead of going over a whole data set, we go over one data point at a time. So we randomly select with equal probability among all of our data points and we step in the gradient direction for one data point.

And so there are two things that are different here: eta depends on t, it will get smaller as t gets larger. We actually could have done this for gradient descent but we can do that here too and it's important to do that here. And now, instead of doing a gradient in the whole data set, we do it with just one data point and then we return. Okay, I'll encourage you to read more about stochastic gradient descent in the notes, it's going to come up again and again. This is an extremely widely used algorithm and it's very important because we deal with these very large data sets and it has a number of other nice properties. So today, we've talked about linear regression, we've talked about regression in general but linear regression in particular, about ridge regression with this penalty, and about various ways to optimize that and it's not always clear that you want to just do this immediate optimum, sometimes you want to do gradient descent or stochastic gradient descent. Okay I'll see you next time.
