Okay it's that time. So before we get started today, I just want to remind everybody that next Tuesday, November 3rd, is election day in the US, so anybody who is able to vote in the US—hopefully many of you have already voted—but if you haven't, now is a great time to make a plan and stick to it. This is a real life picture of my pointer finger. I have voted already in Massachusetts and I'm excited for that. There's a lot of interesting things on the ballot in Massachusetts, including ranked choice voting, and it might be the case that wherever you are voting that's true as well. Also there's this cool website, it's a student group at MIT that's all about get out the vote, so you might check it out: vote.mit.edu. Cool. So first of all, let's just recall what we have been doing this whole class. We've really been talking a lot about supervised learning—about regression, about classification, about these particular instances of supervised —and in those cases, we've been interested in making decisions. Remember back in lecture one, we said that machine learning was a set of methods for making decisions from data, so we talked about decisions like whether I put on a coat today or not. And so far, our decisions are separate: every time a data point comes along, I make a decision and that's the end of the story, like I decide whether or not to wear a coat today and now I'm done. And of course, sometimes, that's just really not the case: when I make a decision, it could change the state of the world and in ways big and small, right? So like suppose I own a fish farm and I'm breeding some fish and I make a decision to not feed my fish, then my fish will die and now I have to make a totally different set of decisions like how to get a new job. And so I have changed the state of the world by making that decision and I can't make the same set of decisions with the same losses and gains that I did before and so that's what we're going to talk about today: we're going to talk about decisions that can change the state of the world and are going to change what you do. And so in particular, first, we'll talk about changing the state of the world with aptly named state machines and then we'll talk about making decisions with the aptly named Markov decision processes. So that's where we're going today.

Okay so first, let me just note that I'm going to be having an extended farming example in today's lecture. So let's just have a little bit of background on farming, definitely not a lot, I'm not a farming expert, but for centuries farmers have known that planting certain crops depletes the soil. So you might go from a rich soil to a poor soil after you plant a certain type of crop and so there are various ways that people deal with this. So one is you might rotate crops: so some years you plant a crop and some years you let the field life fallow, that is to say you don't put anything in it, you don't plant that crop again, and so that gives time for the soil to regenerate. If you grew up in the USA, you might have heard of basically our most famous agricultural scientist, George Washington Carver. He's extremely famous, he developed techniques to improve soil between plantings. He tended to focus more on different crops that you could have in between planting cotton because cotton would deplete the soil and so he would promote, for instance, peanut planting in between the cotton planting. Today we're going to focus more on whether to plant or to fallow and this happens to be the subject of a bunch of research papers. So here's a research paper that's more recent than the very old painting and even George Washington Carver's time, but actually you can find research papers on these farming problems to the present day and, in fact, this research paper and those research papers use exactly the things that we're talking about today, these Markov decision processes, and so they are a bit more complex in the models that they use, we're going to use a simplified model, but this is absolutely something that people are using in practice. Okay, so the first thing that we're going to do is we're going to develop this model for the state of the world and for changing the state of the world and that's going to be, again, this aptly named state machine. So in this case, if we're talking about a field, like maybe I've decided to rent a field and farm in this field, then here I might have as possible states for my field, it might have rich soil or it might have poor soil. So here are two possible states in this particular example.

Now I also need a set of possible inputs, these are the things that I can input into the system, and in this case the possible inputs that we've discussed so far are planting the field or letting it lay fallow, so those are my possible inputs into this system. Okay, now, I'm going to think about a sequence of inputs and how they change the state and in order to think about that, it'll help me to think about where did I start, what was my initial state? And so if I imagine that I'm renting this field from somebody else and I started off, maybe afresh and nobody else was using the field before. In my example it might be that my initial state is that it is rich soil, it's great soil, it hasn't been being planted for…, nothing's planted on it for a while and so it's ready to go, it's ready for us to do some farming. Now, I'm going to need a way to understand how I go between different states and that's what the transition function is going to do for me, it's going to say, it's going to be a function that takes in, first a state and then an input, and then outputs another state. And so, in particular, here is an example of a transition I might make if I have my input state or sorry my first state be poor soil and my input is that I decide to let the field stay fallow this season, then after I do that, I'm going to get rich soil. So I start with poor soil, I let it lie fallow, I get rich soil. Now of course, I need to specify what's going to happen over all the different inputs that I might have and so another input I might have is I might plant my field, in which case I expect that maybe my poor soil is going to stay poor. Now I also have to say for every possible state that I start with, for every possible action, what's going to happen. And so I would also need to specify what happens if I start from rich soil and I expect that if I plant on top of rich soil, next season my soil will be poor. If I plant on top of rich soil and I fa…. or sorry if I just lie fallow on top of rich soil, I'm going to get rich soil again. Now implicitly what I'm talking about, there's a notion of time and so when we think about this transition function, we're thinking about going forward one unit in time. Here it might be a growing season or a year, but the idea is I have my starting state right now, I have the input that I put in, and then at the next time step, I have this output, this new s. And so for instance, I might say, “okay season zero, before I even rented this field, the soil was rich. Then I took my rich soil, I planted, now I'm going to make a transition to find out what happens in season one, what do I get out, what type of soil” and so here's my first question for you in the chat: what is the output of this based on our specification of the transition function in the diagram above? So we've actually fully specified the transition function with this diagram and so my question again for the chat is what type of soil or what state am I going to get out here for s_1? Okay great. Everybody gets it: we're getting poor soil and again we just see that from the transition. This is this next timestep, one, and we're gonna see that we start from our rich soil, that's s_0, and we've just specified that. We decided to plant, that's just a particular action we could choose, and then we just see on this diagram where that goes. That goes to the poor soil and so the output here is poor soil.

Okay. Now something that can often happen, although we're not going to get into it too much in this lecture, is that you might not observe the states directly and so certainly that seems very plausible in this farming example. Like you don't necessarily know directly if you have rich or poor soil, you'll probably have some kind of measurement apparatus that will measure something about the soil for it. So for instance, at least according to this research paper I linked in the beginning, the amount of moisture in soil, the amount of water content is really important to whether it's rich or poor and so you might have some like soil water measurement or soil water sensor that you use and so that's what this would represent is what measurements are you actually taking, what are the outputs that you're actually observing. So, of course, in order to do that, we have to have some way to go from our state to an observed output. Again, throughout most of this lecture and in the notes, we're typically going to be assuming that this is the identity function, that we actually can observe the state directly, but that doesn't have to be the case: so again, I could have something like a soil moisture sensor or some other type of measurement that I take on the soil and then I would just observe that, I wouldn't observe directly whether it was rich or poor.

Okay so in this particular case, as I said, we're just going to assume the identity output function here, we're just going to assume that we exactly observe the state and so then this becomes an easy thing, we know that our set of possible outputs are exactly the state values, rich and poor, and in this particular case, when we know that our state is poor, then in fact we will observe that it is a poor state.

Okay and then we can do this again: we can say, “let's take another step”. Maybe last time we planted and we got this poor soil, so we might think, “hey, how do I get back to rich soil?” Maybe I'll let my field lie fallow this season and so I can ask what is going to be the output of my soil the next season after I've let it lie fallow? Well in this case, you can, again, read off from the state diagram up at the top, that we're going to end up with rich soil. And again, just because this is the identity function for the output, we're just going to get the exact same state output, we're going to get this rich soil observation.

Okay so this is a state machine. It is described by exactly these six quantities that we've just talked about: we have to say what are the possible states that we can move between, what are the ways we can move between states? So first of all, what are the inputs that we can put in to make a movement happen? What is the initial state that we start with, how do we move? That's the transition function, it tells us when I make some action on a state, what's the new state that I get when I take this input, what's the new state that I get? And then there's these aspects related to observation. So I have a set of possible observations, I can make these outputs and I have some output function that says “hey, if this state is hidden, if it's latent, what are you actually going to observe?” And that's this g. Okay so this is all well and good. Now we have this notion of a state machine, a way to move between states. But what we're going gonna do next is we're gonna ask ourselves, “okay well this is nice to be able to move between states, but we said we wanted to make decisions.” Right again, just from lecture one, we said machine learning is a set of methods for making decisions from data and so here we don't really have a way to make decisions: we could move to rich to poor, but the way that we've made decisions so far in this class is we've had some notion of what things are better to do, that's the loss, the loss tells us what's better because you make less loss, like you want to minimize your loss and so we have to have some notion of a loss or a gain of how well we're doing in order to make decisions. You probably have it implicitly in your mind for this example so far and we're about to make that explicit but it so far doesn't exist on the slide, we haven't said what makes a good decision, and so from that perspective we can't really make decisions yet so we're going to have to come up with a way to have a concept of loss or actually the inverse of loss which is reward, so that's what we're going to come up and do in a moment. Just before we do that, I'll also just note here we have a particularly simple state machine with just rich soil and poor soil. Of course, you could have so many more states, you could have maybe a bunch of different variants on every time you plant you get even poorer soil and there's a gradation from rich to poor, you could have more states from something else, but this is our simple example here. Okay but we want to be able to make decisions. So again the thing we're going to focus on right now is coming up with a notion of a loss or, again, a very equivalent conception, just the reverse of, reward. Okay so let's get rid of this example. Now, in general, for everything we do going forward in the lecture, you actually could have this output function and it could be non-trivial, like you don't have to ever observe the states, but that's an extra layer of complexity that we're not covering right now and so for everything we do going forward, we're going to assume that we observe the state directly, you can think of that as an identity output function. So I'm going to get rid of this part too, the part about the output, on the assumption that we observe the state. Okay so now we're ready to start building up this notion of a reward and only once we have a notion of, again, what makes a set of inputs better or worse can we make a decision about those inputs and so that's what we're going to be focusing on here. Okay so what is the reward in farming, well it's probably your harvest. That seems like a pretty natural notion of reward in farming and, in particular, you want your harvest to be bigger when you're farming. The assumption is you have a bigger harvest, you can sell it for more, you can make money or you can eat that harvest, it's generally a good thing, and so we're going to say that our reward function here is the number of bushels in a harvest. Bushel for the purposes that we have here is just some notion of how much, say, wheat that we have harvested from our farm. According to some Kansas wheat website that I looked up, which who knows how accurate it is, but it says for wheat one bushel equals 60 pounds of wheat or approximately 1 million wheat kernels. Anyway, it's a bunch of wheat, so we're going to say we want more bushels in our harvest, that's going to be our reward function. And so in particular, something that's going to be generally true about this reward function is that it's going to output a real number. So in this case, it happens to be how many bushels we harvest and we could even have fractional bushels and so this could be any real number really, any positive real number. But in general, we could have other types of rewards but we're always going to be thinking of them as real numbers, it could be dollars, it could be something else. Okay, so now we have to think of what's the input to this reward function. Well certainly some part of this input has got to be our action, right? I mean somehow if I plant, I should get some bushels out and if I fallow, I should not get any bushels out and so it makes sense that my input is going to be the input to my reward function and I'm going to get some reward based on that input. Now, in the past, when we talked about having a loss, we have some input like I decide to wear a coat or not and then I get my reward and that's again just the end of the story and so what's really different about how we're approaching things today is that it's not just my input that determines the reward but it's also the state that determines the reward. And if you think about that for a second in this application, it's pretty clear that's going to be true: if I plant in rich soil I expect to get a big harvest, a really good harvest. If I plant in poor soil, I expect to get a worse harvest, not as good of a harvest, I don't expect to get as many good bushels at least. Okay and so let's make that concrete: let's just specify, just as we have an example of a set of states, a set of inputs, and the transitions, let's make this specific and say what's a reward function that we might have. This is just an example of a reward function so our example reward function might say if I have a rich soil and I plant, then my harvest is going to be, say, 100 bushels on this field. If I have poor soil and I plant, I generally expect fewer bushels and so in this particular example, let's say we'll get 10 bushels out of the field. And then, of course, if I let the field lie fallow, I really shouldn't get any harvest and so in that case, I'm going to say that my harvest is zero bushels. Okay so this is an example reward function, it's the example reward function that we're going to be using going forward just as we're using this examples of states and inputs and transitions. Okay great, so now we have a reward. We're all set to go, this is what we talked about, a way to make decisions. But while we're here, let's just notice that there's something else that we'd like to change about this setup and that's this transition function. Namely, right now this transition function is totally deterministic: if I plant, I get poor soil. If I let my field lie fallow, I get rich soil. And the reality of farming is that this is not totally deterministic, it's stochastic, there's randomness in this. So for instance, if I have a really good year with lots of rain, the soil might be very, very full of moisture and I may get rich soil even though I planted that year. So there's some randomness, for instance due to weather and other forces as well, and so we'd like to include that in our transition, we'd like to account for that and so we're going to have to change something because right now this transition function is totally deterministic.

Okay so in order to see how we can change this, let's take our existing plot here, our existing diagram of how we move between rich and poor soil and let's focus on the plant action, the plant input. So right now, we have two inputs that we can have: we can have “fallow” or we can have “plant”, we're just going to focus on the plant and we're going to do this to make our so-called transition model and so this language, going from transition function to transition model, and this different word or this different notation T will indicate that we're talking about something stochastic and we'll nail this down in just a second. Okay so we're focusing in over here on just the plant action. Now another way that I could describe what's going on in this particular part of this diagram, this plant action, is I could say what's happening is that, if I plant, I'm going to pour soil with probability 1. So here, I'm going to say we're looking at this plant action and on each of these arrows, I'm going to put the probability of this transition. So I go to rich soil from rich soil to poor soil with probability 1 and implicitly then, I go from poor soil to rich soil with probability 0 when I plant. So these probabilities have to add up to 1, I have to add up to a 100%, that's what they do here. But in general, they don't have to be just 1 and 0, I could have different probabilities like, for instance, let's think about starting from rich soil. Then I have two options: either I go from rich soil to rich soil or go from rich soil to poor soil. Now here, I'm saying with probability 1 if I plant, I'll go to poor soil. With probability 0, I'll stay at rich soil, but in general, I could have any two numbers that add up to 1 and are non-negative. So it might be the case that if I plant with high probability, I go to poor soil, but maybe I stay in the rich soil if the weather was particularly good this season.

Likewise, if I start from poor soil, it might be that the vast majority of the time if I plant, my soil is going to stay poor, but every now and then, with some small probability, the weather is fantastic and I'm actually able to get rich soil. And so especially in something like agriculture where you really do have this randomness, this stochasticity due to the weather, you would like to account for something like this. Okay. Now of course, in order to fully specify this, we need to say what happens under each of our actions: our plant action and our fallow action. Before we do that, I just want to say that there's another way that we can represent the information in this diagram. In fact, by the end, we're going to have seen three different ways. So the first way we're representing this information is in the diagram itself, this is a diagram that fully specifies the plant action in this transition model. But a perfectly equivalent way to specify the plant action is with something called a transition matrix. So here, we're going to write out the transition matrix for the plant action. This is going to be a matrix that tells us how to go from, basically, a starting state to an ending state and so from that perspective, the size of this matrix is going to be number of states by number of states. To specify the matrix, you have to specify the order of the states that you're talking about. So in this case, I've specified that I'm putting the rich state before the poor state. I would get a totally different matrix if I put the poor state before the rich state and so it's very important to say which one you're going to use, so here I'm using this particular ordering. Now we're going to take the convention that the rows are the starting state and the columns are the ending state. So that is: on whatever growing season I'm at right now, that's going to be on the rows and whatever growing season I'm at next season, that's going to be the columns. Okay so my question for everybody in the chat is what is this entry in the matrix? So this entry in the matrix should represent what's the probability of starting with rich soil and transitioning to poor soil under the plant action because again this matrix is just for the plant action. Great, lots of great answers here.

Okay cool. Folks are recognizing that we can look at the diagram above, we can see what's the probability of going rich to poor under the “plant” action: it is 0.9. And similarly, you can get all the other information for this matrix from that diagram, you can notice that going from rich to rich, the probability of that is 0.1, probability of going from poor to rich is 0.01, the probability of going from poor to poor is 0.99. Again, all of this is specifically for the “plant” action. You would have another one of these for the “fallow” action which we'll see in a moment. Okay now a couple of things to note about this matrix. First note that every one of its rows adds up to one because no matter where you start, with 100%, you've got to go somewhere and so that's what we're saying here: that there's 100% probability that you'll end up somewhere and then the individual choices are the different states. There is no such constraint on the columns: we don't have to have them add up to 1, we certainly see that that's not the case here.

Okay so this is a transition matrix: it has exactly the information in the diagram above it, so if you knew this diagram, you could make this transition matrix, if you knew this transition matrix you could make the diagram. Now as I said, we also want to specify what happens under the “fallow” action because we haven't fully specified our transition model if we don't see what happens under each action and so the next thing we're going to do is we're going to say what happens under the “fallow” action. Well fallow has to have exactly the same set of states because they're just the set of states that we're using, but it might have different probabilities for going between those states and certainly we expect that, we expect that, in general, if I have poor soil and I let it lie fallow or if I have rich soil and I let it lie fallow—if I let soil lie fallow—then I'll end up with rich soil and if I let soil lie fallow I'm unlikely to end up with poor soil, although it could happen: there could be some drought, maybe, and then I might end up with poor soil and so we want to represent that in this random, this stochastic transition model.

Okay so I said that there were going to be three ways that we could represent the transition model so now we have one as the diagram, two is we would have a transition matrix for each one of our set of possible inputs and the third is going to be a function and so that function, at this point, we can kind of anticipate what that function is going to look like: it's going to take our starting state, it's going to take the input that we make, and it's going to say for any ending state, what's the probability of ending up there. So that's why we have a state, an input, and another state. Okay so let's do an example like, for instance, let's look at this 0.9 here. Well this 0.9 represents the probability that on the next round, on the next step at time t, the t-th growing time, we have poor soil given that on the previous time, time t -1, we had rich soil and we decided to plant.

Now let me just make a little note about notation here. This capital S_t represents what's called a random variable and so the idea is if we were just running through this model, we don't know what the state of that soil is going to be at time t because it's stochastic and so we can represent that with S_t and say “hey, that's stochastic, we don't know what it's going to be” but if we refer to any particular state, like poor soil is a particular state, we could refer to that with a little s, we could say “hey, this is a particular possible state, it's one of our set of possible states and we'll refer to it with a little s.” Okay so here is this, again, the probability of going from poor starting from rich if I decide to plant and so we're going to call this T, the function T of rich, plant, poor. So the first argument in T is my starting state, the second argument in T is my input, and the third argument in T is my ending state. And so that's, again, a third way to specify the transition model and it's formally what you will typically see in a description of a Markov decision process which it turns out we've been defining all along.

Okay so there's two things that we've done at this point: we noticed that we were missing, when we had this state machine that just told us how to go between states, we were missing a ability to talk about what's a good set of decisions, what's a good set of inputs, and we also noticed that we were missing this ability to talk about randomness, about stochasticity. And so once we include both of those two changes, we essentially, basically have a Markov decision process. I say basically because I'm going to make some cosmetic changes to this in a second, but these are the two really big changes. The big changes are we now have a way to talk about rewards, about what makes a set of inputs good, what makes it desirable to us, and we have a way to talk about stochasticity in the process, randomness in the process. Okay all that remains at this point, again, are going to be some cosmetic changes so the next thing we're going to do is we're going to make those cosmetic changes.

Okay so the first thing that a cosmetic change is we were calling these “inputs” when we were talking about a state machine. Now we're going to call these “actions”. I was kind of calling them both all along but they're basically the thing that we're doing that changes the state and so let me just figure out where in this slide was I using this X notation for inputs and I'm just going to change it to the “A” notation for actions. We haven't really changed anything here, this one's just a nomenclature thing. Okay the next cosmetic change is that, when we were talking about our state machine, we referred to an initial state. In this case, we're kind of going to think about all the possible initial states as we go forward and so we won't specifically say that an initial state is part of our Markov decision process, so I'm just going to get rid of this initial state. Again, it's cosmetic” whenever we actually run this, we're going to have to start from somewhere but we're not going to formally say that that's part of our Markov decision process. Okay and our final thing, this might not be quite as cosmetic, is we have what's known as a discount factor. So basically I'm just going to push that off to later in the lecture, we'll talk about it later in lecture, but it is part of our Markov decision process. Okay and so now we can get rid of this “(basically)” and say what is actually a Markov decision process? Okay so that's gone, we have our Markov decision process, it's got a set of possible states (in our example, they're rich soil and poor soil), we have a set of possible actions that can change our states (in our case, they are “plant” and “fallow”). In our example, we have a transition model: it tells us when we make an action, how likely we are to end up in different states and it depends on where we started too, it depends what state we started in. We have a reward function that tells us what do we get from taking certain actions and what we get, the benefit, the gain, we get from taking certain actions depends on the state that we're in as well. And then finally, we have a mysterious discount factor that remains to be described but basically this is a Markov decision process and, again, the things that really distinguish it from a state machine are, one, a reward function,  understanding what's actually good about our actions, and this idea of a transition that can be stochastic. Okay so we said that the reason that we set up this Markov decision process was in order to make decisions. So now let's start thinking about making decisions: how are we going to make decisions? What are those decisions going to look like? And what types of questions can we answer? What types of decisions can we make?

Okay.

So the types of decisions that we're going to tend to make here are called policies. So a policy is going to tell us: what's our plan? I've rented this field for a few years and I want to know each year how am I going to treat it: am I going to plant it? Am I going to let it lie fallow? What am I going to do? And the policy is going to tell me: it's going to say, “hey, look at the state that you're in right now (again on this assumption that I can observe my state)” and then I'm going to ask what action will I take, this is my plan of action essentially based on the states I have. And so now, these are the types of decisions we're going to make and we're going to ask various things about these. So one question I might ask is: what's the value of a policy? For instance, how many bushels am I going to get from my policy? How much profit, perhaps revenue, can I expect to make? And this could be really important because maybe I'm deciding whether or not to even rent the field, I'm not sure that I want to rent this field and do some farming and so this would help you make a cost benefit analysis and understand is this worth it to me. Also, if I have two different ideas of what I should do with the field, and we're going to see this in a moment if I have two different plans that I'm considering, if I knew the value of each of the plans, I could decide between them, I could say “oh, this one's a better plan, this one's a better policy.” Now of course, in general, once I'm comparing policies, why don't I just choose the best policy? I should just go straight to the thing that's going to get me the biggest harvest, the most reward and so we're going to talk about that as our second question: how do I choose the best policy out of the policies?

Okay so what we're going to do next is talk about these questions. Oh but there is a question from the audience first. Yeah the question is: what if we have a situation where the reward isn't always the same for a given input and state? Could you also have a stochastic reward? Oh yeah, absolutely. I mean I think this is a really interesting question once you understand these things, these are just a bunch of choices that are modeling reality and essentially what we're trying to do is get close enough to reality to have a useful model that we can use to make good choices, but also, typically, what we do is we somehow come up short of reality, just because a simpler model is often easier. So I might choose a non-stochastic reward function just because it's easier to work with, but the reality is absolutely and, in reality, a reward function would very easily be stochastic as well. You can imagine if I have the same moisture of soil or whatever, that I could get a different harvest in different years and so I would absolutely want to include that something. While I'm on the subject, I'll just note something else that is a limitation of this is what's known as Markovity. So we have this Markovian-ness, we have this Markov decision process, we're assuming that basically everything we're doing only depends on the previous state, but you can imagine in a world in which it depends on more states, going back in fact probably that's more realistic in a lot of cases. But, sometimes the simplifying assumption is worth making anyway because it's simpler, we can solve the equations for it, we can do the calculations that we're going to do later in this, but absolutely, I mean there's some sense in which everything that we're doing can be improved and sometimes those improvements are totally worth doing and you can come up with an even better set of decisions based on them and I think this reward function stochasticity is a great example of that. Cool.

Okay but for the moment, we're going to start with the simpler version. I think a good life lesson is always to start with the simpler version and then compare it to the more complex version, so let's start with our reward function as described and see what happens.

Okay.

So first, we're going to start by asking what's the value of a policy. That was our first question from the previous slide. Now when we discuss this, we're going to have almost exactly the same transition model and Markov decision process that I have on the previous slide except, just for the purposes of making the calculations easier, I've simplified everything to be probabilities of 0.9 and 0.1, sometimes they were like 0.99 and 0.01 on the previous slide, this is just for ease of use, you could absolutely have all of the interesting probabilities on the previous slide, but here I just simplified it a little bit and we'll assume that our reward function is the same from the previous slide, so this is basically the same as the previous slide, the numbers are just tweaked a little bit.

Okay let's ask: what's the value of a policy? Well it depends how much time I'm going to enact the policy. If I'm renting my field for one year, I expect to get a lot less harvest than if I'm renting my field and harvesting crops for 10 years and so we need to specify how long we're going to be running this policy and we're going to call that “h”, h is the horizon. It's just basically how many time steps we have left, in this case, how many growing seasons do we have left. So in particular, I imagine that I'm renting, for the purposes of this example, a field for each growing season and then it's going to be totally destroyed and they're going to make a strip mall and so there's just no consequences and that last season whatever I do is a total free-for-all and the field will just not exist at that point. So this is the example that we're going to be imagining here, this is the back story of my field. Okay so I have this field, I have it for each growing seasons and I want to ask myself: well, what's the value of that again? Maybe I'm even thinking about, maybe, I should rent this field, I don't really know, I want to figure out its value.

Okay so we're going to say: let's define something that tells me the expected reward if I enact a certain policy starting at a certain state. So clearly, V, this value, has to depend on the state I start, it has to depend on my decisions, that's encapsulated in the policy, and it has depend on the horizon as we just said, how long am I running this. Okay so we're going to imagine, also, that there could be some dueling farmers: farmer A has an idea of what's the best thing to do. Farmer A says “let's, always plant,” that is farmer A's policy, is to always plan and that seems like a potentially reasonable policy. I mean it's a greedy policy, it says every time let's get as much reward as we possibly can on this route. Farmer B says “hey, let's be careful here, I think that we should plant if the soil is rich and let it lie fallow if it's not and let it regenerate.” And so we have these two farmers and basically, in some sense, we want to resolve this conflict between them; which of these is a good policy? Which of these policies should I use? And so in order to do that, we're going to calculate the expected reward of each of their policies and then compare them.

Okay so let's start with a horizon of zero. Now here I'm putting a horizon of zero, I'm putting in any state, and I'm putting in any policy: it could be policy A for farmer A or it could be policy B, it doesn't matter because if you have a horizon of zero, you can't do anything, you can't grow, you can't plant, you can't fallow, you can't do anything and so it's boring, your reward is zero, there's just nothing you can do.

Okay so things start to get interesting when you have one growing season, when you have a horizon of one. So if you have one growing season, what's your expected reward of a policy starting at s? Well you just enact the policy for that season and that's all you can do and so your expected reward is the actual reward. So here, for one growing season, we can write out what's the reward starting at us of enacting our policy, that would be pi_s. Okay so let's start by talking about farmer A’s policy. So here we're talking about farmer A, so that's why we have V_pi_a, we're talking about horizon of one, that's why we have the superscript of one and we're going to talk about both possible states, so let's start by assuming we start from rich soil and then we're going to talk about poor soil.

Okay so you can ask ourselves what's this going to be? Well we can just plug in the formula: it's R, applied at rich, that's the starting state, and then we take farmer A's policy if we start from rich soil. And so my question for you is what is this value? It is a number. In this particular example, what is that number? This is a question for the chat.

Looking good. Good stuff.

Okay, let's talk about how we get this, this is great, many of you have said 100. Now how do we get to that? Well, we first notice that we have to evaluate pi_A(rich). What is pi_A(rich)? Well pi_A says I always plant. Farmer A tells me to always plant, so farmer A's policy is to plant. So if I evaluate pi_A, I know that I'm evaluating R of rich plant. Now, I'll go over to my reward function, it's in that little gray box, and the R, the reward for being in rich soil and planting, is 100. So in this case, the value of having the policy of farmer A and starting in rich soil is 100 for a horizon of 1. Okay, so I'm just gonna put that over here: we've got a 100. Now let me ask you the same question: what is this value? So if I take farmer A's advice, I use farmer A's policy, I have a horizon of 1 and I start from poor soil, what is the value of that policy starting at poor soil?

Okay great, we're getting a lot of tens so let's just walk through this again one more time. So we're going to say, “okay, what's the value of starting at poor soil?” Well that's the reward function where the first input is poor soil and the second input is farmer A's action for poor soil. Well farmer A's action for poor soil is plant because farmer A’s action is always plant. So now we're going to ask what's the reward for poor soil and planting? I go again up to my little reward box in the corner and it's 10, so I'm going to put in 10. Okay you can do the same thing for farmer B. In fact, I'm going to show you many calculations on this slide and the following slides. I kind of came up with some numbers and tried things out so you should definitely double check all of these numbers. If you find any typos, I would love to hear that, I will update the slides later, but this is the reality of life. Unfortunately there is no checker for my calculations like we have in homework and exercises and everything. You are the checker, you are my colleagues who will check, so hopefully you will do that and just report back that you agree with these numbers. But, basically the idea is we think that farmer B is going to plant if there's rich soil, so we think that there should be a reward of 100 in that case, and they'll let it lie fallow if there's poor soil, so we think there should be a reward of 0 in that case. Okay so that's horizon of 0, horizon of 1. Things get tricky when we get to a horizon of 2 or above and there we're going to start defining a recursive formula. So instead of this formula, we can replace it with more general formula that also includes this formula which is the following, so let's step this through. So this is saying, now, let's look at a more general horizon h. So, for instance, imagine I have five years left on my lease with my farm and what's going to happen is, starting from the state I'm in, I'm going to use my policy to make some decision this time right now: I'm either going to plant or I'm going to let the field lie fallow. Everything else in this is saying what happens when I have four years left on my lease? So I started from five years left on my lease, now I have four years left on my lease, so I have to consider, based on the decision that I made this year, what's going to be the state of my farm next year? Is it going to be poor soil or rich soil? I have to sum over all those possibilities. Now each of those possibilities has a probability, so that's the t here, and each of those possibilities itself has a reward recursively. So if I had already done all the calculations for four years, when I have a farm for four years, now I can use this formula to calculate what's the reward if I have a farm with five years. So we already did the calculations for what happens if I have a farm for zero years. You can apply this formula, in fact we implicitly did, to figure out then what will happen if I have what are the rewards for having a farm for one year and now we're going to apply this formula again, it'll be a little bit more interesting, to understand the value of having a farm for two years.

Okay so let's do that: so we're going to say, “hey, suppose I'm renting my farm for two years and then it will get paved over by a strip mall.” So I'm renting my farm for two years now and the soil was rich when I got it. If I follow farmer A's advice, what's gonna happen? So that's what we're about to work through together. Okay, well the first term, so I'm just applying the general h formula from above and I'm bringing it down here with h = 2, the first term is what happens this year. Well the soil started out rich and I'm going to use farmer A's advice to always plant in rich soil and then I'm going to get some rewards from that.

And then I'm going to have some terms that represent, basically, what happens when I get down to one year. So here, there are two possibilities when I get down to one year: after this year, I'll either have rich soil or I'll have poor soil. Those are the only two possibilities: I either have rich or poor soil. Basically, I'm going over all the states. If I have rich soil, I'm going to say, “well, what's the probability of having rich soil?” That's T times what's the value of having rich starting from here for that remaining year, which we've already calculated. Here, I'll say, “what's the probability of having poor soil?” That's T, and then I'll say “what's the value of my farm starting from poor soil?” which we've already calculated. This is the beauty of recursion: that you do it for the 0 case, then you can do it for the 1 case, then you can do it for the 2 case, then you can do it for the 3 case, and so on. Okay so let's now start solving this, so we're going to take this formula, we've applied the general formula (so that h =2 two case) and now we're going to put in some numbers.

Okay so the first number we want to put in is this reward for this year's farming. So we started from some rich soil, we followed farmer's A advice to plant, and so our reward should be 100 based on the logic that we went through before. What's the probability of going from rich soil to rich soil if we plant it? It's 0.1.

What is the value, with a horizon one, of starting from rich soil? This isn't something you would know off the top of your head, it's something we calculated already, so we're just going to input that number over here.

Now we do the same thing but for poor soil. We say what's the probability of going from rich soil and planting and ending up in poor soil? That's probability 0.9, just according to our transition model.

What's the value, with one season left, starting from poor soil, where again you don't expect to know that off the top of your head, you already calculated it. That was 10.

And so now we put this all together and we say that this value is 119. Now this is larger than any value we saw for a horizon of 1 but that's not necessarily too surprising because you expect roughly, in general, that as you get more seasons to grow, you get more harvest and that's what the value is just saying: how much do we expect to harvest, how much reward do we expect to get over all the seasons of our horizon?

Now of course, depending on the starting soil, it doesn't have to be the case that we get more but, in general, we expect it to increase.

Okay so starting from rich soil, taking policy A and having a horizon of 2, this is the value, the expected value, that we get from our farm.

So I'm going to just put that there and you can do the similar thing—and this is, again, the part where I say check my math—to calculate the value of having a farm for two years starting from poor soil and following farmer A's advice, that's (V^2)_pi_A, (V^2)_pi_A with the poor input. Then these next two, the final two values that we're seeing here, are following farmer B's advice starting from rich soil and starting from poor soil.

And you can do this again, you could just keep recursively applying this: once we know the values for two years, we can find the values for three years, I'm just telling you these numbers. Again, a great idea would be to check them, it's also a good exercise, and so we can just keep getting these values.

And remember we started off with some dueling farmers. So yes, we have these values, we could ask: is it worth renting the farm? But here, we really need to address this burning question of which farmer was right, which farmer's advice should we follow? And something I think we're going to see in a moment is that it really depends on the horizon.

So let's start by asking, we're going to start by asking, horizon one. And what do I mean by who wins? I mean that farmer's output, that expected reward, the value for the farmer, has to be at least as good at all the states and strictly better for at least one state. That will be our notion of winning for the farmers. And so first let's consider a horizon of one.

So if I only rent for one season and then my farm gets paved over by a strip mall, is one farmer's advice better than the other according to our definition of winning at the bottom of this slide? Which farmer's advice is better with a horizon of 1?

Great, I'm seeing a lot of people saying that farmer A's advice is better, farmer A's policy is better. So it's very important that we be focusing just on a horizon of one. We'll talk about other horizons in a second, but if you look at a horizon of one, we can look at the rich state. If we start from a rich state, farmer A has a value of 100 and farmer B has a value of 100, so farmer A is at least as good. If we start from the poor state, farmer A has a value of 10 and farmer B has a value of 0, so farmer A is strictly better and so a way that we can write this is to say that policy A is better than (we'll use the greater than sign, we'll use a subscript h = 1 to indicate that this is for a horizon of 1 which is really important), better than farmer B, better than policy B.

Okay what about a horizon of three? Does anything change for horizon of three? Whose policy is better at a horizon of three? You can answer this in the text. Awesome. People are noticing that it's different here: farmer B's policy is better. So again, we say, “hey, what happens in the rich state at a horizon of three?” The value of farmer B's policy is 192, that's strictly better than farmer A's value, which is 138. If we look at the poor state, farmer B's value is 118 which is strictly better than starting from a poor state with farmer A is 48. Looks like, in this case, farmer B is beating out farmer A. Okay last question: what about a horizon of two? Who wins at a horizon of two? Another question for the… 

I like the answer “eek.” There's a lot of people who are saying “not sure.” There is no clear winner by our definition of winning here. Now you could come up with a different definition of winning, but by our definition of winning here, there is not one policy that is strictly better than the other policy. If we look at the rich case, the rich state, at a horizon of two, then we see that farmer A's policy has a better expected value, but if we look at the poor state, farmer B's policy has a better expected value, so no policy wins for h = 2. Okay and let's just think about this for a moment: what's going on here? Well if I have only one season before everything gets paved over by a parking lot, then I should just be as greedy as possible, I should just eke out whatever I can from the land and run away because nobody's ever gonna use this land again and that's farmer A winning with a horizon of one. If I have multiple seasons, like three seasons or even more seasons, I expect farmer B to win because farmer B is delaying gratification for a better reward, so really seeing this value of delayed gratification but only if I have enough time to take advantage of it, only if I have enough seasons that lying fallow actually does something. If I let something lie fallow and it gets paved over by a parking lot, I don't get a reward from that, I only get that if I have enough time to take advantage and so that's what we're seeing here: we're seeing that it really depends on the horizon which one you want to do.

Cool. Again if you have a different definition of winning, which I see some people are proposing in the chat, you might get a different notion of who wins, but here this is our definition of winning and so for just for this definition, this is the conclusion that we might draw, but absolutely you might consider something else.

Okay. Now something I want to note here is that, in everything that we've done, we've assumed the policy is exactly what we define. We said a policy is: you take a state and you put out an action. But something that you might be starting to think, based on the discussion we just had, is maybe the policy should also depend on the horizon. If I only have one year left, I should just go free for all and farm anything that I can, but I have if I have many years left, then maybe I should do this fallow plant cycle. And so you can actually adapt this formula to allow dependence on the horizon, you can adapt the policy to allow dependence on the horizon. So let's just see what would change here. Here are all the places that the policy appears and now if I allow dependence on the horizon, we might call this non-stationary, so it's stationary if it's the same for every horizon but non-stationary if it could depend on the horizon. Then I'll introduce some subscripts h perhaps. Now let me make a little point here. What's happening here is, when I apply this formula, I now use the policy at time h to decide what action I'm going to take when my horizon is h, so that's why I have this pi_h here. I did not add the h subscript here because here, I'm just referring to the full policy across all the h's, I'm just using it as a subscript. So I only applied the h subscript when I'm applying it for time h and then here I just say “oh, I have my whole policy across all of the h's and it still can change with the h’s” but here I'm just referring to it with my V notation.

Okay so now we have the ability to evaluate a policy, to say what's the value of the policy for some horizon. We can see that it really can vary by the horizon, that the reward can really change and it might even change which is a better policy. Now, let's think about, well, what if I don't want to just talk about two dueling farmers, what if I just want to say what's the best policy, what's the best thing that I should be doing? In that case, I might think, well, I could consider every possible policy with every possible variation over horizon. That's a lot of policies to evaluate and maybe there's a way to more quickly get at the best possible policy. Now this can still be really useful, evaluating the value of a policy. We still want to say, for instance, let's say that we know we have some plan and maybe there's some other decisions out there, it doesn't have to be the best one, we just want to know what's the value and that will help us decide whether to rent the farm or not. Separately, of course we're interested in the best policy because if we can change that plan it would be nice to. So that's what we're going to look at next: what is the best possible policy? Okay so in our discussion of what's the best policy, we're going to again just have the same running Markov decision process that we've had this whole time so we're keeping that. We're still going to have h representing our horizon, how many planning seasons are left, and as we saw we kind of expect that to affect our policy or at least what the choice we're making at each horizon. Now, we're going to introduce a new notation Q, so Q is the expected reward of starting at state s, making action a, so that's what we do on this round, and then for the rest of the future, we always do the best possible thing.

If I had this, you want to convince yourself that I could find an optimal policy. If I knew this, if I knew what would happen for each of my actions (in this case, there's two possible actions, planting and fallowing, but in general for all my actions) I could say, well, I already know what's the best for all the h - 1 steps that are left by recursion. Now I'm gonna find the best step right now by just saying, well, which of the actions I'm making right now is the best? So that's what the argmax here means, it says “okay which possible action is the argument to Q that maximizes Q on this round?”

So if we already know what the best possible policy is in all the future rounds, all the lower horizons, then this is how we can find the best policy on this round. We're going to go through an example of this but I just want to point out the difference with the V that we were just looking at. So some things are kind of similar here: we have this dependence on h, we have this dependence on state, but with V, we were evaluating a particular policy so the policy pi appears. In Q, we're not evaluating a particular policy, we're evaluating actions, we're saying, “hey, here are all the possible actions on this round that I could use” and I'm already assuming I'm using the best possible actions in future rounds and so now I'm asking what happens with these actions. And so V we use for evaluating a particular policy, Q we're using for coming up with the best policy but it's also because of that, it doesn't explicitly have a policy as an argument, we're just trying to find the best action on this round.

Okay I think it'll be helpful to walk through an example so let's do that.

A couple of things before I do that, just a couple of notes: so one, there doesn't have to be a single optimal policy. It's possible that multiple policies could give you the same reward. So we're just saying we're gonna find an optimal policy. One. The optimal policy here doesn't have to be stationary, it could depend on horizon h and we kind of expect that in this example as we already said. I think our intuition tells us in this planting example that, probably, we're going to do something like farmer B's idea of fallowing as we have a higher horizon and we're going to eventually switch to farmer A's idea of just planting.

Okay so now, let's actually calculate Q for the example that we have. So in particular, if we start with any state and we take any action and we have absolutely no time to plant anything, then our reward is zero because we didn't have time to plant anything so we couldn't possibly harvest.

Okay so now we want to find the expected reward of starting at sm making action a and then making the best action for all the steps that are left. So if we're at a horizon of one, there are no steps that are left after this step, there is only this step and so Q is just going to be the reward of being at state s and taking action a.

And so that's easy to fill in. So what, for instance, and this is a question for the chat: is Q^1, a horizon of one, with a state of rich and an action of plant? Okay, so awesome. Everybody's observing I can just say “well, what is the reward of rich and plant?” It is 100. I can just read it off for my reward chart and so we can do the same thing with every other state in action, this is basically just recreating the reward chart. We're saying, “hey, what's the reward of rich and fallow? What's the reward of poor and plant? Etc. And so this should align perfectly with our reward chart.

Okay things get interesting when we get to, again, an h greater than one.

Now before we do that, let's just recall the whole point, in some sense, of Q is that we're trying to come up with the best policy. So let's come up with the best policy for a horizon of one. What's the best policy for a horizon of one, can you write that in the chat? What would you, how would you describe the best policy for a horizon of one based on this Q?

Great, always plant. So no matter what your state is, you just plant. This is what farmer A was suggesting last time as some people have said. At this point, there's no time left, you get the most from just planting. You gain nothing by fallowing because there's going to be a parking lot here and so it's not going to help you to have better soil, you just need to plant. Okay now let's go to the interesting case which is again h > 1. So again, I'm going to replace this equation with a slightly more general equation of which this is the special case of h = 1. So here we have the more general equation. So the more general equation says suppose I have five years left, so h is five, I have five years left of my farm. I want to say: what's the expected reward of starting at s making action a and then making the best action for all the remaining steps? Well on this step, I already said I'm starting at s and making action a and so this is my reward for this step. Now I consider what happens in the remaining steps when I have four years left. Well, there are all the possible states I could have: I could either have rich soil or poor soil. So this is a different s’. I consider the probability of each of those states, that's T, and I say: what's the best thing to do in the future? And so this is quite different than what we saw before: I'm maximizing over all the actions from the recursion. So if I do this on round zero, well that's boring. If I do this on round one, I find the best set of actions that I could do for one horizon left. Then I apply this to h = 2 to find the best set of actions I do with a two horizon and then I find the best set of actions for three horizon etc. So let's work this out for a two horizon. So suppose I have two years left on my farm, two years left on my lease, two years until the parking lot comes. I'm starting from a state of rich soil and I decide to plant.

Well this season, I already said I'm starting from a state of rich soil and I decided to plant, so I'm just going to reap the reward from that.

Next season, there's some probability that I get to rich soil depending on my action, there's some probability that I get to poor soil depending on my action. If I get to rich soil, then I want to ask what's the best action when I only have one round left, when I only have a horizon of one, so that'll be the max over the Q^1s.

Because I'm trying to maximize overall here. And if I have poor soil, I can similarly ask what's the best thing to do from there? What's the best action?

Okay so what is this going to look like in our particular case? Well our... Oh there's a question. Yes could you just clarify again what does it mean when an optimal policy can be non-stationary? Oh yes, so what I mean by that is that the policy can change with horizon. So this is exactly the idea that if I have only one season left, I'm probably just gonna plant and I'm not gonna worry about what's my state, whether it's rich or poor, but if I have 10 seasons left, I probably want to take poor soil and let it lie fallow so that I can get the most from it next time and so those are different policies and if I restricted myself to have a policy that was the same at every horizon, I would get a different policy than I would if I could allow it to change over different horizons. In fact, we're going to see in a moment that our optimal policy here will change with horizon. So we said our optimal policy at h = 1 was to always plant, you may not be surprised to find out that at h = 2 it may not be to always plant. Cool.

Okay so let's look here at this equation and let's just start filling it out. So reward for starting at rich and planting, we already said, was a hundred. We just got that from our chart. Now let's say what's the probability of starting rich at rich soil, planting, and then ending up in rich soil. We can read that off of our transition diagram: that's 0.1.

Now we want to say: what is the best possible thing, the best possible action that we could have done with a horizon of one starting from rich soil? So I'm going to ask you: what is the max value that we're going to get here? What is this thing in yellow? This is a question for the chat.

Great, so what we want to do to figure this out is we want to look at the two different values of Q^1(rich, some action). There's either Q^1(rich, plant) which is 100 or Q^1(rich, fallow) which is 0. Q^1(rich,plant) is bigger and so that's what we put in: we put in 100. Okay, so now we go down here. The probability of saying we're in a rich state, we plant, we go to a poor state, that's 0.9, we read that off of our transition diagram. And here, we compare Q^1(poor, plant) which is 10 to Q^1(poor, fallow) which is 0. 10 is bigger than 0 and so we put in 10. And so the output of all this is 119.

Okay and so we can keep doing this. Again, you should check my math but we do this for all of the other Q^2s, basically for each state and each action. And now, once you see these numbers, even if you didn't derive them you can just look at them and say what's the best policy at a horizon of two? That is, what action should we take at a horizon of two? And so this is again a question for the chat: what action should we take at a horizon of two?

I see some people saying either plant or fallow. At a horizon of two, might the policy depend on the state?

Remember, in general, policy isn't going to be a function of the state.

So in this case, it might depend on the state. If it does, what is the dependence? What should I do if I find rich soil? What should I do if I find poor soil?

Okay awesome.

So the observation is, first of all, that the optimal policy is not the same as at a horizon of one and, in fact, it really will depend on the soil status this time and we can see that by looking at Q^2: if I look at the rich state and I plant, my total reward by doing the best thing all the time is 119. If I look at the rich state and I fallow, my total reward overall is 91. And so it seems like I should plant. If I look at the poor state and I plant, my total reward the whole time is 29, whereas if I look at the poor state and I fallow, my total reward over all the steps is 91. And so this is basically telling you that we want to delay gratification in the second state, we want to let it lie fallow this year so that we can reap all of that awesome harvest next year. Whereas if it's already rich this year, well we don't want to just throw away a perfectly good rich harvest and so we're going to start planting right now.

Okay and so, in fact, we do see now that the optimal policy here or at least an optimal policy depends on the horizon, that in fact it does change with the horizon.

This algorithm that we basically just went through, where you first calculate Q^0 and then Q^1 and then Q^2 across all the states and actions and then you can finally get an optimal policy out of, that is finite horizon value iteration: we have a finite horizon, the number of times, the number of years that I have rented this farm or I'm going to rent this farm is finite, that's h, and I'm iterating, I'm doing this iterative procedure to calculate these best values so we can call this finite horizon value iteration to contrast with something that we may not get to in this lecture but is in the notes, infinite horizon value iteration which is doing this in the infinite case. Now that being said, we will start talking about infinite horizon next so at least get a little bit of background on there. Basically the issue that arises here is what if I don't stop farming? So I just got some good news in the middle of this lecture while we were doing all of those Q calculations that, in fact, there is no plan to replace my farm with the strip mall anymore, they're actually just going to give me the farm and I get to keep it forever. Awesome. So now, I have a different kind of set of calculations: I don't just go up to a certain age, a certain horizon and then assume that the farm doesn't exist anymore, I keep my farm, it keeps going and so we have to ask ourselves: can we use the same tools that we just used in this case? Well they may have to change a little bit. Okay so what hasn't changed? What hasn't changed is the way that farms operate. Just because I'm farming for a longer time doesn't mean that rich soil magically all becomes poor or something like that, like it's still the case that from season to season I have the same dynamics and I have the same amount that I harvest and so all of that is the same, the Markov decision process has not changed or at least the actions of it have not changed. An issue that really arises when we look at really long time scales is that the value of money and goods today is not the value of the same amount of money and goods in the future and so this is the one thing that we're gonna change if I don't stop farming and if I have basically no finite horizon. So for instance, if I have a thousand bushels of wheat today, that's different from me having a thousand bushels of wheat in ten years. If I were hungry and I needed to eat today, I couldn't eat bushels of wheat in 10 years, I could only eat bushels of wheat that I have today. Or I could sell the bushels of wheat to get pizza and then I could eat the pizza today, I could not eat that pizza if I had the bushels of wheat in 10 years. Even if I was doing perfectly fine and able to eat, I could sell the bushels of wheat today, invest money and have lots of money in 10 years whereas if I have the bushels of wheat in 10 years, I don't have that money.

And so for this reason, we can't just really add up bushels of wheat across long time spans which is kind of what we were doing before: we were saying “hey, let's add up all these rewards across all the different years and treat them like they're all the same.” And as we see from this example, that's just not true. If somebody offers you either a thousand bushels a week today or a thousand bushels in 10 years, you should always accept a thousand bushels a week today, and so somehow we want to adjust for this in our calculations and a very typical way to do this is what's known as a discount factor. The idea of the discount factor is that it’s some value between 0 and 1 that tells you how the value of a bushel of wheat changes or the value of anything changes over time periods. So for instance, we could say what is the value of one bushel of wheat after ten time steps? By construction here, this is the idea of the discount factor, we will say that a value of a bushel of wheat now is one, one bushel. The value of a bushel of wheat next time step is gamma, the value of bushel of wheat in two time steps is gamma squared, the value of a bushel of wheat in three time steps is gamma to the third, etc.

And so, something that's kind of cool that you can do once you have this conception, is you could ask: what would I trade somebody who offers me a bushel of wheat every year forever? Sometimes you hear about those contests where people are like I'll give you a year's supply of Diet Coke, every year, forever and so if we were in such a situation and somebody was saying “I'm gonna give you a bushel of wheat every year forever,” there's some number of bushels of wheat this year that I would trade for that. So let's figure out what that is. What is the value of this idea of getting a bushel of wheat every year in today's bushels of wheat?

Is that a new question or that was the old one? Okay hold on. Okay so let's call this value of V and, by construction, it is the value of a bushel of wheat this year, which is 1, plus the value of a bushel of wheat next year, which is gamma, plus the value of a bushel of wheat in two years, which is gamma squared, dot dot. You just keep adding things up like this. Okay, well here's a clever way to write that: you could write the exact same thing as one plus gamma times quantity one plus gamma plus gamma squared plus dot dot. The amazing thing about infinity is that infinity looks the same today as it does next year and that's kind of what we're taking advantage of here.

Okay well that thing in the parentheses is just V, that was our definition of V from right here. And now we have an equation in V. It's a linear equation in V and so we can solve it and says V is 1 over one minus gamma. And as we said, gamma's between zero and one and so this will be a perfectly well defined equation, we can figure out what is this value. So let's look at an example:  suppose I take gamma to be 0.99, so I'm saying that a bushel of wheat next year is almost the same value as a bushel of wheat this year, it's only 0.99 the value of a bushel of wheat this year. That's pretty close to 1.

Well in that case, the value of bushels of wheat forever, one bushel of wheat per year forever, by our formula, is 1 over 1 minus 0.99, so it's 1 over 0.01, so it's 100 bushels. So if my discount factor here is 0.99, it doesn't even change that much. I should be willing to trade somebody who's offering me a bushel of wheat forever 100 bushels right now or vice versa. Now something that you should definitely ask yourself and think about, I'm not asking this in the chat, I'm just asking you to meditate on it later is: if I change this gamma, if I make it higher or lower. how does that change the value of a bushel of wheat forever? So definitely think through that later, give that a little thought. If you have any trouble with it, ask on Discourse, that's something that you want to make sure you have in your mind.

Okay so now, now that we've talked about the discounted value of money in the future and the discounted value of bushels of wheat in the future, we can talk about the expected reward of a policy pi starting at s when we keep our farm forever. So now there's no h on this V, there's no horizon, we're imagining an infinite horizon, that we're just always going to keep doing this.

Okay. Well we can write a recursive formula like before. This also looks very similar to the formula that I just wrote for this other V.

Basically what's happening is we're calculating the value of having this policy starting at state s and going forward. Well, what is the reward I get on this time step? Then, I look at all the future time steps, but remember infinity next year looks the same as infinity this year, so it's still just V_pi(s’). We actually haven't changed a horizon because there is no horizon and we just discount everything next year by gamma. So you definitely want to compare this in your mind to what we just did a few lines above, because it's a very similar idea. And this is also very similar to what we were doing back when we had a finite horizon, now we just have this discounting term and also the horizon doesn't change.

Now, we're not going to go through an example right now although I'm sure you'll be working through this at some point, but this is a set of number of states linear equation and number of states unknowns and so you can solve that, that's the kind of thing that we have linear algebra to solve so this is something that you can totally do. Now we haven't had time to get to it but basically there's going to be a very analogous idea for the Qs. So here we've been able to say what's the value of farming forever, just like we were going to say what's the value of getting one bushel of wheat forever. Now we can say what's the value of farming forever? And now, of course, you want to ask, well, what's the best thing I should do to farm forever? What's the optimal policy to farm forever? And so that's the remaining piece but, again, it's going to be very analogous to what we did in the finite horizon case just with this extra discount factor stuff in. So now we have the magical final bit of the Markov process, the discount factor that's telling us how does this change over time and I'll just mention you don't have to only use this in the infinite horizon case, you could have a discount factor in finite horizon too because, really, this is just the value changing over time and so it will change in 20 years. We just talked about how in ten years the value of a thousand bushels of wheats will change to you and so that's something you can do as well. Okay great, so we're gonna be using these ideas going forward, these ideas of changing state and so on and some really cool things in the next few weeks and I will see you then.
