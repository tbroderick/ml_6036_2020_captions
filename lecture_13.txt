Okay it's about that time, let's go ahead and get started. So far in this class, we've spent a lot of time talking about supervised learning. We've seen a couple of examples of this: things like classification, regression. You’ve seen them a lot of times and so today, we're actually gonna explore this other part of machine learning that we've mentioned before but not really dug deeply into, unsupervised learning, and we're going to focus, in particular, on probably the most popular form of unsupervised learning, clustering, and we're probably going to focus on the most popular form of clustering which is k-means clustering. Okay, so let's dive in. As usual, we'll start with a motivating example. So it happens that today is Giving Tuesday. I think that's an international phenomenon now but I'm going to talk about just a couple of charities in America: Feeding America and Meals on Wheels America. I think both are charities that distribute food to people who have food insecurity and we're going to imagine, for the moment, that a new charity has approached us. This new charity is called Yes Free Lunch and they are interested in also distributing food to people with food insecurity and, in particular, their model is that they have these food trucks that they're going to place in various places around America and they want to place them in such a way that people can walk to the truck, pick up some food, and head back home and so they've approached us, machine learning experts, to help them place their trucks in some optimal way for this food distribution. So we're gonna think about that problem today. Okay so they have, let's say a very fantastic existing model and existing trucks, but now they have a new area of the US and they're interested in placing their k food trucks and asking where they should park. And so what we're looking at here is perhaps the area that they're thinking of placing these k food trucks and each dot here represents an individual that they think might use their service, that might pick up some food from their place and so from one of their trucks. And so, in particular, the horizontal axis here, we can think of as longitude, the vertical axis we can think of as latitude. So this is really like we're just looking at some dots of where people live on a map and we want to ask: how can we minimize the loss of the people that this charity is serving? And so we have to think about, well, what does it mean to have a loss here? On what are they losing? It might be some aspect of, like, “how much of their time do they have to give up” or “what do they get by walking to these trucks.” Okay so let's establish a little notation to just be able to write this kind of thing down. So first of all, let's say that there are n different people that we’re interested in serving here and we'll say that the i-th person is at location x^(i), so this is where the i-th person lives. So for instance, this little dot is where a particular person lives and this will be described by a feature vector. That feature vector is the longitude and then the latitude of that person's location. Similarly, we can talk about our j food trucks. So our food trucks will be taking values from 1 to k or be indexed by values from 1 to k and they'll be at some location too. So here let's suppose I have five food trucks, so k = 5, and I am interested in locating them in an optimal way so I want to write down their location. So let's say that for the j-th food truck, its location, again, is going to be given by a feature vector, a longitude and a latitude. Okay, so now I want to say, let's assume that each person walks to a single food truck and maybe we communicate with them beforehand on the phone and we say “hey, here's your food truck. You can expect your meal is going to be at this particular food truck.” and that food truck we need to identify to them and so let's call the food truck for the i-th person y^(i). And so for instance, if this person, if we call them up and we say “hey, you should go to this food truck,” then we're going to want to denote that with y^(i).

Okay so finally, we have the notation to be able to say something like “what's the loss of the i-th person walking to the jth truck?” Now it could be anything. I mean really, you could choose whatever loss is most appropriate to you. I'm going to make a choice because we have to make a choice to proceed but you could always make a different choice since we're thinking about what the choices you make imply. So one choice that we could use, we've been talking in this class a lot about some kind of squared error, and so here we have a notion of a squared error loss: we're taking, from this particular point, from wherever this person lives, to the food truck, we're taking the Euclidean distance and then we're squaring it. So one reason that we might consider squaring the Euclidean distance in a loss here is that people really, really don't like to walk very long distances. Having 10 people walk 0.5 miles is not the same as one person walking five miles. Five miles is prohibitive, they might just not even bother to go to the food truck at that point and so the squared loss penalizes far away food trucks even more than we might expect from just maybe a street distance or some other type of loss. It's a choice but it's a choice we're going to make here: we're going to use this squared Euclidean distance as this loss. Okay, so now we can ask: well, what's the loss across all of the people that this charity is serving based on this choice? What is the loss for a particular arrangement of the food trucks and the assignments of people to food trucks? Okay, let's think about what's going on in this equation here. So first, we're saying, for every person, there's a component of the loss (so that's the sum from i = 1 to n) and now, for all of those people, we're going to say “hey, this is the loss that we assigned to them.” The j index here is replaced by y^(i) because that is the food truck that this person is going to go to. Now what I'm going to do is I'm going to write this exact same formula in a slightly different way. So exactly the same thing, just adding up the loss over all the people, we're going to write it in a slightly different way. So here's a slightly different way, let me just talk through this. So again, we're just summing up the loss for every individual that this charity is serving, so i = 1 up to n. Now what we're doing is we're saying one way that we can represent this loss that we said was the squared euclidean distance between an individual and the food truck that they're going to is let's consider every possible food truck that they could have gone to from j = 1 to k. Only one of these values will be non-zero. So remember, this indicator notation that we've used in a few lectures before, since indicator notation is 1 if what's inside of it is true and 0 if what's inside of it is false, and so all but one of the food trucks are going to have 0 here. The one that the person is assigned to is going to have 1 and in that case we're going to say “what's the square of Euclidean distance to that food truck?” So hopefully you can convince yourself that this is just exactly the same formula as summing up over all the individuals and saying what is the the loss between them and the food truck that they're going to. Okay, so now we have this loss and what we've done in the past in this class whenever we've had a loss is we've said, “hey, let's try to use that to optimize some parameters that we're interested, some values that we're interested in” and so we can think about what are we interested in doing here. Well, we're interested in placing the food trucks and assigning people to the food trucks and so we can do that here, we can say okay… Oh by the way, and I'll just mention really briefly before I do that, this summation order actually doesn't matter and hopefully you can convince yourself that I could just switch them, I could have the sum of the k before the n and that wouldn't change anything. So totally the same, doesn't matter which one I use. Here's just a different one. Okay so now that's the loss and I want to minimize it. So here mu is going to be the collection of all the food truck locations, y is going to be the collection of all the assignments of individuals to food trucks, and I want to find the mu and the y that are going to minimize the loss, that are going to make it so people don't have to walk very far to get their food trucks, food from these food trucks. So that's going to be my goal and trying to satisfy this goal is called k-means clustering and this thing that we're minimizing here is called the k-means objective. Why is it called the k-means objective? Well, one, we've seen before that the thing that we're optimizing when we have an optimization problem, we typically call that an objective, so that's why this is called an objective. Why is it called k-means? Well we have these k food trucks that we're trying to place. So that's the k in k-means (we have these k centers that we're trying to place) and the means are going to make a little bit more sense in a moment. So that part isn't yet fully explained but we'll get there. Okay so I have my food truck problem here. I know what I want to do: I want to try to place these food trucks optimally and I want to assign people to food trucks optimally, and now I have to think about how am I going to do that. And, again, this has somewhat of a similar flavor to what we've done in the past in this course that we start by saying “here's an optimization problem that represents some values that we want to learn, some values that we want to optimize on these parameters of our problem, and then we think about how can we actually do that, how can we actually optimize that problem, what's an algorithm that might help us do that.” Now, as with any optimization problem, there are a lot of different ways that we can approach this, it doesn't have to be one particular way, but it turns out that there's a really venerable algorithm that just about everybody uses. If you ever do any clustering in your life, there's a really good chance that you'll be doing it with this algorithm and it is called the k-means algorithm so we're going to walk through what that does. Okay. So our k-means algorithm is going to take two inputs. In this particular case, it's going to be k (essentially the number of food trucks from what we just discussed) and tau which is basically a maximum number of iterations. Something we've seen in the past a lot is that it's always good to have some kind of maximum iterations, if nothing else has a fail safe, so that you don't overdo it and run your algorithm forever if there's a bug or something like that or even if there's not a bug, some of these might just keep going. Okay now we've kind of vacillated on whether the data is an input to this algorithm and so like implicitly it certainly is here and you can make it explicit too and in fact the notes do that. So let me just say: you will be inputting the data into this problem and let's say what is the data. The data is the collection of feature vectors, of these individuals, all the individuals that we're thinking about saying where should they get their meal from a food truck and so, in particular, it's a bunch of feature vectors and I want to emphasize that it is just feature vectors. So in the past, when we were doing regression, when we were doing classification, the input to our algorithm was always a set of pairs: a feature vector together with a label. Maybe it was a feature vector together with a class label if it was classification. If I was doing regression, it would be a feature vector together with some kind of real value label and now we're totally changing that: we just have feature vectors. There is no label on those feature vectors, we just have these, in this case, x_1, x_2. In general, it could be a more general feature vector.

Okay so the first thing that we're going to do is we're going to drive our trucks into town. So we initialize their location somehow. Essentially that's bringing them into the picture so here's our trucks. They're appearing, they're now initialized. It's worth asking: how would we do this in general? How would we initialize the food truck locations? We could say randomly. I feel like random is one of those words that like always really sounds good until you actually have to code it up and then you have to think to yourself what does random actually mean and so here are two examples of what you might mean by random in this problem that would be very concrete and that you could actually code up: one, you could choose randomly from the data points you have. So one thing that's really important if you do that is to initialize without replacement. It's worth asking yourself, once you've gone through this algorithm, once you've seen everything, all the details in the algorithm: what would go wrong if you initialize two food trucks at the same location? It's like a good exercise to go through later. Another option is you could find the span of the data in every dimension and you could say uniformly at random from that span, you could initialize the location of each truck. That's another perfectly fine option and that seems to be what I've done here because these trucks are not on top of particular data points.

Okay, so now what we're going to do is we're going to do our iterations for t = 1 to tau. Tau, again, is the maximum number of iterations. We're going to take a few steps. What are those steps? Well the first thing we're going to do is we're going to go to every person and we're going to say what's the best food truck for you? What's the best choice of food truck given the current locations of the food structure? One way to think about the k-means algorithm is that it's essentially a sort of greedy algorithm. We're going to say, and it's a coordinate descent algorithm if we're minimizing, so what we're going to do is we're going to say, for all the people, what's the best option for them given the current location of the food trucks and then, for all the food trucks, what's the best option for them given the current assignments of the people and we'll go back and forth and back and forth doing that. Okay, so first let's go to all the people and let's say what's the best food truck for them to go to? So how can we write that? We can say that, remember y^(i) is the assignment of the i-th person to a food truck so that food truck ranges over the indices j = 1 to k, and so we want to say what's the closest food truck essentially? And so that's what we're doing here, we're just saying what is the closest food truck?

And so if these are our food trucks, which I've now colored to hopefully encode them a little bit, we can color all of the points that are closest to a particular food truck with the same color. That's us assigning them to the same food truck. Now sometimes, it's a little bit hard to distinguish different colors and so I'm also going to draw some lines to hopefully just help with the visuals. So all the points that are near a particular truck, again, are being assigned to that truck. You don't explicitly get lines like this (technically what we're constructing here is a Voronoi diagram), you don't explicitly get them from k-means but you can construct them as soon as you have the food truck locations. But again I'm just using that as an extra visual aspect here.

Okay so now, we have an assignment of every person to a food truck, that's the colors or it's distinguished by the lines, and we're going to take the next step in our algorithm.

So the next step in our algorithm is we're going to say, “okay, for every food truck now (so we just assigned every person to a food truck), now given those assignments, we're going to say: what's the best place to put the food truck?”

Okay let's think about what is the best place to put the food truck.

I propose that it is the mean of the individuals assigned to that food truck. You can actually work this out for yourself to show that that's true, but let me just convince you that what I have written here is in fact the mean. So what are we seeing here? So we're saying mu^(j) is the location of the j-th food truck. So this should be some vector of size equal to our feature vector because this food truck exists in the same space that our x's exist. What am I going to do here? I'm going to sum over every person but I'm only going to count the people who are assigned to this food truck. So another way you can think of this sum is it's actually just a sum over people who are assigned to the food truck because we're zeroing out everything else and then for everybody who is assigned to this food truck, we're summing up their x value, their location.

And then finally, in the denominator, we're summing up the number of people assigned to this food truck. We make a 1 for every person who is assigned to this food truck and a 0 for every person who is not. So essentially, this is the number of people assigned to the food truck and so this is the sum of locations of people assigned to the food truck divided by the number of people assigned to the food truck and so that's just going to be an average. And, in particular, it's going to be an average in every dimension of the feature vector because the x's are our vectors and the mu’s are vectors.

Okay so let's see what that would look like. So here's my current location of the food trucks. Now I'm going to take this formula and update the location of the food trucks with the average of the people who are assigned to them and so I'll have the food trucks move over there.

And now what we're going to do is we're going to keep iterating on this. So we have our iteration t and, again, it goes from 1 to tau.

Oh there's a question I see: is the j loop within the 1 to n loop? And it is not and this is an important point that we have our overarching t = 1 to tau and so within that, we have two separate steps: the first step is assigning people to food trucks conditional on knowing the food truck locations, then we assign food trucks to people conditional on knowing the people assignments and those are just two separate steps and they're not within each other.

Okay so now we're gonna go back to that first step, the assignment of people to food trucks, because now we have new food truck locations and so we want to see what are the new assignments of people to food trucks and so I'll just update that. A lot of these food truck locations were somewhat similar. We're really seeing that the blue and the green moved quite a bit and so we see a lot of change in those assignments down at the bottom. Okay so the next thing that we're going to do, again, is we're going to, with these updated assignments of people to food trucks, we're going to move the food trucks again. So we'll go down here, we'll move the food trucks to the mean of their currently assigned people, and we just keep doing this. So now, we're on the next iteration of t, go back to the assignments of people to food trucks and we update that.

Now we update the food truck locations. You can see that it was really just the blue and the green that really moved this time down there at the bottom and now we update the assignments again and now we update the food truck locations and we update the assignments and we update the food truck locations and it turns out that nothing changed this iteration: none of the assignments of people to food trucks changed at all.

So I propose that they will never change again. Now that they have not changed once, they won't change again. Do you agree with that? How can I know? This is a question for the chat: does anybody have any thoughts?

Okay great. So first of all, people are observing in the chat that once these assignments have not changed on one iteration, they will never change in the future no matter how many iterations I run. The second observation that people are making is that, remember, we're choosing the assignments based on the means and so once the assignments are fixed, the means are fixed, the mu^(j)’s are fixed, and if the mu^(j)’s don't change, the assignments won't change because we're just changing them based on those means and so you want to convince yourself that this is true but hopefully you'll at least go along with me for the moment that once we have no new assignments, it's not going to happen again, we're not going to get a change again. Okay and so for that reason, maybe we set tau to be a 100,000 just to be super safe, but maybe that's a big waste of our time and so we might want to cut out early if we're not making any more changes and so let's change this algorithm to reflect that and to take advantage of that. Okay so the first thing that we're going to do is we're going to record what were the old assignments. So just before we go into changing the assignments, let's say what were the assignments that we had before we made any changes. So we're going to say y_old is the set of assignments of people to food trucks before we make any changes, before we do that for loop over the y's, and then we'll ask ourselves: did we make any changes here? All we're saying is that if y didn't change in any way when we had our chance to update assignments, then let's not bother keeping going, let's just break out and be done. Okay now technically speaking, there's something missing from this right now which is there is no y to assign to y_old at the beginning of that for loop and so we need to initialize y somehow. Now since, in this particular instantiation, the literal only reason that you have an initialization of y in the beginning is to check if anything has changed. Your first initialization of y should be something that would detect that there's a change so you might, for instance, assign all the data points to the same truck or something like that. You wouldn't want to accidentally not have a change in this case.

Okay and then, finally, what we're going to do after we've done all of this, is we're going to return our food truck locations, the mu^(j), and our assignments of individuals to food truck, the y^(i), because that was the point: we went into this algorithm trying to optimize our, in particular, what we want to do is we wanted to figure out where to put our food trucks and where to tell people to go, which food truck should they get food from, and so we want to make sure that we return those things. Okay so this is the k-means algorithm. Let's think about what it's doing in relation to things that we've talked about earlier in the class.

So, in particular, there are a lot of things I think that sound pretty similar between k-means and classification. Let's ask ourselves: was this classification? Did we need a new algorithm? Could we have just called this classification and used one of our existing algorithms? Certainly something that is true is that we assigned a label y^(i). y^(i) takes k different values as the k food trucks to each feature vector x^(i). Okay so my questions for you, for the chat are: one, is this classification and, two, why or why not?

Okay great so we're getting some great observations that a big difference here is that we didn't have any labeled data, that we were learning from… If you look at this picture it looks like there are labels on all of our data but remember, we came up with them: we came up with not only the assignments of data points to labels but the labels themselves. Where were these trucks even located? This is the data that we got in to begin with. This data has no labels and that is the difference between clustering and classification. In classification, you start with labeled data and you learn how to label new data based on the old labeled data. You start with labeled training data and then you consider, for new test data, what are you going to do with it, how are you going to come up with labels, and you use that label training data. In clustering, you start with no label data, there are no labels, and you just find a pattern in that data, you just find these groups.

Okay so big difference here between what we did and classification is that we didn't use any labeled data and there's a more subtle difference too which is that the labels here don't have meaning. So if I were doing classification, if I were doing something like saying “hey, I'm trying to decide whether I should run outside and I run when it's really warm and I don't run when it's really cold,” if you told me to run when it's really cold and not run when it's really warm, I would have a bad time: my life would be really annoying, I would be very uncomfortable, like my life would fundamentally change. In this case, if I took my labels, my trucks, and I just permuted them, like I moved them around like here I've just put different colored trucks in the same location, nothing has changed: I still am feeding everybody just as well as I was with the previous locations of the trucks. I could totally change all the labels, I could permute them, and I'd have the same result and that's because what we're doing in this problem is fundamentally very different than classification. Classification, again, we're trying to provide labels based on our old labels. Here, we're just partitioning the data. Now a partition is a grouping into mutually exclusive and exhaustive sets. That is to say, every person is assigned to one truck and only one truck and so all we're trying to do here is group the data together. We're just trying to say “Here are data points that go together. Here are data points that go together. Here are data points that go together.” and the label is beside the point. So this is a more subtle point but if you're ever trying to decide “am I doing clustering or classification” it's helpful to think: did I have labeled data that I was learning from?

Okay, now that all being said, I've kind of said a few times now what we're doing. Let's make that formal. What are we actually doing here because it's not classification? Hopefully we agree and have established that.

So what did we do? Well, it's going to be something that needs a new name because we didn't do it before: we clustered the data, we did what's known as clustering. Okay what is clustering? It's grouping data by similarity. So that is to say the data, we start with, a bunch of unlabeled data, just a bunch of feature vectors. In this particular case, it was latitude and longitude of particular individuals but, in general, it could be any feature vector: it doesn't have to be two-dimensional, it doesn't have to be latitude and longitude, it doesn't have to have this particular interpretation. It could be as high dimensional as you want but it's a bunch of feature vectors and they're unlabeled. 

Now we have some notion of similarity. In this particular case, we use something like Euclidean distance. In other cases, we can use other notions of similarity. It doesn't have to be Euclidean distance on the feature vectors or could be Euclidean distance on different feature vectors, but we have some notion of similarity and we use that to group the data, again, to form a partition of the data where we have a bunch of groups of data points where all the data points belong together when they're in a group together. So that's what we did: we did clustering.

And this is what we got. These are the clusters that we got out. Now it's worth noting, again, we don't have to have this food truck interpretation when we do clustering. In general, we could use the k-means algorithm or we could do k-means clustering and come up with a set of cluster centers for more general feature vectors. So if I have a general set of feature vectors that are my data, then I'm doing clustering on them. I can use this k-means algorithm or some other form of k-means clustering to come up with a bunch of cluster centers and the assignments of data points to clusters and, in fact, I don't have to have cluster centers to do clustering: all I have to have to do clustering is grouping data points together. In this particular case, I use cluster centers to help me do that but I don't have to to do clustering. I just have to say, “here's the way that I'm grouping data points together.” Okay now let's talk a little bit about: why bother with k-means clustering or the k-means algorithm? Why don't I just plot the data and see that there are these clumps? I mean, in this case, it seems really natural in our example that we're looking at where people live. They probably tend to live in cities. If I looked at a map, I could probably find out where those cities are. Like why can't I just plot the data? Well my first point to you is you should always plot your data. You absolutely should plot your data. You should plot your data before you start doing any kind of machine learning algorithm and then when you're done with your machine learning algorithm and your analysis, you should plot it again to make sure it makes any sense. You always plot your data. Never don't plot your data. Man if there's almost one thing that you get from this class, it should be to plot your data. It's just such a useful, useful thing to do to understand what's going on. It'll help you understand so many bugs. The point that I'm making now is not that you should not plot your data—you should definitely plot your data—it's that sometimes plotting the data isn't enough on its own, that you do still want to run some kind of algorithm. Okay so why would that be? One is precision. I mean we wanted to park these cars, these trucks in a particular location that really helped people. I don't know that you could eyeball this data and find the absolute best location to park your trucks. You could probably get a reasonable approximation but if we actually care about exactly parking them and really saving people's time, then we probably want to run an algorithm that will find the best place to do that.

Something that's worth thinking about is that if you have 10 million data points and you try to plot them on one plot, you will fail: it will just be a big blob. It'll be a big blob that means nothing. Now that's not insurmountable. You could do a heat map, you could do a density map, there are things you can do to get around that, but you will have to do those things. A bigger issue in terms of just plotting your data is high dimensions. So remember, we've talked about this before in this class: as soon as you get above two dimensions (I mean, we as humans can only see two dimensions. It's an unfortunate limitation that we have but we have to deal with it), you should still try to plot aspects of your data, but when you get into high dimensions it'll be helpful to have ways to deal with high dimensions to be able to use them and you can still do that with an algorithm, in particular, with the algorithm we just covered. Another big reason that we care about having these algorithms is high volume. Let's say that you're Google and you want to do some clustering on every one of the search pages that appears to individuals who are using your search. There is no way that you're going to have an individual like an actual person look at every one of those and say something about it. You want an algorithm that's going to be quick and automated and fast and actually get you results. Okay so even when we're not trying to exactly place our trucks, we're just trying to find these clusters in our data, it might be useful to have an algorithm like k-means clustering. Okay so what we saw here was an example of what is known as unsupervised learning. So unsupervised learning is any case where we don't have labeled data and we're trying to find patterns and this is really distinct from, again, the types of supervised learning cases that we've seen before where we do have labels and we're trying to do something with those labels. We're trying to predict those labels on future data like classification and regression.

So before I go on, I'm just going to make a little Venn diagram to emphasize these relationships. So in particular, something that we just covered was using the k-means algorithm. I might have a problem where I use the k-means algorithm but that is not the only way to do k-means clustering. Anytime I have the k-means clustering objective and I'm trying to optimize it and I can optimize it in other ways, it doesn't have to be with the k-means algorithm, I would still be doing k-means clustering. Now likewise, I can do clustering without using k-means clustering. So for instance, maybe I have different forms of the loss. Maybe it's not exactly what I talked about before like, for instance, the reality is that people walk along blocks and that's actually the Manhattan distance, that's not the Euclidean distance. Maybe I want to encode that somehow. If I do some other type of loss or if I do something totally different (it doesn't even have to be exactly this optimization objective that we talked about but if it's some way that brings my data points together in groups), that's still clustering but it's not k-means cluster and it's worth noting that if I do change the loss a little bit, it's not clear that you can use something like the k-means algorithm to optimize that still. Finally, you can have forms of unsupervised learning that are not clustering. For instance, suppose I have a bunch of data points and my data points represent documents. Maybe they're all the documents in Wikipedia, maybe they're all the documents on the web, and I want to cluster them to find the topics or the themes in those documents. So I could do clustering but maybe I think that some of my documents have multiple topics in them, like maybe if I look at an article from the New York Times and it's about the movie “Moneyball,” well it's about arts because it's about a movie, it's about sports because it's about baseball, but it's also about economics because it's about trading players and making money and so in that case, I might not want clustering where each data point is assigned to one and only one group. I might want something else where I can belong to multiple groups and so, again, there are many forms of unsupervised learning that are not just clustering. Okay so this gives us a little bit of a hierarchy how is this all related, what's the the bigger picture, but now let's delve deeper into understanding the k-means algorithm and what it’s doing because it is a particular clustering algorithm that's very popular, it's a particular k-means clustering algorithm that's very popular. Tamara can I ask a quick question? Yeah there's a super interesting question related to this: is there a version of k-means that doesn't take in k as a parameter but rather learns it instead? Oh yes we will talk about choosing k very shortly but it's a very natural question: how do you learn the number of clusters and, unfortunately, there is not a fast answer to it. It turns out it really depends on what you're trying to do but we'll add a little nuance to that shortly. Great.

Okay before we get to that, let's talk about initialization of the k-means algorithm.

Okay so first of all, why are we doing the k-means algorithm at all? Does it even work? Does it do anything useful? So here is a result that actually you can prove for yourself if you think about it for a bit. If we run the k-means algorithm for enough outer iterations, so I'm talking about that big t iteration that's on the outside, the biggest for loop, then this k-means algorithm will converge to a local minimum of the k-means objective. So what I mean here in this particular case by converge is just that it will stop changing and what I mean by local minimum is that if I change the mu or the y a little bit, I'm not going to get something that's better. Okay so let's think about this. That sounds really good. We get to this local optimum and, in fact, that's what we saw if we initialized like this. Then we get to this optimum and, in fact, this is a global optimum. It turns out it looks pretty good, right? I mean each of the trucks is in the middle of one of what seems to be a small town or city and so that feels like a good assignment of trucks to individuals. Now the horrible truth that I want to tell you here is that this is not the first random initialization that I ran. I actually ran two other ones first before getting this one and so maybe we should take a look at those and see what happens. It will turn out that the local minimum can be bad. Okay so let's look at the first random initialization of trucks that I tried. So this was our original one. This is the actual first one that I ran, just a random initialization. What happens if we run k-means from these trucks, from this initialization? This is what we get.

Question is: is this good? Is this bad? What do you think about this alternative clustering of our data, of our people? Is it better? Is it worse than the one that we saw before? This is a question for the chat.

Okay there are a lot of really good responses here. Okay so the first one that I'm going to highlight, the first type of response, is people saying it's worse and so let's notice a couple of things here. So one, there's not really like a truck per cluster and so if we really think that “hey I can kind of identify visually that there are roughly five clusters here, I think that maybe a truck should be on each one and so it seems intuitively that we're not picking that up” but even more than that, and I think really to the point, people are observing that, one, it depends what you're trying to do which is always a good observation in machine learning and, two, what we're trying to do here is we're trying to optimize the k-means objective. We really care about minimizing this loss to the people who are using our food pantry service in this case and so what we really care is: did the loss go up or down? Did that total k-means objective go up or down and I can tell you and you can think to yourself why you might find this intuitive. Then, in fact, the k-means objective is worse in this case. It is not as good as in the previous case that we saw. Overall, the trucks are farther from the people who are using the service and so, in that sense, and that's a very specific sense, this is a local optimum not a global optimum and it is bad because it's not optimizing the objective as well.

Okay let's try the second random initialization that I actually tried. So we just answered this question, let's try the second initialization that I randomly tried. So here, it is: there's another random initialization. Now we're going to run k-means clustering on this random initialization and this is what we get.

Again, I'll just tell you but you might want to convince yourself that this seems intuitive, that this is a worse value of the objective function because it's a higher value of the objective function than the first example that we saw, the one that we saw when we were first introducing the k-means algorithm.

Okay so the first observation here is the initialization can make a big difference. If I have different initializations, I'm getting really different clusterings out and remember that's all I can do with one run of the k-means algorithm. Once I start running it, I run it, it stops moving eventually and then that's just it: it's never going to change again and that's how I'm getting to these points. I'm getting to the point where it just isn't changing anymore and it's going to get stuck in that local optimum, it's not going to move to a global optimum. And so we have to think about, okay, well is there any way that we can deal with this? Is there any way that we can maybe get out of this local optimum? Now there's a venerable idea in machine learning that comes up anytime that you might get stuck in a local optimum and that is random restarts. So the idea of random restarts, and it's essentially what I've done here, is I randomly initialize. I run my algorithm (in this case, the k-means algorithm), I get out my output, and then I do it again: I randomly initialize, I run my algorithm, I get my output and then I do it again. I do this for some number of times. In this particular case, it was three: I have three different random initializations, I run my output, I get my final output, and this is something that's inherently massively parallelizable: you could just do this on a bunch of different computers in parallel and so you could take advantage of that and now let's be concrete. Suppose that I randomly initialized three separate times, I ran from every one of those initializations, I got out for every one of those initializations an assignment of trucks of where the trucks are and an assignment of individuals to trucks. What do I do with that? This is another question for the chat. Suppose I did those three random restarts and I got the output of the k-means algorithm every time. What do I do with that? What do I do with these three outputs? 

Okay great so lots of people are observing I should calculate the loss or the objective for every one of those random restarts and then I should take the one with the lowest objective because, again, our goal is minimizing this objective or at least that's our stated goal and so if that is our goal, then what we want to do is we want to take all of these different outputs, these different random restarts, we want to say, “hey, which one is really minimizing the objective, at least across those random restarts?” and keep them.

And at the very least, we'll know that we have the minimized objective across all of the different assignments of mu and y that we had explored. That's not a guarantee that we'll get the global optimum, we can still only guarantee a local optimum, but at least it should be better than any particular run or at least not any worse than any particular run to be very precise. Now it turns out that there are other things that you can do that are beyond the scope of our lecture today. I'll just mention one of them. Oh okay so we just did these three random restarts, what do I return? That's what we just talked about: we're going to return the one with the lowest objective. I mean it turns out there's lots of work in this initialization area. One particularly cool idea is what's known as k-means++, it's an initialization scheme for k-means and so if you're looking for something cool to read up on, I'll definitely recommend k-means++.

Okay so we've talked about initialization, we talked about how it can really matter, can really change things, and you need to be aware of that and potentially try to cope with that in some way. One idea is random restarts, one idea is k-means++.

Let's notice, as in fact our question asker has already noticed, that k has an effect as well. So we just saw that the initialization really has an effect and of course, intuitively, the choice of k must have an effect. If we have five trucks at our disposal, if we have five trucks that are available to us, then we can get them this close to the people who are getting the food from the trucks but, of course, if I have a different k, that will get different results and so, for instance, if I had four trucks, I just can't possibly get the same clustering that I did with five trucks in this case. I, in fact, if you think about it, we'll just not be able to get the trucks as close to the individuals who need the food and, in general, this is what we can expect. I mean hopefully, again, this is intuitive that if I have fewer trucks, I can't get the trucks as close to individuals. If I have more trucks, I can get them closer to the individuals who need the food. This is why we would like to, if we were this agency, if we had more money, we'd like to spend it on more trucks so that we can reach out to more people.

So we see that the larger k, having more trucks, gets them closer to people or, in general, if we're not thinking specifically about trucks and food service, that having more k, having more cluster centers, having more clusters, allows the clusters to be smaller, to be more compact. Okay so here's a question for you: can you identify, can you describe in words, in the chat, the optimal clustering if the number of clusters is n? Oh sorry this should be little n, the number of data points. I'll correct that in the slides afterwards, but if we have k = n, the number of data points. So, in particular, in this particular case, if I had the number of trucks equal to number of people, can you describe the optimal clustering?

Great. So the observation is every person gets a truck, there is a food truck that is right outside your home, you just pop outside, get your food, and head back in. It's the best possible clustering: everything is zero feet or zero degrees of latitude and longitude away from your house, so everybody gets their own cluster, everybody's in their own cluster, and the objective, we'll have the distance from every person to every truck, is just gonna be zero. It's hard to do better than that. Okay so now that we've observed that we get these different clusterings when we have different k, let's think about choosing k. Now the first thing to notice is that sometimes we just know k: we're given k by somebody else and so we don't need to worry about choosing it in the sense that somebody told it to us. So an example was if we are working with our food truck friends with this charity and the charity tells us I have five trucks, they have five trucks, so k is 5, and that's the optimization problem that we're solving and there's no more work to do in that sense. But this could happen in a lot of other cases. So suppose that I am working for a web search engine and that search engine is interested in clustering search results to display just a few similar results and maybe they only have space for four results to be displayed on their page and so they tell me, “hey you gotta come up with four clusters.” So for instance, suppose that I search for the word “drake.” Did I mean Drake the musician? Did I mean drake the male duck? Did I mean Sir Francis Drake? Did I mean Drake University? Who knows and so the search engine might display a cluster for each of these types of articles. There would be a cluster for articles about Drake the musician which are all somewhat similar on the order of articles about “drake”, there is a cluster of articles about Sir Francis Drake, a cluster of articles about drake the male duck and a cluster of articles about Drake University and because they only have four spaces to display something on a page, I mean there's only a limited amount of screen space that you have, I know that k is 4. Similarly, I might be clustering customers into groups and I'm going to assign a staff member in my business to each one of my clusters and so maybe I have k staff members or the budget for k staff members. Maybe I'm clustering books on k bookshelves and so I know that I have k. Okay but the observation that the previous question made which is that sometimes we'd like to choose or learn k and so, maybe, actually what happened is I was looking at a bunch of topics, again, a bunch of documents that are on the internet, and I want to know how many topics are in those documents. Maybe I'm looking at a bunch of animals and I want to know, I have a bunch of observations of animals, and I want to know how many little groupings of those animals are there. Maybe I'm working with this food truck service and they want to know how many trucks they should buy, how many trucks that they should deploy and so, in all those cases, there's some sense that we'd like to choose or learn k but it's worth noting that those are actually pretty different use cases that I've just described.

So the first observation is that in none of these cases can we just minimize the k-means objective over k. So let me first be explicit about what I mean by that. So here is our k-means objective from before. Again, it's just the sum over all the individuals, of their loss, so it, in general, it's this sum of square Euclidean distances. In our truck example, it's the sum of the squared Euclidean distances of individuals to their nearest truck or to a truck in this case but we want to find the nearest. What we had previously done is we tried to minimize this over the assignments of individuals to trucks or, in general, the assignments of data points to clusters and the cluster centers or, in our particular example, the truck locations and so now what I'm proposing is: why don't we also just minimize over k? And this is a real question for you in the chat again: why don't I just minimize over k? Why don't I just add that as a parameter in my optimization problem or should I add that as a parameter in my optimization problem?

Okay great, so lots of people are observing that if I do this, I can already tell you the k that's going to optimize this objective, it's k = n, because remember, on the previous slide, we observed that when we choose k = n where n is the number of data points, we can set y and mu such that this objective is zero. Notice that this objective is always non-negative and so zero is a global optimum and so this is optimized at k = n. But k = n is not interesting, like that's not why we did clustering right? I mean all that does is it says, “hey look here are my data points” but we want to find actual structure in the data, we want to find something that's useful that could tell people where to put their trucks. Nobody's gonna buy one truck for every single person and so we're going to need to rethink this, we're going to need to think of a better way to approach choosing k.

And of course, as is true for everything in machine learning, it really depends what you want to do. So let me at least name two different types of problems that we've talked about here. So one type of problem there's a cost benefit tradeoff. Let's say I'm working for, again, this charity that distributes food trucks and I want to decide: how many food trucks should we have? Well, in some sense, there's a cost to having fewer food trucks which is that people have to walk farther, it's harder for them to get food, but there's also a cost to having more food trucks. There is literally a cost for the truck and for maintaining it and so on and so, very roughly, something that we could do in this case is, if we could quantify the cost of having more food trucks, we could actually put that into our objectives. So that's what I've done here: I've said, “hey, there's a cost to not having enough food trucks and there's a cost to having more food trucks, maybe we can trade that off explicitly an objective or something like this.” Similarly, with the web search example presumably there's a cost to having a larger page of search results and that people tend to not want to scroll too far and so maybe we could trade that off explicitly. If I'm a bookstore owner, there's a cost to having more space in my bookstore and more bookshelves. If I'm a customer service representative and I want to group my customers in order to assign staff to them, there's a cost to having more staff. So in this case, if we can, we'd like to explicitly represent that cost benefit trade-off. Now in other cases like we're trying to find the topics in a set of documents, we're trying to find a grouping of animals, that's not a case where there's a clear cost-benefit trade-off but it's also worth noting that there isn't necessarily a single right answer to the number of clusters. Like let's say that I made a bunch of observations of animals in some environment and I want to cluster them. Well if I cluster on one level, I might get the species. If I cluster on another level, I might get the genuses. If I cluster on another level, I might get the kingdoms and so it's not clear that there's this one right answer to clustering and so is there any reason that we should expect that an algorithm would find that? So this isn't a clear cut and dry answer to how do we choose k in part because there is no clear cut and dry answer how do we choose k, it really depends on what you're trying to do and at least this gives you a little bit of a flavor of the pitfalls that you could encounter in trying to choose k.

Okay speaking of pitfalls, let's talk about some other ones that can arise with k-means clustering.

So k-means, I'm going to observe and then I'm going to show you, k-means works well if I have well-separated circular clusters of the same size. So I'm going to break down each of the elements in this observation.

So first, let's just notice that these are well separated clusters. So I'm just using our visual intuition here that we can kind of identify the clusters in cases like this and so here we think that there are probably two clusters. It's unlabeled data but I think you can kind of identify what they would be just intuitively. They're well separated: they're not right on top of each other. They're circular roughly, the data is roughly within some kind of radius in each case and they're roughly the same size both in terms of radius and, I'll tell you right now, they have the exact same number of data points in each cluster. If I run k-means on this, at least I run it maybe with multiple random restarts and I do all this stuff, this is the optimum assignment of the cluster centers and the assignments of data points to clusters. This is what I'm going to get. That seems pretty much in line with what we were talking about earlier with our truck example, with our food trucks, where we were trying to assign the food trucks, we're trying to assign people to food trucks. We ended up seeing that we got, our cluster centers, were in the center of the clusters and the groupings that we got made sense. When we looked at that picture with all the data, maybe it was a bunch of where people live and in some square-shaped state like Colorado or Wyoming, and we saw that roughly we picked up the cities or the towns, the groupings of people, and it seems like we have an intuition that that's what we'd like to do but let's notice that our intuition about what clustering should do is not necessarily what k-means is going to do and it's worth separating those two things out. k-means is always going to assign your cluster centers as though they were trucks and trying to find the optimal placement of trucks for serving individuals who need food. That is not necessarily the same as something that kind of looks like a cluster to us and so that's what we're going to spend just a moment on now. Okay, first let's talk about “same size”. Now there are two different ways, at least two different ways, that clusters might not be the same size. So the first is radius, so let's think about radius.

So here, again, I have two what I might intuitively call clusters. Maybe, before we were thinking about our two features that we were plotting as latitude and longitude, maybe now let's think about them as we gathered a bunch of animals from the environment and these are height and weight. So maybe maybe the horizontal axis is height and the vertical axis is weight and if I gather this information, you might think “hey maybe there are two species here: there's one that has a lot of variety within the species, it's pretty spread out, and then there's one that's pretty concentrated and maybe I would like to pick up those two species.” So we can ask ourselves: that's maybe what I would like to do, but what does k-means do? So I want you to think for a second to yourself. You don't have to write it in the chat but I want you to to think to yourself what you think is going to happen here.

Okay now what I'm going to do is I'm going to show you the results of running k-means. Probably the best thing that you could ever do is play around with k-means later: try it out, see what happens, try out some examples that you're not sure what's going to happen, and run them. k-means is basically programmed just about everywhere, it's very easy to get code, it's very easy to write your own code. I mean we basically just wrote the pseudocode before and so just like everything it's probably best to just try it out, but here's what we're going to get. Okay. So the observation here, first of all, is that there's a lot of data in each cluster and so the centers are roughly in the center of each cluster as we expect, they are the means, the k-means, the two means. In this case, k = 2.

Now remember: we assign data points to clusters based on which is the closest cluster. So there's basically going to be a line down the center here between the two cluster means. Everything on the left goes to the left cluster, it means everything on the right goes to a right cluster and so because this right what we might think of as a cluster overlaps with that dividing line, some of it gets assigned to the left cluster. Again, whether this is good or bad, and I think people have rightly observed this in some of our Q&A throughout this lecture, it depends on what you're trying to do: if you're trying to assign trucks so that they are as close as possible according to the objective we described earlier to the people that they are giving food to, this really is the optimal thing to do, it optimizes the objective. If you're trying to discover that there were two species here and you wanted everything in one clump to go together and everything in the other clump to go together, this is not doing that and it's worth being aware that that's true.

Okay now this was one notion of "same size". Let's talk about another notion of "same size". So here, the two clusters were not the same size because they had different radiuses: one was bigger than the other. Here, I've made two clusters where one has way more data points than the other. So it turns out I generated these both from what's known as a Gaussian distribution. So they have the same generation scheme, it just turns out when you have so many more data points in one, it also gets a little bit bigger but hopefully you'll see that the point that I'm about to make really depends on the number of data points more so than the exact shape. Okay so, again, I want you to just take a moment and think to yourself. Again, you don't have to tell me, but you should tell yourself, commit to yourself, what do you think might happen when we run k-means here.

Okay now I'm going to tell you. Incidentally, this is a great exercise to do on your own later when you're running k-means: ask yourself beforehand:what do you think is going to happen and then run. Here's what we get. Now again, if what I'm trying to do is optimize my food trucks so that they serve people in the best way possible and I minimize the loss of people, this is the optimal thing to do: I reach more people more closely by having two food trucks in the absolutely giant cluster, by putting two food trucks in the big city, rather than one in the very small town. If what I was trying to do though was to observe that there are two different species here and I massively over sampled one of the species, maybe without knowing it, I mean, because I didn't have the labels to begin with, but maybe one of them is just really, really easy to find in the environment and one's relatively rare, I'm not going to find those clusters with k-means, I'm going to find this instead. So again, is this good or bad? It depends on what you're trying to do.

Okay so that's two different notions of “same size”. Let's talk about “circular”. Okay so now, something that we talked about way back in lecture three, we were talking about features, is that feature encoding matters and, in particular, the scale of your features matters. So one way that you could create the plot that I have right here is that I could take the original height and weight measurements from way back when I had those two perfect round clusters that were the same size with the same radius and the same number of data points and I could just change the scale of the vertical axis, like maybe I measure length in feet instead of meters or maybe I measure weight (if it's weight), I measure it in kilograms instead of grams or kilograms instead of pounds. This is a very easy thing to change the scale without thinking about it and this is what I would get. I mean these are clusters that would be circular if I had a different scale but here they don't look circular, they're very spread out because I have a spread out scale.

Okay again, it's worth thinking what might k-means do here. Just take a second for that. Again, just to yourself, no need to tell me but think about it for just a second. Well I'll tell you that this one is worth playing around with yourself because it really depends how stretched out the clusters are in the vertical direction what you're going to get, but in this particular case, these are the means returned by k-means. Again, if I'm trying to optimize my food trucks, this is great, it turns out to be the best thing to do, but if I was trying to discover that these were these two totally separated different species, I would not be discovering that here. Now an interesting metapoint, remember one of the things that we talked about to deal with different scales and different features, is to do some kind of standardization and that might help you quite a bit here but it also might not totally solve this problem. If you're just saying, “hey, I should standardize my features in such a way that maybe I have a particular range of my data and I always want to make that between 0 and 1,” I could actually end up getting something like this and so it's just worth keeping in mind. Okay so this is a case where we get away from circular clusters. What about well separated? So here, I'm not just showing you the data to begin with, I'm showing you what if we had secret knowledge that, in fact, there are two species that we're about to observe, it just so happens that if we only look at their height and weight, they actually have pretty similar values and so they're really going to overlap quite a lot and so if we were to run k-means, what we would see is not our secret information about having two species, we would see this data where there's a lot of overlap and then, of course, if we run k-means it can't distinguish that overlap. Some of the things we've talked about these examples with the circular clusters and the same size are somewhat specific to k-means. We're making this specific assumption about what objective we care about, we're making a specific assumption about how we're going to optimize that objective, but really these are specific to the objective. This one is more a point about clustering in general that sometimes what you would like to distinguish maybe isn't distinguishable in your data. If there is no separation in your data, you can't expect that an algorithm is going to get around that, that an algorithm is going to find that. It's similar to what we talked about previously with classification: if you try a linear classifier and there's just no good linear classifier, there's no way to distinguish classes in your data, you can't expect that an algorithm is going to solve that for you. It may be that you just need to go and you need to get better data, you need to go back to the drawing board, you need to think about what information you have at your disposal.

Okay so at this point, we've talked about k-means clustering in some depth: we've seen that there are things that can do really well, there are definitely limitations, and I think that this is not specific to k-means clustering, it's not specific to clustering, it's not specific to supervised learning, it's a very general set of observations about machine learning that this is true and hopefully it's something that we've really conveyed in this class that these can be very useful and powerful tools but they also have limitations and it's worth knowing those limitations when you're using them in practice.

Okay so I'd like to revisit my very first slide in this class, it was “Machine learning: why & what.” So remember, way back in lecture one, we talked about machine learning is in the news all the time. At the time I just looked up some of recent articles and I found a bunch. I'm sure that if we just did this exercise again, if we looked in Google News, we would find all kinds of new machine learning articles. I happen to know there are some big advances on a biological side that happened recently, there's just all kinds of things in the news. And so my hope is that in taking this class and then going over everything that we've done in this class that you are better able to read this news, to engage with this, and also to make the news yourself by using machine learning tools. We talked about, in this class, what is machine learning and in the very beginning we had this big idea that it was a set of methods for making decisions from data. We said see the rest of the course. Hopefully you have a better sense of what that means now that we've gone into so much more detail than we had in the first lecture. We've covered so much in some sense in this course: remember we talked about classification with a ton of different models, logistic regression was one of them but there were many others, we talked about regression, we talked about linear regression, and again many other models. For both classification and regression, we talked about neural network style models: we talked about convolutional neural nets, recurrent neural nets, also totally different models that are also very useful like decision trees, random forests, and now, on top of all of that supervised learning work, we've added in unsupervised learning. We've seen clustering as an example, we've gotten a sense that there's a lot more beyond that, beyond just clustering. We've talked about not just all these different models and things that we can do but also how to learn within them, what kinds of algorithms can we use. We talked about greedy algorithms like perceptron way back in the beginning. We didn't stop, we didn't say, “hey, we'll never touch a greedy algorithm again.” We use them to build trees. In some sense, we've even done k-means clustering now that has a greedy type flavor. We also talked about gradient descent, stochastic gradient descent. We talked about local optima in all of these cases being an issue. In general, this is a real challenge in optimization and how sometimes we have these nice convex problems but sometimes we don't and that's a challenge and how can we overcome that. We've even talked about that today but we've also talked about that in other contexts before today. Even beyond all this, we've also talked about interacting with the world: we've talked about state machines, Markov decision processes, reinforcement learning, and we've also talked about, and I think this is really important, that machine learning isn't just a set of algorithms. It's really a whole pipeline that you need to be really careful about: we've talked about choosing features, we had a little lecture on choosing features and hopefully emphasizing that it's pretty non-trivial in practice. Like those pre-processing steps before you run your algorithm really, really can matter. We've talked about initializing a lot. Here we talked about it today but we've also talked about it in other cases in this class and plotting your data. We've done a lot of plotting your data but I want to call that out as something that is really useful for you to do and practice, you can catch so many issues that way. You can catch not only so many issues beforehand, but also after the fact to try to understand what are you getting out from your output, what does it mean, what's going on there. It's really worth asking the tough questions about what you're doing. We've seen other aspects of the pipeline like ensembling methods to help predictive performance, to get gains in that way, things that you might want to do that aren't purely predictive performance like interpret what's going on with your machine learning algorithm. We've talked about evaluation, cross validation is a big example. We talked about cross validation versus using pure test or validation data and even though we spent so much time in this class on all of these topics, there's a very real sense in which they're all the tip of the iceberg. There's so much to machine learning that we've covered and there's so much that we haven't covered, there's so much else to learn, and I think there are various ways that you can go about doing that. You can go and take more machine learning courses. You can go and take courses that use machine learning and apply them to different ideas, to different application areas. It's really interesting the things that people do in different application areas. Certain methods tend to be relevant to different applications and it really depends what area you're working on, which methods might be particularly relevant to you, but also you can learn so much just by using these methods, by carefully interrogating them, by plotting what's going on, by trying them out. It's not all necessarily just about taking classes but actually getting some real world feel by working on problems, by getting a sense of what happens. I mean you really learn by doing and I think that's just a great idea to engage in that kind of thing. So remember, we said in the beginning: why study machine learning? This is like just the exact same slide that I had when I started. To apply it. Of course that's going to be a big issue is that you want to apply it and you want to apply it carefully. I think a really important thing that we've spent a lot of time on this course is not just what machine learning can do but what it can't do: what are the limitations? Where are the places where it might be inappropriate, where you might come up against some kind of barrier or maybe machine learning just isn't the right tool for your problem or maybe there are things that you have to do before you can apply machine learning successfully and so I hope that you have a better sense of all of those things. But I also think it's really important to understand machine learning for the purposes of understanding it because, again, it's influencing so much of our lives these days. It's used, increasingly, in ways that affect all of our lives: in cyber security and personal finance and health and all these different areas and so it's really important to be able to engage in a way that has a deep understanding of what it can and can't do because I think if you don't have that, it's easy to exaggerate in both directions, to think that it can be magic, that it can do everything, or to think that it's horrible and doesn't do anything well and it's evil and so you want to have that nuanced understanding to know when is it doing something good and what is it doing something bad and to evaluate, to understand when people have a discourse about machine learning, what are they talking about, what does it really mean. It is entirely plausible that some of you could be applying this not just in your regular lives but that it could come up maybe in a jury trial or that you're interfacing with the medical establishment in some way and it comes up somehow and you want to be able to engage with that and to be able to say something about it. I think that matters and hopefully at this point, it's much more clear, not just because we're telling you, but because you've experienced it yourself that machine learning is not magic. It's not going to find a signal where you can't find a signal, it's not going to solve all of your problems, but it might solve some of your problems and hopefully you have a sense of how you can do that. Also it is very much built on math. Hopefully you've gotten the taste of the math behind it and I think if you're mathematically inclined, there's a lot of really great math courses that you can take to get to a point where you can understand even more machine learning and maybe innovate in different ways yourself in machine learning. So before we wrap up on my part of the lectures, I just want to give a huge shout out to the amazing staff in this course. It's just been a huge pleasure working with our absolutely fantastic instructors TAs and LAs. I think, hopefully, all of you have had this experience as well, they're just so wonderful and I've really enjoyed working with all of them and I want to give a huge shout out to you: I think it's just been really fantastic working with you all this semester. I've so much enjoyed how much all of you have participated in the Q&A, how much you've been very engaged and your questions on Discourse and then your questions, in general, in the class have been just fantastic, I'm so glad that you're asking them. I hope that you realize that this lecture and the questions on Discourse and all these things: there is no penalty to asking, to asking about anything, to trying questions and trying answers that maybe aren't just right, but we learn by doing that and I think it's wonderful that you've engaged in that way and then finally I want to plug. Next week we're going to have a guest lecture by the fantastic professor, David Sontag. He's one of the instructors in this course and he's going to be talking about machine learning for healthcare which I think is just absolutely fascinating and ties into all the things that we've been talking about and so I hope that you will will be there next week. Thank you very much and I hope you have a great week.
