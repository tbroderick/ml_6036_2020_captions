Okay, before we get started today, let me just preface everything with: it is wonderful how many of you show up to live lecture. I love how many of you participate when I ask a question and answer it, I think it's actually especially awesome if people aren't getting it right and they're still participating, I think that's one of the best ways to learn, is to commit to something and then see why it's right or why it's wrong. That being said, there are sometimes special circumstances that mean we can't make a live lecture and luckily we are able to record this and make it online later and I think this week, there's an obvious special circumstance for many of us. If you are in the U.S. and you have not voted yet, just go do that, just go do it right now, come back, we're gonna have this all recorded later and you can check it out and we'll see you in live lecture next week and hopefully many of you are doing this already and you'll see this recorded later, but if you're in that situation that you are able to vote in the U.S., today's the day to do it if you haven't already, it's election day so make sure to do that and then check out the lecture. Now of course, if you've already done that or you're not able to vote in the U.S., then please join us along for today's live lecture. Okay so that being said, let's get back to our content for today. So in general, or in these two lectures, so in last week's lecture nine for week nine and this week's lecture ten for week ten, you can think of this as a pair of lectures, this is like the sequel to last week. Last week, we built up this idea that you could make actions that would change the state of the world and potentially gain some reward and so we built up this idea of a Markov decision process, we said how we could change the state of the world via actions, how we could get a reward by looking at both the state of the world and our actions and we did all that, we built that up that Markov decision process, and then once we did that, we said okay if we know everything in this Markov decision process, we actually now have the ability to choose the best action and best in the sense of getting the most long-term reward and so we did all of that and all of that very much required us knowing basically everything about the Markov decision process and so that's what we're going to be addressing today: we're going to say, “well what if you don't actually know in advance what reward you're going to get from being in a particular seat and taking an action or what transition function is operating in this MDP world, then how do you choose the best action, again, in the sense of getting the most long-term reward?” And so that will be our focus in the lecture today. Okay so obviously, in order to talk about that, it'll help to have everything that we did last week at the tip of our fingers. So hopefully, part of that is that you've been working on your labs and thinking about this and everything like that but let's just do a little recap anyway. So we had to find a few things, but we ended up talking about a Markov decision process and so there were multiple parts to the Markov decision process: one was we had to say that there were some states in our world and we had this running example which we'll have again as an example today of farming. So in particular, it might be that we have found ourselves buying some kind of farming field and it could have either rich soil or poor soil. So we're going to say that those are our states in this example and then we can take some actions and so in this particular scenario, the two actions that we said we could take are we could plant our farming field or we could let it lie fallow: those are the actions that are available to us. Now once we have those actions, we can apply them to both states and so we're going to end up having this diagram that suggests what happens when we apply our action of planting to either rich or poor soil, what happens when we apply our action of letting the field by fallow to either rich soil or poor soil, so let's just set up the diagram like so. And then we had a transition model. So this transition model tells us for every state, for every action that we make, and for every state that we might go to, what's the probability of that? So one of the ways that we set this up was with this diagram. We actually saw a few different ways to set this up in terms of just writing the transition model function T, writing these transition matrices,  but we're just going to go with this diagram for now. And recall, in the diagram, the idea is that we look at our starting state, we take some action, we look at the potential ending state and we say, “what's the probability if we started in the starting state, we took that action, and how probable would it be that we end up in that ending state?” So that's that probability. So here we have if we started rich soil and we let it lie fallow, then we have a 10% probability of ending up with poor soil and a 90% probability of ending up with rich soil again and, of course, these numbers coming out of a particular state should sum to one.

Okay so that's our Markov decision process. There's still a couple of things that we need to do in order to fully define it. So one is a reward function. So here we're not just talking about how we transition between states, but also what do we get for doing that and taking some action. So our reward function says for each state and each action, what reward do we get? So if I had rich soil and I planted it and I harvested from that, I would get a harvest of, in our example, 100 bushels. If I had poor soil and I planted it and I harvested, I would get a harvest in this example of 10 bushels and then finally, the last part of the Markov decision process was the discount factor. So this tells us how does a bushel next year compare to a bushel this year, how much is it worth to me this year, and typically we say that a bushel in the future is not worth as much as a bushel right now and so a discount factor will be typically strictly less than one.

Okay so that's our Markov decision process, these were all the things that were involved in it, it tells us how we interact with the world and what rewards we can get from the world and now we would like to do that and so, in particular, we're going to define a policy. So a policy tells us what's our plan going forward. If I am in a particular state, what action will I take in that state? We explored various policies last time, examples of policies: maybe I just plant all the time, maybe I alternate between planting when the soil is rich and letting it lie fallow when it's poor, maybe I do something else, there are a lot of different policies that I could take and we talked about this possibility. Now, of course, what I'd like to do is somehow evaluate these policies and maybe even choose the best one and so we talked about both the finite horizon case of this and the infinite horizon case. Today we're just going to focus on the infinite horizon case, so let's do that. So in the infinite horizon case, we're imagining that I have this farm for infinity: I own it and I'm going to be using it in all the future years and so I'm going to say what's the most long-term reward that I could get from this farm? And so first question I might ask is, “okay, well under different policies, what is that long-term reward, that reward added up over all the years including discounting if I start in a state s?” And so this might be useful if I'm thinking about selling my farm, I could compare that to the reward I might get from the farm under a particular policy. If I wanted to compare two policies, maybe there are only two that are available to me then I could look at those values. And so, in particular, we came up with this equation: so here, the idea is that the value of a policy starting from state s (so we imagine that the farm is in some state when I get it and I'd like to be prepared for whatever that is) is going to be the reward I get this season, let's say it's every year, then this year, by first enacting the policy this year, and then for the remainder of time, I'll have some additional long-term rewards. So that's all that's in this chunk and everything after this year. So, in particular, everything after this year will be discounted by gamma because it'll start next year. We're going to have the expected value of the remaining time, so to get that I'm going to sum up over all the possible states I could go to next. I get to those states by enacting my policy, so my action here is the pi of s according to my policy, I get to state s’ with this probability and then once I'm in s’, this is the value of my policy going forward. Now when we talked about the finite horizon case, we saw that the horizon could come in here. Here there is no horizon because when I start, I have infinity in front of me and the nature of infinity is such that next year I still have infinite time in front of me and so with infinity, you get this interesting thing where the same value appears on both sides of the equation and it's still V_pi. We're only going to be considering, in this case, stationary policies for that reason because there is no conception over horizon and infinity and so for this reason, we basically get this equation and this equation—it's actually going to be a set of equations in V_pi—and we could solve these to figure out V_pi. So this isn't telling us V_pi, we can't just read it off, but we could solve these equations. These are linear equations so it's not too bad to solve them, you could solve them to get this value of the policy. Now another thing that we define, in part because it'll really help us get the best policy, is the value of an action given that I'm in a state if I make all the best actions after this action. So the idea is I'm going to make this action a and then I'm only going to do the best thing ever after it. And so this was the idea of Q*. So Q* is exactly this value of action a if I start in state s and then I make all the best actions in the future. Now we didn't quite get to this in the last lecture but it's in the notes and it's also extremely analogous to the V equation and you can derive it in much the same way. So here we're saying, “hey, this value of action a and state s, if I make all the best actions in the future, well first, I'm making action a and I'm in state s and so I'm going to get the reward R(s,a) just from this year, from this time step.” Now I'm going to say, “well, what's the value in all the remaining time steps?” Again I'm going to discount that by gamma to start because as soon as I go to the next time step, that's how much the value of my harvest decreases in terms of today's dollars or today's bushels and I'm going to sum up over all the states to get the value going forward and I sum up over all the possible states I could go to, what's the probability of going to each of those states in turn to s’ and then what's the value if I do all the best things in the future? So if Q* is, in fact, making all the best actions in the future, then this will be the value of making all those best actions in the future and just for the same reason that we saw V_pi on both sides of the equation above, we see Q* on both sides of the equation here: once we have taken a time step, we still have infinity in front of us and so, in that sense, we're still in the stationary realm, things still look the same in the time sense, we just might be in a different state and make a different action. Okay so why is this useful? Why is it useful to know the value of an action a in a state s if we make the best action of the future? Because then we can solve for the best actions to make. Now it's worth noting: here this is also a set of equations that, in some sense, we can solve for Q* just as the previous set of equations we solve for V_pi. Here it's just a little bit more difficult because of that max, it's not just a linear set of equations and so we had to come up with this value iteration idea, in particular, the infinite horizon value iteration which would basically solve this by doing an iterative procedure or at least get pretty close to the answer. Okay and so once I have Q*, I can find the best policy. I can say basically, okay, for every state I'm in, I know the value, the best possible value of every action, and so now I can ask myself, “well, what's the best action to take out of all those actions?” So here what we're saying is there's some value for every action, that's the Q*(s, a), we're going to ask which a maximizes that and then the arg max just returns the a that maximizes it, what is the action. And because, again, life has to be stationary when you're in infinite time, there is no horizon that is changing: you're not getting to a point where again your farm is about to get paved over by a parking lot, it's just always going to exist, then we're going to have a stationary policy here. So this is one that is just going to depend on us, there's no horizon dependence or anything like that. Okay so we get the best policy, so let's recap: so our goal here is to maximize the reward. I want to say, “what's my long-term reward?” And I mean that, in the sense, if I add it up in this discounted manner over all the years in the future. And so, in particular, what I would do if I was a farmer And I knew a lot about MDPs is I would say, “hey, I've just gotten this cool new field so I'm going to plug in all the information I know about my MDPs, my states, my actions, my transition model, my reward function, my discount factor, I'm going to solve for the best policy by first figuring out Q* perhaps with infinite horizon value iteration and then once I know the Q* value at every (s, a), I'm gonna plug in, I'm gonna find my pi_Q*.” So before I even set foot on my farm, I figured out my best policy. Now I'm gonna set foot on my farm, and from day one on my farm, I'm gonna follow this policy. So sounds great: I know exactly what I'm going to do, I know exactly how I'm going to farm everything, I'm all set for life. And the observation that we want to make now is: is this realistic that I would set foot on my farm and know all of this off the bat? Well, everything in machine learning is a little bit realistic and a little bit not, but let's try to make it a little bit more realistic today and think about what are the things that I might reasonably know and might reasonably not know. So let's go look at this Markov decision process. It seems plausible, at least in many cases, that I would know the state of my farm, that I could measure whether I had rich or poor soil, that's something that I have measurement tools for. Certainly I will know the actions that I can take: I know whether I can plant or fallow, I know that these are the actions that are available to me. I might be in a situation with other actions but then I would know those actions too, so it seems very plausible that I would know the states of my farm, I would know the actions. The discount factor is my choice: how much is a bushel going to be for me next year versus this year, that's something that I can choose. Okay I'm gonna say two more things and I'm gonna take the question. So let's just briefly note, though, that even if I'm a super expert farmer And I know all about farming (which is, by the way, not true, I am definitely not an expert farmer), but even if I were, it actually seems a little unlikely that I would know this transition function. It might be that this field is different from other fields, so why would I know exactly the weather conditions and the exact probabilities for this field that lead my rich soil to become poor soil? It seems unlikely that I would know this in advance, that I would know these probabilities of going from rich to poor. Maybe also I plant and fallow different from  other farmers and that might have a reasonable change on this transition function. Same thing with the reward function: even if I know rewards in other fields, I might not know the rewards in this field, I might not know how many bushels I'm going to get from rich soil and how many bushels I'm going to get from poor soil and so it's totally reasonable that I might know the states and actions and discount factor that I could apply to this field but I might not know the transition model or the reward function. And so in that case, I can't do this thing where I do all these calculations before I even arrive on my farm and then I try to go for this goal of maximizing reward, like I start off with this best policy from the first day, because I might not know the transition model and reward function are and they come up everywhere in these equations, these equations very much clearly depend on knowing both T and R and so if I don't know them, I'm gonna have to come up with a new plan basically. Okay sorry, what was that question? The question relates to the V_pi and Q*: is the V_pi(s) and the max Q*(s,a) given from past values of previous iterations? Right, so okay. So this is a little bit different than what we did in the finite horizon case. So in the finite horizon case, you could actually calculate each of these in this really straightforward manner, the value of a policy or the value of the action a and state sm taking the best actions of the future. What you would do is you would calculate them for a horizon of zerom and then you would calculate them for a horizon of one, and then you calculate them for horizon two, and you would just have the values. And here it's a little bit trickier because what really happens is that you have this set of equations that they satisfy and now you have to solve for them in those equations and so it's not like you just plug in these equations and you've got an answer because unfortunately the equation for V_pi depends on the value of V_pi, the equation for Q* depends on the value of Q*, and so we have to come up with some way to solve them. Well the first one, it turns out, is a set of linear equations. Like if I had the equation x = 2x, I could actually solve that for x, x in this case would be zero. I could come up with more interesting linear equations but once I have a linear equation, I can solve that and so that's what's going to happen with V. In the case of the Q*, because it's not a linear equation (that max makes it non-linear in Q), we have to use more sophisticated methods to solve and so, in this case, what you're going to have to do is, for instance, something like infinite horizon value iteration and what that is is basically a way to find a Q* or something very close to it that solves these equations. That's great.

Okay but today we're going to go beyond that, we're going to assume that you have the ability to get this V_pi and this Q* if you have these equations, but the problem is that you don't have these equations and the problem is that you don't have T and R and so what can you do? And so, in particular, we're going to have to come up with some different way to behave: we're not going to have just the best policy off the bat and then immediately go forward and make the most from our farm every moment, we're probably going to have to make some mistakes and figure out what to do and then we'll keep going and eventually make some really good choices.

Okay so here, we're in a situation where we're imagining that there is a Markov decision process out in the world that is operating, that is happening, that decides all of these things, we just don't know the details of it. So my farm operates according to a Markov decision process (you could ask whether or not that's realistic but we're going to pretend that it does) but I just don't know the transition function and I just don't know the reward function and so I have to figure out how to proceed. And again, proceed under the assumption that I would like to harvest as much as possible in this long-term sense including these discount factors and things like that.

Okay well let's try out some ideas. I mean I've got a farm, I got to do something with it so let's throw out some ideas. I mean this is the idea of brainstorming, right, let's throw out some ideas even if they're not really good ones and then we'll learn from there. So let me just say: maybe my friend, farmer A—remember farmer A and farmer B from last time—maybe they have stopped by and offered some strategies for me. So farmer A stops by and he offers strategy and he says, “what you should do is you should try a random action, see how that goes, and then do the best thing ever after from there, based on what you know so far.” So let's try it out: so let's say that we follow farmer A's advice, let's see what this might look like. So first, the state of my farm is just the state of my farm when I show up and so maybe I show up and it already had rich soil so I'm pleased with that, that seems like a nice situation. Now I choose what action to make and if I follow strategy A, I'm supposed to choose a random action and maybe I'm gonna make a uniformly random action. So I have two actions, plant or fallow, let's give them equal probability. I'm going to flip a coin to decide which one and so maybe, in this case, it happens that my coin flip tells me to plant. Okay so I planted on my rich soil and now nature is going to tell me what's the state of my soil after a season. So nature runs its course and it happens that now the soil is rich. So I didn't have anything to do with that, that was just nature operating. Hey, I got lucky and it's rich again! That sounds pretty nice and so now I'm going to get some reward. Again, this is nature operating: I just tried to harvest as much as I could and it turns out that I was able to harvest 75 bushels, so this already seems to be a different field than the field that we were investigating last time. I was able to get 75 bushels from this field and so, incidentally, it seems like I've already learned something about this secret MDP that's operating where I don't know the T and I don't know the R. Is there something about the rewards that I have learned? So this is a question for the chat: have I learned anything about this reward function here?

Okay great, a number of people have noticed that if, in fact, my farm is operating according to an MDP (so it's an MDP, I just don't know what it is) and it's got some reward system and some transition function (I don't know what they are going in but I know that they exist), well here I decided to take some rich soil and plant on it. So I took the state “rich” and I took the action “plant” and I have now observed that I was able to get a reward of 75 bushels if, in fact, this is an MDP that means that the reward for taking a state rich and planting must be 75.

Because it is deterministic here, there is only one reward for that state and that action, this must have been it so I have already learned something about this MDP just by going through one round of planting things.

Okay and so that seems pretty cool. I noticed that, in fact, I got some non-negative, non-zero reward, like I actually got 75 bushels so planting seems like a good idea and so I said, after one round of trying things out, I was going to do whatever seemed the best. This seems best so I'm always going to plant going forward. This is what farmer A told me to do and I'm following farmer A's advice. Was that good advice? Is this a good strategy or, just to be a little bit more concrete, could anything be improved about this strategy? Is there anything that, perhaps, is sub-optimal about this strategy? This is another question for the chat.

Okay great. So basically I think the theme that is emerging from people's answers is that we don't know anything about this farm basically. I mean we know a little bit—we know the reward for rich soil and planting is 75—but what if it turns out, in this farm and for this crop, that poor soil is actually really great for planting and I would get a thousand bushels if I planted in poor soil. Maybe I should try these things out. Even if it's just like the farm that we saw from last week, if I plant all the time I'm going to end up with poor soil and a low reward and it's going to be a sub-optimal reward. And so it seems like a big issue with this strategy is that we haven't really explored this farm at all, we haven't even gotten a sense of what are the possible things that might result, what are the possible rewards that are out there, we certainly haven't filled in this reward diagram and so we could ask ourselves, “hey, is there some way that I could get more information about this farm before making these big decisions, like, what am I going to do with the rest of my time?” Okay so that seems like a good idea. So okay, farmer B now walks by my farm because I like hanging out with farmer A and farmer B a lot and they're very welcome to come by and so farmer B says, “hey, here's a great way that you could learn about your farm. Just always try actions uniformly at random, there's no reason you have to stop after one time. Just keep trying these uniform random actions.” And so I say, “hey farmer B, that seems like a good idea, let's try that out, let's see what this might look like” and so I'm gonna do that. So again, I start off with a particular state, maybe some rich soil. I decide to plant in it but then nature decides what state I see next. In this case, perhaps, it's rich. Nature decides what reward I get. In this case, it's 75 bushels, and we just said that once I see that, once I was in the state rich and I decided to plant and I got 75 bushels, that means I've learned something about the reward structure of this farm: I know that if I'm in a state rich and I plant, I'm going to get a reward of 75, again on the assumption that this farm operates according to an MDP. Okay so now, I decide what to do in this new state. Well again, the way that I'm deciding here is I'm trying uniformly at random and so again I flip a coin, I say “am I going to plant or am I going to let it lie fallow?” In this case, it happens that I plant. Nature decides what the state of the soil is. In this case, perhaps, it happens to be poor. Nature decides what reward I get. Can you tell me what nature is going to decide for this reward? This is a question for the chat: do you happen to know in advance what this reward is going to be?

There's some split thoughts on this one but let's double check. So remember the first time I was in rich soil and I planted and I got a word of 75 bushels. This time I'm in rich soil and I planted. If this is, in fact, an MDP, if nature operates according to an MDP, then this has to be the same reward and so this is going to have to be 75 bushels again. It doesn't matter the state I landed in, it just matters the state I started in, the action that I made, at least for an MDP. Now you might disagree whether nature really operates according to an MDP but we're assuming for the moment that it does and if it does, then this is going to be the same reward.

Okay so we can't update our reward structure because we basically just got the information we already had, so here I'm going to flip a coin. Again, now a fun fact is that people tend to think that if I randomly flip coins, that everything looks like it goes back and forth between the options. In real life, when you flip coins, you get quite a lot of runs. In this case, it turns out I got another plant even though I was uniformly flipping this coin. Nature decides what the state is going to be. In this case, it happens to end up being poor and let's say I observed that I get two bushels because this was really a new situation. Now I started with poor soil, I planted in that poor soil and I got some reward. In this case, it happens to be two bushels and so I can update my reward structure and say hey, in fact, I observed that there were two bushels for a reward when it's poor soil and I plant. Okay and, again, the question is: is this a good strategy? Is there anything that could be improved about this strategy? I mean it seems like I've been harvesting a lot of great bushels, like I've gotten 75 bushels twice already, should I just keep doing this strategy and will I get the best possible rewards? This is a question, again, for the chat and , again, in particular, you should think about are there things that you could improve about this strategy?

Okay while you're still answering, I'm actually going to respond to one point. Somebody suggested that this is better than strategy A. It's not obvious whether that's true, actually. So it's totally possible in strategy A, we didn't know anything at that point: maybe if I planted in rich soil and maybe I planted in poor soil, then maybe I'd always get a reward of 75. Based on what we knew at the beginning of strategy A, that is possible and there are worlds in which that is possible. I think what you're getting at, though, is that there are also worlds in which strategy A is very bad and strategy B will yield more output than strategy A but the problem is that we don't know which one we're in in advance and that's the real issue that we're dealing with.

Okay so I think what some folks are observing is that in strategy B, I'm learning a lot, but the problem is that I'm not necessarily exploiting that learning, I'm not necessarily using that learning. So for instance, in strategy B, suppose I eventually learn that the best possible thing I could do is I could plant whenever there's rich soil and fallow whenever there's poor soil. Like if I knew the whole MDP, that would be the optimal thing to do. Now I'm always going to try actions uniformly random, though, in strategy B and so if I have rich soil, half the time I'm going to choose to fallow it, I'm going to choose to not plant anything and that seems like a lost chance to make a lot of harvest, that seems like something that I could have improved on if I knew all of the information about this MDP which, if I do strategy B long enough, I'm pretty much going to know everything about the MDP and that's what we're going to get at the moment, then I could probably do better by exploiting that information, by using that information. So it's worth thinking about this to yourself: why and how these strategies could be made better. But I think the key that's emerging here is that strategy B is really focused on exploring: it's going to tell us a lot about the MDP. We don't know anything going in, it's going to give us all the information about the MDP, but the problem is that eventually we're going to have tons of information and we're not going to be using it for anything, there are better choices that we might be able to make with that information but we're not going to make those better choices. So even if I have this observation that when I have rich soil and I plant it I make a ton of harvest, I might just not plant in rich soil or sometimes I might have poor soil and I might just keep planting it for a whole bunch in a row even though I know that's not a good idea. These are all totally possible things under strategy B. Conversely, strategy A we saw was focused on explaining it, just did it too soon. We didn't really know anything and it was like “yeah, let's go for what we think is best even though it's based on really imperfect partial information.”

And so, of course, what we'd like to do is we'd like to really trade off these ideas, we'd like to have a little bit of both, we'd like to have the best parts of both really. Okay, so we're going to think about how we can do that. Okay so this is a super common concept in computer science and machine learning, this exploration exploitation trade-off and it very much is a trade-off because each time, each harvest season, we have to make an action and we can either choose to explore or we can choose to exploit. So at a very high level, abstracted away from this problem, what we mean by exploration is that when we explore, we're trying to understand the world in some sense or we're doing different things in the world and that might reveal something about the world to us. In this particular case, the things that we're learning about the things we don't know are T and R, but in general, exploration could refer to something much more general, just trying to understand the world.

In exploitation, we take what we know, which will always, in general, be something imperfect. It's not necessarily going to be a perfect understanding of the world but we take that, we try to make the actions that get the best reward and so farmer A was going straight to exploitation without doing exploration, farmer B was doing all this awesome exploration but never taking advantage of it to make all of the best actions and so we'd like to have some trade-off between them. Now this is just a huge area that you could spend basically like a whole class, certainly a whole lecture on, saying how could we do this trade-off and so we're gonna take a much simpler approach and just say, “let's randomly decide which one to do.” So I just want to emphasize so much that this is not the only way to make this trade-off but we're gonna make a choice that does at least encapsulate that the trade-off exists and make some kind of decision on it and this is known as an epsilon-greedy strategy. So the epsilon-greedy strategy says, “okay, it's coming time to make an action. It's the next harvesting and planting season and so I'm going to make my action in the following way: I'm going to choose a random variable, a uniform random variable or, well, I guess a way to think about this is a random variable that has probability 1 minus epsilon of being one value and probability epsilon of being another value. So with probably 1 minus epsilon, I will choose to exploit, and with probability epsilon, I will choose to explore.” And the reason that we have this epsilon here, epsilon often denotes a variable that tends to be a little bit smaller, so the assumption here is that most of the time I'll probably be trying to exploit and some of the time I'll be exploring but epsilon ultimately is something that you get to choose, it's a hyper parameter of what we're doing. Okay so this sounds very good but the reality is we have to say what this means, what does it mean to explore and what does it mean to exploit? And so let's try to get concrete here. Now again, these choices do not have to be the choices that you make, I think that even right now you could come up with other ways to explore than what I'm about to say but one way that we could explore is we could choose an action uniformly at random. Certainly, if all of our transitions are sufficiently rich, then we'll eventually get to every state that we could get to and so this will help us explore the space that we're in, so this is one option, certainly not the only option, for exploration.

Now we have to say: what does it mean to exploit? And in some sense, that's going to be basically the rest of the lecture: how do we exploit the imperfect information that we have at any finite time step? We saw in the beginning we have no information, we hope that by exploring and also by exploiting that we learn something from our MDP or, in general, the world as we go but now we have to take that information and do something with it, we have to come up with an action from it and so that's what we're going to be talking about next.

Okay how can we exploit? Okay well let's just think for a moment about how we would do this if we had perfect information. So if we knew Q* (remember we said Q* was the value of taking action a at state s if we make all the best actions after this step) then we know exactly how to exploit. We actually did this in the previous chapter of reading, this is what we set up previously in the case, basically, where we know the whole MDP, we know how to exploit and that is we maximize Q*, we find the action that maximizes this, basically the value, what is the reward that we're going to get. So in some sense, we did this work already: we said if we know everything, we know exactly how to get the most reward from it and the whole issue, the whole reason that we're having a discussion about how to exploit is that we don't know Q*, and there's different ways of not knowing. So if you knew T and R, if you had the values of T and R, you could solve for Q*, it just might take a little while, it might take a long time, you might have to run value iteration but like somehow it's defined by T and R. Here, we're saying we don't even have T and R, we don't have to have the information, it's not just about running a computer program, we just don't have the information to even solve for this and so we have to ask what can we do in that case.

Okay so here's the idea that we're going to explore: can we guess or estimate, can we make an educated guess about something that hopefully is near Q* based on the information we have so far? And that's exactly what we're gonna do going forward. So once we have some kind of guess or estimate, let's call it Q for Q*, some notion of “I think this might be the value of taking action a at state s and then making all the best actions after this step.” Then I could choose my action based on that Q, I could say, “okay for this, for this guess q, what's my best action?” And that'll be my policy pi_Q.

Okay and so this reduces our question to: how do we get a good guess or estimate Q for Q*? And so we're going to explore different ways to do that basically, we're going to say: is there some way that we could get a decent guess for Q*? And then that will let us do something like exploit as well as we can and we're just not going to be able to exploit really well in the beginning because we don't have a lot of information and then the hope is that by learning, we're able to do it better and better.

Okay and so, in particular, how are we going to learn Q*? How do we get a sense of the value of taking an action at state s and making all the best actions? Well we're gonna see some data. Now, what do I mean by data? I mean the observations that we make in each step. So you can think of those observations as being the state that we started at, the action that we took, the reward that we got and the state that we ended up at. Now this is a little bit redundant, we're going to see, obviously, some of these states across the steps but you can think of it this way: I mean, basically, when we see a reward at a particular state in action, that obviously tells us something about the R function. I mean that basically completely tells us about the R functions. That's going to be a relatively easy thing to estimate and if we see a state s and the state that we go to, then that's going to tell us something about the transition function and so our first idea here for estimating Q is how about we estimate T and R and then plug that in to get an idea of what is Q.

Okay so we're asking: can we learn Q because that'll help us do this exploitation?

And so our first idea is that if we knew the transition function T and we knew the reward R, we could directly solve for Q*. And so if we estimate the transition function T, we estimate the reward R, then we can get a Q that is hopefully near our Q*. Okay so what might this look like? So here is a proposal, a strategy for how to proceed. So we had our strategy from farmer A, we had our strategy from farmer B, now let's try out this new strategy. We're going to initialize with the state of our system, this is just a notational thing. I mean the farm is what it is when we get it, let's call that s_0, and we're going to say: what is that state now? What we're going to be doing is we're going to be estimating the transition model T and the reward function R and so, before we've seen any data, before we've tried planting our soil or fallowing it or taking any actions whatsoever, we have to ask ourselves: what could we save for T and R? Basically nothing. We don't know anything about T and R at this point, so we're kind of just initializing these values to something that's an initial value and so here we're gonna say, just as an initial guess before we know anything, that we're gonna say all the rewards are zero at the very least. That tells us we're not erroneously exploiting something by saying that it has a big reward when it doesn't. Let's just pretend all the rewards have zero and then we will be corrected as we go if there are different rewards for T. Remember T is the probability of starting at state s, taking action a and going to state s’ so there are S possible values or there's this script S which is the set of possible states and the absolute value of script S here means the number of possible states and so what we're saying is, “let's just assume, because we don't know anything about the states, that they all have equal probability to begin with. We're going to update this as we get information, but that'll be our guess before we've seen anything okay.” Now once I have any T and R, I can get a Q from it using, for instance, infinite horizon value iteration. So in this case, because all the R's are zero, the Q is just going to be zero everywhere too because, remember, the Q represents a value that we get from rewards and so it should be that if all the rewards are zero then you never get any reward.

Okay so now we're going to go forward with our farm, we're going to go from year one to year two to year three and so on and we're going to aim for the following strategy.

The first thing we have to do in any given year is select an action and that action will depend on our current state, so that's s^(t) and we just said we're going to use Q to figure out how to exploit what we know so far. So what would we do here? Well this is where something like epsilon-greedy comes in, so it tells us how to select an action. So in epsilon greedy, we say with probability 1 minus epsilon we exploit. So we use Q, we choose the best action according to Q. And with probability epsilon we explore, we just choose uniformly at random. So epsilon-greedy gives us, essentially, an algorithm for how to choose that next action. Now, once I've chosen an action, nature takes over. So what's happening in execute is I'm doing my action—for instance, I plant my soil–and then nature is going to tell me what happens next. So nature's going to take the action that I made together with its current state. So really this depends on the state as well, the current state, and then it's going to say, “okay, given your current state and the action you made, what was the output state?” So for instance, if I were planting in rich soil, then nature's going to run its MDP that I don't know about and it's going to decide am I in rich soil or poor soil now and it's also going to run its reward function and say what is my reward.

Now this involves the MDP with T and R that I don't know but I'm going to try to learn T and R. Okay learning R is relatively straightforward: as soon as I see a reward, I know that that's the reward that I get for state s and action a, just by the definition of the MDP that must have been the reward, and so I'm able to immediately fill this in. This is a great estimate: it's exactly what the reward function is at that point.

T is trickier.

So for t, we're trying to say if I'm in a state s and I take action a, what's my probability of going to s’? And I want to do this for every state s and every action a and every possible s’. Okay well let's think for a moment. What are we trying to do here? We're trying to estimate the probability of something happening. So let's think for a moment of another example. Suppose that you were trying to estimate the probability that you eat dessert after your dinner. How might you estimate that probability? Well one idea is you could record, every now and then, when you eat dinner what happens. So you put a little mark down every time you're recording and then separately record “did I eat dessert.” So in some sense, you're gonna count a number of times that you ate dinner and then you're going to count a number of times that you ate dinner and you had dessert and you're going to divide, you're going to say, “what's the number of times I eat dinner with dessert divided by the total number of times I eat dinner?” That would be a reasonable estimate for the probability that you eat dessert with dinner and we're going to do the exact same thing here. So we're going to say: what's the probability that I start in state s, I make action a and I go to state s’? Well in order to do that, I can count the total number of times I was in state s and I made action a. So that's all this sum is doing: it's putting down a tally of one every time I was in state s and made action a up to the present point (so once I get to some t, I've gone through time period one, time period two, up to time period t) and then I can say what's the number of times that I was in state s and I made action a and I ended up in state s’?

Okay so this is a perfectly reasonable way to estimate the probability of being in state s making action a and going to state s' with a slight exception and that exception is the following: let's say I come through this and t is one, then there's a good chance (in fact, it's totally unavoidable for any non-trivial state space and action) that they're going to be some cases where I have not yet observed being in that state and taking that action and so this denominator would be zero and that's problematic, I don't want to divide by zero.

Another issue that can arise, even if the denominator isn't zero, is that I might not have yet transitioned to s' in which case the numerator is zero but I don't want to say it's impossible that I could have transitioned to s'. It might be unlikely but I don't want to say it's impossible.

And so a way that I can deal with this is what's known as a Laplace correction.

So I add one to the numerator and the number of states to the denominator and the way to think about this is to think about what happens at the extremes of time. So if my t is zero, I haven't seen anything yet, then there's no summations and I just say that I think the probability of transitioning to any state is equally likely. In fact, that was exactly the starting states that were the starting guesses for these probabilities that we used.

Now as I get more and more and more and more data, in the sense of I observe more and more things on my farm, I go through more steps on my farm, I go through more seasons, these counts are going to be really big, they're going to get larger and larger and larger and they're going to dominate the 1 and the number of states are going to be relatively small and so this is basically going to go to, asymptotically as we get more and more data, go to the value as though there wasn't even that one and that |S| in there.

And so, in some sense, this little class correction is balancing between that case where we start off and we don't really have much information or any information and the ending case where if we've actually observed a ton of data, then we really want it to be basically the fraction of time as we went to state s’. 

Now I've been referring to data a few times and so I just want to make it explicit, again, what I mean here. We're imagining that the data at step t is these observations we're making, the state that we start at, the action we take, the reward we get, the state that we end up at. These are the observations that we're making and it's what we're learning from, it's what tells us about T, that's what lets us make this estimate for T, that's what lets us make this estimate for R. Now I said that we do this for any s, a, and s'. It's worth thinking to yourself: do we really have to update this for every state and every action and every s', every time we update T? I think you will find that if you think about this for a moment, that there's only some that will actually change and there's only some of these that you'll have to update. So you might, if you have some free time later, think about which ones those will be.

Okay so finally, once we have our estimate of R hat and T hat, what we do is we say, “okay, we can get an estimate of Q from this, an estimate of the value of making any action” and then we can use that to choose the best action when we come to the next round of this loop with our exploit strategy. So the whole idea here is, again, we're going to take epsilon-greedy,  epsilon-greedy is what's going to let us trade off between exploring and exploiting. And then for exploiting, what we do is we estimate these transition probabilities, we estimate the reward, and then we use that to decide what we think is the best action right now.

Okay so let's see just a little bit of an example of what this might look like. So maybe I choose epsilon equals 0.3, I'm going to explore three tenths of the time, and I choose gamma equals 0.99 because that's how much a harvest next year is worth to me now. And so first, I'm going to initialize and so maybe it happens that my farm has rich soil when I first encounter it.

Now I have to initialize all my T hats and my R and so I'm going to have my T hats all be one over the number of states and so, in this case, each one of these, there's two states that we could go to, and so it'll all be 0.5.

And likewise, I'll initialize all my R's to be zero. Now I have to select my action and if I'm going to use epsilon-greedy, what I'm going to do first is I'm going to have a random draw that, with probability epsilon, explores and with probability 1 minus epsilon exploits and so maybe this time I happen to choose explore.

And so if I explore, then with probability one over the number of actions I choose each action, and so maybe this time I randomly chose to let this field lie fallow.

So in this case, the reward happens to be zero, I don't harvest anything if I let the field lie fallow and so that's just gonna get updated, it's just gonna be zero. So I execute my action, I get my reward, turns out it happens to be zero here, I get to the next state (again this is something that nature tells me what state I get to), and this one happens to be poor. Okay again, I update my reward. In this case, it's trivial to update because there was no reward. I update my estimate of T hat. So in this case, I started for a rich state and let it lie fallow and so I'm going to update the corresponding probabilities.

I learned a little bit of something about those probabilities but it's not a lot because I've only seen one random draw at this point. My hope is that over time I'll see more and maybe I'll get some better estimates.

Okay now I run my infinite horizon value iteration, I get my Q and so I know the best thing so far to do next time, although nothing's really changed with Q because all the rewards are still zero.

So I can go through this again. Maybe this time in my epsilon-greedy random draw choose to exploit, but here's the annoying thing about exploit: when Q is all zero, there's nothing to exploit, like exploit is pretty bad at this point because I don't know anything, basically every action is the same and so maybe it just so happens that fallow comes first and I have to have a tiebreaker and I just choose what's first. Okay well in that case, I'm going to take this person, I'm going to let it lie fallow, nature tells me what's the next state. In this case, it's poor, so I'm going to update my R's and update my T's. Nothing really got updated with R, I did update T a little bit. Maybe again I happen to exploit. In this case, again, there's no information: my exploit function is really bad at this point because I haven't learned anything and so, at this point, it might just tell me to let my field lie fallow, so I do that again. Maybe I happen to choose, this time, to exploit. Again there's no information at this point, so maybe I choose to let the field lie fallow even though it's a rich field. Sounds like it might be a good idea but we don't know that yet and so we'll do this and so we can keep doing this, but you'll just notice, in the beginning, we're not getting too much from the exploit function because there's nothing to exploit, there's no information that we have. Here we finally explored and we finally tried planting a rich soil and so we might get some good information from that, but it took a while. So now let's say we go far into the future when my ancestors are farming or my descendants, I guess I should say my future descendants 2000 years from now, are still farming this field and at this point, we have so much more information, we've probably seen all the rewards, we have a lot of information about the T function, we really have a good sense of what are the transitions, and so now the exploit function is fantastic: it tells us, really, what is a good thing to do to get some reward from this harvest and, in this case, it tells us to plant. If I exploit when it's poor, it will still learn. If it has a good information about T and about R, it'll still learn this idea of the delayed reward and if I let things lie fallow this time, then I can get more reward next time and, in this case now, when I'm in this distant future where I've learned so much about what's going on, the explore function's a little bit annoying. I have this chance of exploring and when I explore there's a chance that I might do something that's sub optimal, that I might let my field like fallow even though it's rich soil and I know that it would get so much reward from rich soil and so there's this interesting trade-off that, in the beginning, you're not doing too much by exploiting, maybe there are better ways to handle that, maybe we want to explore more in the beginning and later on you're not doing too much by exploring if you really know everything that's going on.

And so it's interesting to think about what strategies you might use beyond epsilon-greedy but here we're using epsilon-greedy for the moment.

Okay so this was an option. One option was to learn T and to learn R and then put them into Q and use that to proceed. It turns out another option is to estimate Q directly and what I mean by that is to basically not estimate T and R along the way, to see if we could just come up with this function because if we had any estimate for Q and it was a good one, then we can use that to get the best actions from there.

Okay so let's see if we could do this. We're gonna have to put a little thought in. So this is what infinite horizon value iteration looked like. So here, we're just pointing out… Well, to be more precise, this is the function that Q* has to satisfy, this is just the iterative procedure that Q* has to satisfy, and from that we got infinite horizon value iteration by saying what we'll do is we'll start off by initializing Q_old to zero and then we'll just iterate through and get Q_new and then make it Q_old and then just keep doing that until we get something that's very close to Q*. So if we knew T and R, we could do this okay. And the issue, of course, as we said is that we don't know T and R, but as we noticed just now, we can pick up R along the way. As we try out different soil types and different actions, eventually we're going to pick up what R is. We know what gamma is and so somehow the issue is, perhaps, this last bit which we can see is something called the expected value of a discrete random variable. So what I mean by that is it takes the following form: it's a sum over all the states of the probability of the state times the value of that state.

Okay so this is something that comes up in a lot of different areas not just here in this reinforcement learning lecture. Another area might be something like insurance. So an insurance company really wants to know what they expect to pay out to you in order to tell you what's the price of their insurance policy.

So unfortunately fires are very much in the news right now. You might be interested in fire insurance, there's been a lot of fires happening, certainly in the US and in many of the Western states, and so you might be interested in saying, “oh, I want to get fire insurance for my house” and so then the insurance company is going to have to ask, “well, how much do I expect to pay out for you for this fire insurance?” And so they might use a formula like the one that we have above. So we say that we're going to look at the two states: fire and no fire. In the fire state, we have the probability of a fire happening. Hopefully it's very low, but it could still happen. Maybe it's a one in ten thousand probability happening across all of the different people who might have a fire. If a fire happens, then they're going to have to pay to replace your house and maybe the average insurance payout or the typical insurance payout we will say is 100,000. Now most of the time, a fire doesn't happen so the probability of a fire not happening, let's say, is 1 - 1 / 10,000, so basically the rest of the probability. And in that case, they're not gonna pay out anything, they're gonna pay out $0.

And so if all of this is what might happen in a given year, like probability of a fire in a given year is 1 in 10,000, they'll pay out a 100,000 if that fire happens, then the insurance company could say that their expected value or their promised insurance payout is $10 because that's just running this formula that we saw here, that's what they expect to pay out to you and so in order for it to be worth it to them to set up this insurance payout, they're going to have to ask you to pay more than $10 in a given year. Now that's a lot better than a 100,000, so you're probably interested in doing this insurance payout, but it's worth noting that there is still some value here, there's still some non-zero value.

Now just as in our situation where we don't know T, we don't know the transition probabilities, very realistically this insurance company might not go in knowing the probability of there being a fire for a particular house and there not being a fire and so they might want to estimate what's the expected value of their payout. Okay so let's think about how we might do this estimate and then we'll apply it to the case that we have.

Okay so this is, just again, the expected value of a discrete random variable: we have our example, the expected value of an insurance payout, and so how could they possibly estimate how much they expect to pay out? Well if they're an insurance company that operated last year and they had a ton of people that were being insured by them last year, they could ask, “well, what happened last year? How many people did they insure and what proportion of those people got a payout?” That could tell you what's the probability, an estimate of the probability of the payout, but they could also ask how much did they pay out and so then they would know what's their average payout. And so, in this case, they might actually estimate the expected value by saying, “okay, again, what did they pay out to every single person? How much money did they pay out and how many people were there overall?” So this could be their estimate of the expected value for any particular person.

Now there was this chapter that we never had a live lecture for. Remember back in chapter seven, we were talking about various concepts and one of them I want to bring up again now. So, in some sense, it's review: you read about this in chapter seven, but in some sense it's new because we didn't do it in a live lecture, but this is the concept of a running average. So we can take this expected value that we have and we can write it in a different way, or this estimate of the expected value. So, in particular, we can write it as a running average with a particular choice of the weights for the average. So remember the idea of a running average was that we started off with an initial value of zero, it's our estimate before we've seen anything, and then we update our estimate by saying what's the estimate we have so far, multiply by one minus this alpha weight, so we just down weight the things we've seen in the past, and add the new observations. So every time we have an observation, we can make an update to our estimate and then we'll get our our new estimate (E tilde)^(t) and so something that, if you didn't do it in chapter seven, this was one of the questions that was in the reading, the study question on your own, but it's worth doing if you haven't done it or refreshing your mind if you have, is to show that if you use this running average (so every time you get a new value observation, you update your e tilde and you do this t times), that you get the same thing as the typical average, the one over t, sum of the values of t.

Okay so this is one way that you could estimate this expected value, here's another way, here's a different running average, we could use a different running average. We could instead use a running average where the alpha^(t) is just a constant: it's just equal to alpha.

So here, what we're going to do is we're again start with, now e hat, let's call it equal to zero.

We're gonna take, iteratively—we're gonna do this iterative procedure where we take our estimate so far—we scale it down, scale down everything we've seen in the past and then we update with the new information that we have. And the big difference between this line and the line above is that our alpha is just always the same in every one of these updates, we're not changing it with every update.

We can ask ourselves if the running average, where we choose the alpha^(t)’s to be 1 over t is equal to this estimate of the expected value, is there similarly some choice if we make this running average with constant alpha, is there similarly some way we can express our estimate as some sum across the values?

In fact this is true, so you can check that the following formula holds. In fact, let's do that super briefly right now.

Basically what we want to do is we want to check, well, one: that (e hat)^(0) is equal to 0, but when t equals zero there's nothing in the sum, there is no sum, and so this is just equal to zero and then we want to check that the recursion holds.

So for the recursion, let's first write the right hand side of the recursion: 1 minus alpha (e hat)^(t) minus 1 plus alpha times the value. Now what we can do is we can sub in our proposed formula for (e hat)^(t). 

So all we're doing is we're sticking that in but we're applying it at t = t - 1 so we get in (1 - alpha)^(t - 1 - i) and then with a little algebra, we can see that the first part is just going to be the first t - 1 summons and the last part is going to be the final one and so this satisfies this recursion, this is a different way to write this different running average. And so now we can ask ourselves: well< what's different between this formula and the typical average, the usual empirical average that we would use? Well what's different is that this latter formula gives bigger weights to the more recently observed value, so let's just double check that for a moment. If t is just one, we're just going to get alpha times this value, so we only have this one thing, alpha, times the value that we have. Oh sorry, let's think about if t is very large but if we take i = 1, then what we're going to do is we're going to have a lot of these one minus alpha things out front because we're gonna have t (very large) minus i just equal to one. If t is 1,000, then 1,000 - 1 is 999 so we're getting a lot of these terms that are all between zero and one. When we multiply them together it's gonna get very small whereas if t is 1,000 and i = 1,000, that whole 1 - alpha part goes away and we just have alpha times the value and so we're giving a much higher weight to the values that occur towards the end, the ones that we've just seen, maybe the very recent people who got an insurance payout, we're giving very little weight to the ones in the beginning. Now what are the benefits of using one versus the other? Well if you use the estimate up top, the e tilde one, this has a lot of nice theoretical properties. If you are actually drawing according to some distribution, which is the probability of s' equals s' for each of these different outputs, the fire and no fire, it's always the same, then this is going to go to the exact expected value as you get more and more data. But, it seems plausible in this fire/no fire case that actually fires are becoming more and more probable over time, at least in certain areas, and so in those cases, you might want to weight the more recent values higher because they're closer to the distribution that's happening now and that distribution changed over time and so one reason that you might prefer this alpha weighting, instead of the 1 - alpha^(t) weighting is if things are changing over time and you trust the more recent ones more.

Okay so this is just a review of running averages, this 1 - alpha and alpha running average and we're going to apply it now to our Q estimation case.

Okay so let's go back to where we were. We said recall infinite horizon value iteration, this is something from last week's reading. Now another way that we could write this expected value is we could take it outside the R. So R has no s' dependence, gamma has no s' dependence, the T's all add up to one across the s primes, and so we can just take this initial typical infinite horizon value iteration step and we can just write it slightly differently by putting that sum over the T's up front. So we haven't really done anything here, we've just written it differently but you'll notice this is an expected value too: it's a sum across the states of some probabilities times some values and now the values are the R(s, a) + gamma * max Q_old.

Okay so here's the proposal: every time we observe a state, an action, and a place that we go (so we make an action and nature transitions us to some s'), we're going to update our estimate of Q by using exactly this running average with alphas that we saw from before. So in particular, we're going to take our old estimate of Q, we're going to scale it by 1 minus alpha, we're going to take our new observation of the value that we just got for this new s' and we're going to scale it by alpha and we'll add them together. So this is just exactly this alpha running average we just talked about.

Now something that's interesting about this is, in theory, what we would do if we wanted to do infinite horizon value iteration is we would update this. Every time we got a new s', we would eventually get a pretty good estimate of Q_new, but now we still have to do infinite horizon value iteration, so we still have to put that Q_new in as Q_old and then make the update and keep going and so an observation that we could make is, well, maybe we don't have to always put in Q_old here, we could just immediately use the new Q that we've just gotten, the new, better estimate of Q  and so a different proposal is to do exactly that. So these proposals are exactly the same, the only difference is that instead of using this Q_old, we just take whatever is our latest estimate of Q.

And so this is what we're going to do to update Q, you can think of it, even if all that went kind of fast and you just want to get a high level idea of what we've done, you’ve taken the ideas of value iteration, the dynamic programming ideas from that, and we've taken the idea of a running average, to estimate what's going on with this random variable, what's going on with this random T transition, and we've combined the ideas from both worlds to make an update from our Q estimate that we have so far to a new Q estimate and so let's put this all together to learn Q. So the question was: can we learn Q? In some sense, we already answered that. Yes, we could do that by learning T and R and then putting that in to our value iteration algorithm, but let's try it even more directly and just learn Q directly using this kind of update.

So yes we can learn Q and, in fact, we're going to call this algorithm Q-learning to indicate that it is learning Q.

Okay so it's going to take various inputs. Again, we're going to assume we know the states, we know the actions.

We're interested in whatever state we start in, because now we're really saying, for the farm that I have, what is the set of actions that I'm going to take.

We know the discount factor, we're assuming we do, and we're assuming now we have this learning rate alpha that we've chosen. So basically, we assume we have these parts of an MDP except for T and R, we assume we have our starting state, and we assume we have the learning rate alpha.

Okay so the first thing that we're going to do is we're going to start off, just as when we were doing this for T and R we started off with some guess for T and R and then we updated over time, now we're starting off with some guess for Q.t This is basically like assuming all the R's are zero, we're going to assume all the Qs are zero and then we're going to update it over time.

Okay so we initialize the state that we're in. So again, this is just saying, “hey, we start off in the starting state, and then we're going to say: ad infinitem, into the future, we're going to follow our strategy.

Okay the first thing we do is we select our action. Just like before, we're gonna use epsilon greedy. We could do this some other way, but here we're using this exploitation/exploration trade-off with epsilon greedy. So in particular, given the Q we have so far, which encapsulates what do we know about the problem so far (it's not very much in the beginning, but over time there'll be more) and the state we're in right now, which is s, we make an action. So if it's epsilon-greedy, we say with probability 1 - epsilon, we're going to choose to exploit using Q and with probably epsilon, we're going to just choose a random action without using Q. So technically speaking, this really also requires specifying epsilon in the beginning.

Okay now we've selected our action, we make our action, nature now does its work.

We execute our action in the world that we're in, nature sees what states it's in, nature sees our action, and nature returns what is the reward we get and what is the s' that we get, what is the new state that we're in.

And now we make our updates, so we can make better actions in the future. So before what we did is was we updated T and we updated R. Now what we're going to do is we're going to update Q directly. So in particular, we have our Q from last time, our Q(s, a). We observe R, so we're putting that directly in, the reward that we observe, R. We know gamma, we can maximize Q based on what we knew about it so far. And so we can put this whole update in and then update Q. So the right hand side is using our old Q estimate and the left-hand side is our new Qs.

And now we just say, “hey our new state or our state that we're in is the state that we have transitioned to.”

Okay so just one alternative perspective on this update, there's just this one update here, it's this Q update. We can think of it, instead, as taking our existing… In order to get our new Q, we take our existing Q and then we change it a little bit.

So in particular, we can think of this as being like a one step estimate of how Q changes. This is what you would calculate if you were running infinite horizon value iteration for one step forward. You could also think of it as the expectation estimation that you would get from just one data point and then we're going to ask how that compares to the existing Q.

If our existing Q is smaller than we would have expected, based on this one step, so if this whole thing is negative, then this whole thing will be positive and we'll add a little bit to Q. So if Q is smaller than we expected, we add a little bit. Conversely, if Q is larger than we expected, or this update that we get from this update, then this whole thing will be positive and this whole thing will be negative and we'll move Q down a little bit. And so it's doing a minor adjustment based on this step ahead versus the Q that we have because, somehow, in steady state, these things should just be equal.

Okay, so this is just the exact same update, it's just written slightly differently. You should definitely make sure that you agree that these are two ways of writing the same thing, especially when you see the second version, this is sometimes called temporal difference learning and the idea being that you compare the Q from your time step right now with this one step ahead look ahead. So it's like one step in time and you compare that difference and you learn based on that difference. So that's where that name comes from. So just a couple of points about this: one, unlike our previous algorithm which is also a totally legitimate algorithm, this algorithm never learns or stores T or R.
Even R, which is in some sense not too hard to store, it doesn't: it just looks at the reward you got this time and updates based on that. It definitely doesn't store T: T doesn't appear anywhere. It completely bypasses that. It doesn't instantiate those things and so if you really did care about finding out what was the reward function or, even more specifically, what was the transition function, you don't get that along the way here. If you don't care about that, then that's nice, because these are really easy simple steps. Something that's a big difference between this and the previous algorithm is you don't have to run infinite horizon value iteration which could actually take a really long time to run. Here, you're just making this update directly and quickly to the Q and so you don't have this inner step of running this possibly annoying thing to run: all we're getting here is Q. A quick question. Yes, quick question, probably to summarize, but could you just explain again what we're trying to find in Q and an explanation of what Q is? Yeah so remember Q* is this function that if we had it, we could extract what are the best actions to take. And so if you get nothing else, Q is just an approximation to Q*. More specifically, Q* and this is from the last lecture, the last reading, tells you what is the value of taking a particular action in a particular state and then doing the best thing ever after. So you can think of it as being the value of an action in the current state under the assumption that you do the best thing after that action. And that's why it lets you choose what's the best action, because you choose the one that has the best value and so everything that we're doing in this lecture, in some sense, and in this reading, is saying, “hey, could we approximate that?” Because unfortunately we don't have access to Q*. If we knew the transition functions and the reward function in what we were doing, then we would know Q*, but because we don't know the transition and the reward function, we don't and so we have to somehow approximate it. And so the idea here is to somehow say, “hey, can we come up with an approximation?” It's going to be a bad approximation in the beginning because in order to get a good approximation, you have to observe something about the world that we're interacting with and so what will happen is that you'll have a really bad approximation in the beginning and so when you exploit you're not going to do very well. But over time, by exploring and by some of the exploitation moves, you'll get more information about the world, you'll get a better estimate of Q* (that's Q), and then you're going to use that to make better exploit moves, to make better moves that get more reward. And so that's totally the main idea here is, basically, how can we learn more about the world and how can we encapsulate that? How can we summarize that in a way that we can take advantage of to make good actions?

I'll just leave you with one last thought: you'll notice that there's a difference between this optimal policy, or the optimal policy based on our knowledge of the world, Q, which is what we do whenever we're exploiting, and the actual set of actions we take. We don't just take the best guess at the optimal policy here, we do some exploration moves and some best guess of the optimal policy moves and so it's a little bit different than just exploiting or just trying to exploit. Okay great. I'll see you next time and remember: get out there and vote.
