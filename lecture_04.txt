Okay, good morning, it's MIT time so let's go ahead and get started. So today we are in lecture four, there have been so many great questions at discourse during all the previous lectures, hopefully we'll keep that up with this lecture, we've got a lot of great staff answering those and of course you can answer them as well. So last time and last times we've been talking about linear classifiers for a bit now. We have developed various algorithms, one of the big ones that we've been focusing on is this perceptron algorithm and last time, the very last time, we talked about the, sort of, more complete machine learning analysis, really going a little bit more from the start, you know, you have some data thinking about what are good sets of features to be using running your algorithm, you know, be it perceptron or something else and then doing something with that, with what you get out, interpreting it, using it for prediction, or something else. And so today, we're gonna step back into thinking about algorithms, so focusing on that algorithm piece, and learn about what is really just an absolute workhorse algorithm. If you analyze data in real life, at some point in your life, there's just an almost surety that you will be using logistic regression at some point. Now this is a classification algorithm, and we talked about how regression is different from classification, and it just so happens that, you know, for historical reasons and sort of subtle nuanced reasons it happens to be called logistic regression but we're going to be learning about basically this really fantastic classification algorithm today and we're going to see that gradient descent is a more general algorithm that lets us minimize or come close to minimizing a general function that we can sort of slot in and use for the purposes of logistic regression. So first, before we get started on what exactly is logistic regression, let's recall why are we doing this, you know, why don't we just use the perceptron for everything because we have an algorithm now? And something that we saw in lecture two is that the perceptron struggles with data that's not linearly separable and the reality of life is that probably most data that you're interested in is not linearly separable. So, for instance, we saw this example of some penguin data: maybe we've collected data on the flipper length and body mass of penguins and we'd like to classify the penguins into their different species and we can just immediately see that that's not linearly separable and we saw some examples in the health data that we were thinking about, the medical data that we were thinking about, last time as well. And, you know, technically speaking you can use something like average perceptron to get around this but it's very ad hoc and there's no sort of nice guarantee about how that's going to perform and in fact it can perform pretty poorly with data that's not linearly separable, so we'd like to have something to deal with that. Now let's notice another issue with perceptron that may or may not have come up in your discussions already, which is that it sort of doesn't have a notion of uncertainty, like not just what's our best guess right now but how good is that guess or what do we actually know in this problem and that can be a real issue and this is actually pretty related to the linear separability. So for instance, it is fall and I'm going to have to start thinking about whether I'm going to wear a coat outside and especially if you're in a big building with a lot of apartments you really want to make the right call before you get outside and, you know, you want to decide whether you have a coat and in general I might think as the temperature increases I'm less likely to wear a coat and maybe as the wind speed picks up I'm more likely to wear one and so I imagine that perhaps I've collected a bunch of data on whether I've worn a coat or not in the past and I'd like to make a nice little classifier for myself so I can decide whether to wear a coat as the fall approaches in the future and perhaps it just so happens that I did not collect a lot of data at these really key fall temperatures which are sort of in between and so I might run perceptron but an issue with the perceptron is that sort of any classifier that separates this data is equally good in its eyes. So this seems fine, it would stop here, this is fine, but this is just as fine and somehow that seems a little bit off I think if we think about this. You know, the reality is we don't have any data in this area and we'd like to express that as a result we're not so certain about how we should make a decision in this area. Now let's suppose that we've gone and collected more data. So this is just the same set of data but now we've actually collected some data in that intermediate area and so maybe it turns out that this is a good classifier, maybe I like to wear a coat when it's a little bit warmer out just to, you know, maybe I run a little bit cold and so that's well and good but the reality is I definitely don't wear a coat at some perfect temperature wind speed boundary. I think if you if you took actual data of when I'm wearing a coat, the reality is sometimes I'd be wearing a coat at slightly higher temperatures and sometimes I wouldn't, sometimes I'd be wearing a coat at slightly higher wind speeds and sometimes I wouldn't, and this is the way that a lot of things work. You know, in the medical example that we saw last week, you know, it's not like there's this perfect division on any medical indicator of whether somebody has heart disease or not, the reality is there's so much that we don't know about this person or that we're not measuring or that might just be a little random and so we don't capture that with the perceptron, we just say “hey here's a boundary” but we don't say “well actually some of this data, there's a region where I'm just not really certain exactly what's going on and I'd like to be able to capture that.” Okay so we're going to ask, you know, how can we do that, how can we capture these different forms of uncertainty and deal with this not linearly separable data? Okay so let's think about how we could capture uncertainty. Well let's start by thinking about how we can't capture uncertainty. So let's imagine that I might plot my probability of wearing a coat at different temperatures and so if I were absolutely, you know, 100% certain that I would wear a coat below a certain temperature and absolutely 100% certain that I would not wear a coat above a certain temperature I might put this. So the 1 probability represents that I'm 100% certain that I will wear a coat in some range of temperatures and the 0 probability represents that I'm certain that I won't wear a coat. And now, I could imagine if this is actually how I behaved, that I could generate some data about, you know, what I'm going to do on a given day. So particularly, you know, if it's a warm day, let's get a label of “am I wearing a coat,” if it's a really warm day I'm not going to wear a coat because I have 0% probability of wearing a coat. Any warm day, there's 0% probability of wearing a coat and now I could go to a colder day and now there's 100% probability that I'm wearing a coat and so it's very easy to generate these labels, it's just I look “am I wearing a coat or am I not,” the probability is either 0 or 1. And when we look at this, we realize, at least for me, this is not an accurate representation of how I wear a coat, a slightly more accurate representation would be sort of a smoother version of this. It would be to say something like oh, you know, actually, yes, there are some temperatures where 100% I'm wearing a coat, you know, if it's like, you know, well below zero celsius, I'm definitely going to be wearing a coat, but there's some range of temperatures, you know, maybe somewhere around 15 degrees celsius or so where it's a little bit of a toss-up and I'd like to say that my probability of wearing a coat is really between 0 and 1: sometimes I will and sometimes I won't and that that sort of varies smoothly across the temperatures, that as the temperature increases, my probability of wearing a coat goes down until eventually, you know, it gets to be super hot and I'm definitely not wearing a coat. Now if we had this as the model of me wearing a coat and then I were to follow this exactly, I were to generate whether I wear a coat based on these probabilities, I'm going to get something a little different. When it's really hot out, again, I'm basically definitely wearing not wearing a coat because the probability is almost exactly 0 but as it gets colder, as we get into fall, it starts to be the case that sometimes I will wear a coat and that probability is increasing as it gets even colder until we get to a point where it's basically 1 and I'm always wearing a coat. Okay so this seems like a sort of better representation of whether I'm wearing a coat or not and so it'd be nice to see, you know, how do we make this shape, the shape of going from, you know, a 100% probability of wearing a coat, you know, down to 0 but in sort of a smooth way? Okay so it turns out this has a name, this is called the sigmoid or logistic function, sigmoid because it looks sort of like an s. So let's just draw out the canonical sigmoid function, so sigma here is a function: it takes as an input z, it is (1 / 1 + exp(-z)) so let's just draw that. So we're going to draw it as a function of z, we're going to draw sigma(z), and let's just check that we agree with this drawing. So in particular, as z gets very, very, very low, like it gets very negative, we notice that -z gets really big, so the exponential function of -z gets really big, and so the denominator gets really big and so we're, you know, taking 1 over a really big number, that's going to be like 0. Now on the other end, we're gonna have our z getting really, really, really big so -z gets really, really, really negative so exponential of -z gets really close to 0 and so we're dividing 1 over 1 plus something really close to 0, so that will be near 1. So we see that all of this function is always going to be basically between 0 and 1 and we can ask what happens at the origin. So the origin, we'll have z = 0. Exponential of 0 is 1 and so we're gonna have 1 over 1 plus 1, that'll be 0.5. Okay so this seems to check out, this is a function, but uh oh it's not quite the function that we drew at the top, there's a few things that are a little bit different. You know, if we look at this original s curve that we made that describes, you know, how I behave in different temperatures in terms of wearing a coat, it's a little bit different than the s curve that we just made, this sigmoid logistic function, and so we want to ask how can we go from this canonical sigmoid logistic function to this one that describes, you know, the probability of wearing a coat in different temperatures? And we can do that basically by just stretching and pulling and moving around this bottom function. So in particular, something that we can do with this bottom function here is we can say, okay, I could have z as an input, but if I just scale that input, like if I had slightly different, you know, units for z or, you know, I just multiply it by some number, some constant, let's call that theta, then I can sort of squish it or pull it out. And so here what we've done is we've just said by multiplying by this constant we sort of squished it but you could also pull it out so that it has sort of a bigger width. Now another thing that we could do is, you know, and certainly that we want to do here is flip it, but we can include that in theta as well. We could just have a negative theta and so then we would get something that's flipped. Now similarly we actually might want to move this around, we might want the point where it's 0.5 or sort of the area where we're a little uncertain about what's going on to to be at a different point along the z-axis and so in order to accomplish that we can just add a constant to z. And so we can move this around there. And so in general, we can start from this canonical sigmoid and again just by stretching it with theta and moving around its offset with theta naught, we can get basically these different sigmoid functions in different spaces that behave, you know, in these ways that we want. And so just going back up to our temperature example up here, here we're thinking of x as a feature. So earlier I was just using z as some generic input so that we didn't confuse it with our our data, but here we're thinking of x as our actual feature, you know, it's our feature that we observe and perhaps some classification problem and so let's describe this curve of probabilities as g(x) and so then let's ask what would be the formula for g(x) based on what we just said? Well g(x) would be some application of the sigma function, but to this sort of stretched and offset version of x, so we'll take sigma and apply it to theta x plus theta naught to get this g(x) and then all I'm going to do here is write exactly that formula. So I've just taken sigma(z) which we defined below and I put in this new input theta x plus theta naught. So this is just exactly the sigma function that we see at the bottom of the slide but with this input theta x plus theta naught and so that's the g(x) that we're considering here, that'll let us have this sort of smooth change instead of having this abrupt sort of one-zero function, we're having a smooth change between, you know, always doing something and always not doing something, like always wearing a coat and always not wearing a coat. Okay, so at this point we've sort of described how you could take, you know, some smooth notion of probabilities and turning it into sort of simulated data. Now what we really want to do is the other way around, what we really want to do is have our real data and then learn this smooth function of probabilities and we're going to do that in a moment, but first I just want to look at what this simulation looks like in two dimensions. So we have a sense of what higher dimensions look like, not just one dimension. Because in general, of course, our features aren't just one dimensional, they could be higher dimensional, in fact they typically are higher dimensions. Okay so let's take a look at that, so first recall, we just looked at one dimensions with one feature. So in that case our g(x), this is just the g(x) that I just wrote on the previous slide, we saw that it looks like this nice sort of s curve and that I could generate sort of my simulated data by drawing, you know, “am I wearing a coat or not wearing a coat”, according to the appropriate probability, so let's look at two features now. Okay so what's going to go on with two features? Well I'm going to propose that we do a g(x) much like we did last time, that we have a sigma function and we apply it now to, instead of theta x plus theta naught, theta transpose x plus theta naught, which is starting to look pretty familiar if it wasn't already, definitely theta transpose x plus theta is very familiar from linear classifiers at this point. Now first of all, let's just do a dimensionality analysis to make sure this makes sense. So remember theta transpose x is going to give us a scalar, theta naught's a scalar, so the whole input to sigma is still a scalar which is good because that is what we assume that sigma takes as input, just a one-dimensional scalar. So even though x is high dimensional, the input to sigma is just one dimensional. Okay so that checks out. Now let's think about what this actually looks like as a function. So remember in two dimensions our feature set is going to be x_1 and x_2 and now we're going to draw our g(x), that's going to be some function of both x_1 and x_2 and to figure out why this shape is appropriate, let's start sort of looking at this function a little bit more. So first of all, this looks very familiar again: theta transpose x plus theta naught. What do we know about this function? Well theta transpose x plus theta naught, when that's equal to zero, that's a line. Now at the same time, when that's equal to zero, we just said that g(x) equals 1/2, so theta transpose x plus theta naught describes the line where g(x) is equal to 1/2 and in fact, in general, theta transpose x plus theta naught equals to a constant describes a line where g(x) has the same value across that line. Okay so this is sort of, this dividing line this is, you know, this 1/2 line and in general this will be a hyperplane and in this case it's a line. Okay so now let's ask ourselves, well what happens in this direction, as we go away from the line in the direction of theta? So remember theta was sort of our normal vector, it still is here. As x increases or gets more in the theta direction than this line, well as we do that this dot product is going to increase, so this whole quantity is going to increase. So the negative of that quantity is going to become very negative, so the exponential is going to become very small, and so we're going to get something like 1 over 1 plus a very small number, and that'll be near 1. And so as we move x in the direction of theta, we're getting numbers that are increasingly near 1 and that's what we see in the plot. Conversely, if we move x in the direction of negative theta, then we're going to have the opposite thing occurring: we're going to get that exponential input being very small, we're gonna get something—sorry that being very negative—and so the negative of that input is going to be very large and so we're gonna get a very large value, we're gonna get 1 divided by a very large value, and we'll get something near 0. And so it's still that “s” sort of function, but now you can see that in higher dimensions it's like it's spread out along this theta transpose x plus theta naught equals zero line. And then what does it look like to generate data according to these probabilities? You know, suppose again that x_1 was temperature and x_2 was wind speed and so what I could do is I could say, “okay, here's a particular day and it has maybe a low temperature and a high wind speed” and then I could draw, you know, according to this probability, that's extremely near 1, am I going to wear a coat so I'm going to probably wear a coat. I can make some draws along that boundary, where the probability is really closer to something between 0 and 1—I mean it's always between 0 and 1, but it'll be closer to, you know, maybe 0.5 or so—and then sometimes I'll wear a coat and sometimes I won't. And then we can look at that sort of lower corner where the temperature is very high and the wind speed is very low and there I see my probability of wearing a code is near 0, and so in general when I flip a coin that has, you know, probability or, you know, roll a dice or something that has probability near 0 of wearing a coat, I'm not going to be wearing a coat most of the time. Okay so again what we've done here is we've said, “hey let's imagine a world in which I decide whether or not to wear a coat by, you know, randomly choosing whether to wear a coat according to this sort of sigmoid function, here's how it can generate data.” But again, of course, the world that we live in is one where we're given the data and then we want to learn what's a good sigmoid function that fits that, what's a good, you know, smooth set of probabilities that describes that data that we're seeing and so that's what we're going to do next, we're going to sort of flip this on its head.

Okay, so when we do that, that is called one of two things. One way we might call it is linear logistic classification, because it's classification: we're trying to decide whether I wear a coat or not, that's a classification problem, that's a binary classification problem. We used a logistic function, also known as a sigmoid function, and we saw that there was that linear component, that theta transpose x plus theta naught. Now the reality is when you go out into the world and you use the method that we're going to be describing, it's called logistic regression. We're going to cover regression in a lot more detail in, I think, next week and we've already talked about how regression is a little bit different from classification, so don't get too hung up on this, it's just a name, but this is definitely a classification algorithm. Okay so we have two big questions at this point: one, how do we learn a classifier? Suppose we have a bunch of data, how do we learn these parameters, theta and theta naught? And so the idea there will be we'll have a bunch of data, you know, somebody will give us data, we'll have collected data, and then we're going to want to learn theta and theta naught, and we've already seen here that theta and theta naught actually describe this sort of curve of probabilities, this sigmoid function. Okay but that's not quite what we've done in the past, right? What we've done in the past is we've said we've come up with some data and then we come up with a way to predict on future data and so what we'd like to do too is to say, “well once we have this theta and theta naught, how do we make predictions with it?” You know, how do we say “hey, on this part of the space I'm going to predict that I'm going to wear a coat, and in this part of the space, I'm going to predict that I'm not going to wear a coat?” Okay so we need to answer both of these questions but I'm going to start by answering the prediction question and then I'm going to come back to the “how do we learn a classifier” question. So let's start by saying “hey suppose that I have a theta and theta naught, what predictions does that imply?” So that's the question we're focusing on on this slide. Okay so we need to decide whether we're going to predict +1 or predict the other option.

And here's a way to think about it well, you know, if we lose just as much by being wrong in either direction, we might say, “hey I'm going to predict +1 if it's more likely that I'm wearing a coat, so if the probability of me wearing a coat is greater than 0.5.”

Okay well what is that? Well we just said that the probability of me wearing a coat, the probability of a +1 label, is sigma applied to theta transpose x plus theta naught and so just another way to write the probability of me wearing a coat being greater than 0.5 is exactly this equation in sigma.

Okay well just to see what this means, let's write this out. So just as we said on the previous slides that a different way you could just write this out—mainly you just use the definition of the sigma function, the sigmoid function—is we'll just write it it's 1 over 1 plus exponential of negative the argument. So this is another completely equivalent way to write that the probability is greater than 0.5 and now we can sort of solve for this. We can say, “hey this is completely equivalent to the exponential here being less than 1.”

That's how I can get this quantity to be greater than 0.5 and then you can recall facts about the exponential function to see that this is completely equivalent to theta transpose x plus theta naught being greater than 0. So we're going to predict that I'm going to wear a coat, we're going to predict a label of +1, if theta transpose x plus theta naught is greater than 0 and we're going to predict the other label, in this case -1, if theta transpose x plus theta naught is less than or equal to 0. That should sound super familiar, this is like exactly the hypothesis class of linear classifiers that we have studied throughout this course so far. We are looking at the classifiers that predict +1 if you're greater than the theta transpose x plus theta naught equals 0 line and -1 on the other side and -1 on the line. That's something that's just exactly what we've looked at extensively and so what's new here, you know, if that's just the same hypothesis class that we've looked at this whole time, what are we changing? Well there's two super key things that we're changing here: one, something we have not gotten out in the past and certainly you don't get for free from this hypothesis class is uncertainties. So we're going to come up with a method that learns this theta and theta naught, but crucially when we have our theta and theta naught, we can put them into the sigmoid function and get this whole function, so not just a prediction of “am I wearing a coat or am I not wearing a coat,” but a sense of “well, am I wearing a coat most of the time or am I definitely wearing a coat?” These are the types of questions that we can ask and answer once we have this sigmoid. But also, just because our hypothesis class is the same, doesn't mean that we're learning it in the same way. In fact, we're going to be learning it in a very different way, we're going to be using a totally new learning algorithm that we haven't seen before and that learning algorithm, unlike our previous ones is going to have nicer guarantees for when data is not linearly separable.

And so we're going to see that going forward: what are those guarantees, what is the performance when the data is not linearly separable? Which is something that again we just really care about in practice and so this is just a new algorithm, it will have new performance guarantees. It's just that it's operating on the same class, the same class of classifiers that we had before this hypothesis class.

Okay so at this point we have this sigmoid, we have this hypothesis class, we have the ability to predict, but what we haven't yet done is to say if we have a bunch of data, how do we learn this sigmoid, how do we get that out? And so that's what we're going to look at next: how do we actually learn a classifier?

Okay so let's look at that now. Now let's start by just getting a little intuition for how could this work. You know, what's in some sense, you know, the way that we've been approaching learning a classifier throughout this class is going to be the way that we approach it here, which is that we kind of want to make a classifier that fits with the data, we want to choose a classifier that kind of fits with the data, and so let's ask ourselves what does it mean to fit with the data here?

So here in particular, you know, if we were looking at this wearing a coat example over different temperatures, this looks like a pretty good description of this data, a pretty good fit to this data, this curve, because it says the probability of wearing a coat is high where I'm wearing a coat.

And it says that the probability of wearing a coat is low, almost 0, where I'm not wearing a coat. And it also says the probability of wearing a coat is somewhere in between, where I'm sometimes wearing a coat and sometimes not wearing a coat, so there's a lot that sounds pretty good about that. Conversely, if I was looking at this sigmoid, this one seems pretty bad because it says the probability of wearing coat is really high where I'm not wearing a coat and it says that the probability of wearing a coat is really low where I am wearing a coat, so somehow this curve needs to align with the data and, in particular, somehow the probability of the data should be high for the curve. This is a way to describe this kind of alignment, so what we're going to do is we're going to look at what's the probability of the data under a particular curve. So for a particular theta and theta naught which describes one of these curves, we're going to ask what's the probability of the data? And then one way to go about doing things is to say well how can we maximize that probability?

Okay so in order to accomplish this goal, first we need to write out what's the probability of the data, so let's start by doing that.

Okay now here I'm just going to make a point about probability, it's not a super important point if you haven't taken probability, I'll just sort of say where this comes from, but it's not something that we'll, in general, be expecting you to be on top of. But this idea that if each of these data points, you know, given this curve is independent, you know, we're just making a draw and it doesn't depend on the other data points, if that's true, then the probability of a bunch of independent events is the product of their probability, so that's the only thing that's happening here. Now I do want to describe this notation, this big, you know, product symbol is what's happening here. So you're probably familiar, hopefully you're familiar because we've already done it a lot with the summation symbol. So the summation symbol tells us “hey there's a bunch of, you know, things that we're summing over and, in particular, we're going to say let's take, you know, the thing after the summation symbol with i = 1 and add it to the thing after the summation symbol with i = 2, and the thing after the summation symbol with i = 3 and so on.” And it's just the same thing with this product symbol: we're going to take the thing after the product symbol for i = 1 and multiply it by the thing after the product symbol for i = 2 and multiply it by the thing after the product symbol for i = 3 all the way up to n. So we're just saying that the probability of all the data is the product over the probability of the data points for each data point, so we go from data point 1 up to data point n. Okay well what is the probability of a data point? We said that the probability of wearing a coat, of having a +1 label, is this g function applied at x. So let's call g^(i) exactly that function applied at x^(i), at the i-th data point. So this is the probability of the i-th data point having a positive label of me wearing a coat. Now, that's not quite the same thing as the probability of data point i, because some of them have negative labels and so what we want to say is that if data point i has a positive label, its probability is g^(i), if it has a negative label, well that's the the only other thing that can happen so that probability is 1 -  g^(i).

So those are the only two options and so a way of expressing probability of data point i is to say it's got g^(i) probability if y^(i) is +1and 1 - g^(i) in the other case because there's only one other case.

Okay and now all we're gonna do is just write this in a clever way.

So let me explain what's happening here. So first of all this notation, where there's a 1 and then these brackets, you should evaluate that to be 1 if the thing inside the brackets is true and 0 otherwise. So this quantity is 1 if y^(i) is +1 and it's 0 if y^(i) is not +1.

And so what's happening here, well if y^(i) is +1 then this evaluates to 1 and so effectively we have a factor of g^(i). If y^(i) is not +1, then that exponent evaluates to 0 and anything to 0 is just going to be 1, and so we don't have a factor of g^(i). Similarly, if y^(i) is not +1, then this evaluates to 1 and we get a factor of 1 - g^(i). If y^(i) is not +1—sorry if y^(i), yeah if y^(i) is not +1—then we get a factor of 1 - g^(i). If y^(i) is +1 then that's false, so the exponent is 0 and so that term goes away. And so you just wanna convince yourself that this is literally just a fancy way of writing the line above.

Feel free to to take some time later and think about that if it's not immediately obvious but it's really just a fancy way of writing this line and it's one that comes up a lot.

Okay so one observation here is that every one of these probabilities is between 0 and 1 and as soon as you take a product of a bunch of things that are between 0 and 1, they get really small really fast. And that's fine if you were working with all of the exact numbers and the reality of our lives is that we work with computers and so computer precision is a real thing that can trick us up and so taking these products can actually be a real issue in terms of running into computer precision and running into sort of numerical instabilities, and so purely, purely because this will run better on our computers, it helps to look at the log of the probability instead of the probability. So whereas the probability is between 0 and 1, the log of the probability will take these real values that can be much more spread out, and that we can actually get sort of a handle on. And it's worth noting that if we were just trying to find the theta and theta naught that maximize the probability, that'll be the exact same theta and theta naught that maximize the log probability, because log is a monotonic function and it doesn't change the ordering of these different theta and theta naughts. Okay one more observation: in the past we haven't talked about things being better than other things, we've talked about things being worse than other things, that's the idea of loss, right, that we sort of don't want the worst thing, instead of we do want the best thing. And so if we want to turn this into our loss framework, if we want to change this so it's in our loss framework, we can just take the negative. So instead of looking at something that we want to be more positive, we're now looking at something that we want to minimize. We want to minimize the loss whereas we wanted to maximize the probability or the log probability.

Okay now let's write this out. Now in order to write out the log probability, I'm just going to take a log of the thing that's at the end there.

So it feels like there's a lot going on here, but I really just took the log of the thing at the end and put a minus in front because we're looking at the minus log probability. So let's just go through briefly how this happened. So we started off with a product. When we take the log of a product, we get a sum of logs.

When we take the log of something with an exponent, that exponent comes down.

And then we're left with log of g^(i).

And so finally we have this whole quantity which we got by saying “hey this is the negative log probability of the data, it depends on theta and theta naught through the g^(i)” and so we can think of this as our loss. Now this, also if you think about it, looks very familiar. So in particular, when we define training loss in the beginning we said that it was basically the sum of losses on individual data points and that's what we have here. Now that's not quite the definition we use though, we actually said it was the average, so it was 1 / n times the sum, and so we can just add that here too. So here we have a sum, but from the perspective of, you know, how do different theta and theta naught compare, it would be completely equivalent to minimize the loss which is 1 / n times that sum. And so I'm just going to go back here and say hey our loss can actually be negative: 1 / n times the log probability.

It's sort of like the same trick we played with taking the log: that it didn't change the orderings of things and it was convenient, and so that's all that's happening here, it's just something that doesn't change the order of things and it's convenient.

And now once we've done this, we have a loss that really fits into the framework that we talked about from the beginning: it's 1 / n times the sum of losses on individual data points, so we look at the loss of the i-th data point here.

I realized that some of these i-s should be superscripts, I will correct that in the slides before we share them, but apologies for that, they should be superscripts. Okay so now let's just finally identify what is the loss on an individual data point. So we did all this work, we said “hey we have something that looks like 1 / n times the sum of losses on individual data points,” so that would be exactly this term for any particular i is the loss on an individual data point. And so let's give that a name. A really natural name is negative log likelihood loss. It's a loss, likelihood's basically another name for probability, we're looking at probabilities, we took the log of the probabilities, that's that log of g, because again we're thinking of g as a probability and we took the negative. So this is literally just describing negative log likelihood loss and again we we have this sort of thing where we think of g as being our guess and a as being our actual value and so writing that in this is what we get.

So we're taking this negative log likelihood loss, so L_nll. Our guess is a little bit different than in the past. In the past, our guess was something that was really binary, it could just take two values. Here we're really letting it actually be a probability, something between 0 and 1.

But our actual, what we actually observe, is we never actually observe a probability. What you actually observe is “did I wear a coat or not.” I just did or did not that's, you know, that's life. I'm not wearing like half a coat and so the actual is something that really is just one of two values.

Okay so what have we done here? Well we're trying to learn a classifier, we're trying to learn theta and theta naught from this data, we have classification data and we want to learn this sigmoid. And what we've here defined is a loss, a totally new loss that we hadn't seen before, and just as before we kind of, you know, tried to find a value of theta and theta naught that got as close to the lowest loss as we could on our training data. We're basically going to do the same thing here, only a bit more systematically. In particular, here we have this function of theta and theta naught and because it's actually pretty nice, it's continuous, it's differentiable, we're going to be able to use really powerful optimization tools to actually try to minimize that and theta and theta naught. So let's just make this a little bit more explicit.

Okay so how are we learning this classifier? We're going to try to find parameter values of theta and theta naught to minimize this sort of training error loss that we've talked about, this average loss, specifically looking at negative log likelihood across the data.

Okay so this is exactly what we defined on the previous slide. So we defined what it was to have a negative log likelihood loss, we said it took a guess and an actual, where in this case the guess is actually a probability: it's between 0 and 1, and the way that we get that is this sigma function. That gives us the probability of a particular x. Our actual is our actually observed y, that's the y^(i). And then we take this average, this 1 / n sum i = 1 to n. Okay so let's give this a name because we're going to be looking with working with it. The sub lr, the subscript lr, that can stand for logistic regression because again we're doing this classification, you might call linear logistic classification if you wanted to really describe it in exact words that are pretty clear, but in general it's called logistic regression and so let's call it logistic regression. And so we can say “hey, this is the loss that we want to try to minimize, this j_lr.” Now because we're going to be trying to minimize it and we're going to be taking sort of an optimization approach, how can we optimize this, how can we minimize this over theta and theta naught, another word that you might use to describe this is an “objective,” an optimization objective. It's sort of your objective to minimize that number.

Now one last notational thing here: because we're going to be talking pretty quickly about general optimization approaches that don't have to be for classification, that don't have to be for this particular loss, that could be for really any function, it'll help us to have a way to talk about really general inputs and so we're going to say this capital theta is the input to our objective. In this particular case, it's equal to theta and theta naught together, it's all of our parameters, but in general it's just, you know, it's some collection of inputs to an objective.

Okay. Oh there's a question. Yes, going back to the last slide when we were looking at the objective, why do we multiply all of the data points together to learn the classifier, the probability? Yeah so great, so if I understand the question correctly, I think it's about the first line here, which is in some sense: why is the probability of data equal to the product of the probability of the individual data points? This is a fact of probability that is really outside the scope of this course but you will find if you take a probability course, that the probability of a bunch of independent events, so events that essentially don't depend on each other, is the product of the probabilities of those events. And so here the events are “am I wearing a coat on the day that is described by i with the features that are described by i,” “am I wearing a coat on the day that is described by i +1,” etc. And even if those features were exactly the same, we could still think of those as independent, because yes today might be 55 degrees and tomorrow might be 55 degrees but I actually might, you know, just forget my coat one of those days or, you know, something else random might happen. And so here we're just using this definition of the probability of the data as being the product of the probabilities of the data points. It turns out that that's the same as having the sum of the log probabilities of the data points, so if you just really like sums then that's a really natural thing to do. But there's no similarly nice way of thinking about the sum of the probabilities, it just turns out that multiplication is the more natural thing when we're talking about probabilities here. Hopefully that answers the question. Great.

Okay so we have this loss.

It's a function of the parameters, that's the thing we're trying to learn: we're going to try to find the parameters that minimize this loss.

And so again let's ask ourselves: well sort of what is this going to look like? We're going to imagine that, much like in our previous algorithms, we're going to start from some values theta, theta naught that probably aren't that good because in general we don't know what we're doing when we start our algorithm, and then we're going to look at them and we're going to say “hey for this particular, for this particular curve it's saying that there's a really low probability for these times I wear my coat so it's not really good, so we should move it to make those a higher probability. Hey we can make them an even higher probability, hey we could make them an even higher probability.” And we just keep doing this until we get to a point where the minuses, the times I'm not wearing my coat, have a low probability of wearing my coat. The times where I am wearing my coat have a high probability of wearing my coat and the times where we're not sure are somewhere in between. Now what's really cool about this objective function and this loss, instead of the previous losses we were working with like zero one loss and so on, is that it has so much more information about how do we do this, about how do we move in that direction. What is the best direction to be moving? And in particular the reason for that is that it is smooth and in particular it has a gradient: it's differentiable and the derivative tells us basically a good direction to be moving and so what we're going to do on the next slide is we're going to make that idea a bit more precise but we're going to talk about more general functions and then we're going to come back to logistic regression. So basically we have this linear logistic classification, this logistic regression set up this thing that we want to minimize and now we're going to talk about a tool for minimizing functions that we can apply to this, but it's going to be a much more general tool and that tool is called gradient descent.

Okay so to talk about gradient descent, let's start by talking about a function. So here we have inputs, theta_1 and theta_2, and this is just totally general inputs and a totally general function f. You know, forget about logistic regression for the moment, we just have some function and we would like to minimize it and it seems like we should. I feel like I see a minimum on this and I would like my algorithm to see that minimum too. Now another useful way to draw a function like this, is with what's known as a contour plot. If you have ever looked at a topographic map, you have looked at a contour plot. So the idea of a contour plot is just to say: let's draw lines at points that have the same height in the original plot. And so this is a contour plot for exactly the f that's just above it and in particular what we're seeing here is in that sort of valley there are little contours and then they get bigger and bigger and bigger as we go out for things that have the same height.

Okay, now how does a gradient help us if we're trying to minimize this? Well, we can look at a particular point and if we look at the line that is tangent to the contour, that tells us where to go to stay at the same height. The whole idea of the contours is if we keep walking on it, we stay at exactly the same height. But we don't want to stay at the same height, we want to drastically change our height. The gradient tells us the direction to increase our height the most: where could we go from where we are right now to most increase our height? That's the gradient. Now the gradient has a precise mathematical description: suppose our theta is in R^m. So here our theta is in R^2, but in general it could be m-dimensional. The gradient is the vector of partial derivatives. So the partial derivative of f with respect to the first variable, the partial derivative of f with respect to the second variable, up to the partial derivative of f with respect to the n-th variable.

Okay so that all sounds really good, but actually we didn't want to go in the direction of increasing height, we want it to go in the direction of decreasing height, we want to get to the lowest point. Now in order to do that, we won't go in the direction of the gradient, we'll go in the direction of the negative gradient. So we just have to go in the exact opposite direction to decrease the height as much as possible from where we are right now. This is the idea of gradient descent, we're just going to keep doing it. Okay so let's define gradient descent: this will be an algorithm, it takes a bunch of inputs, but let's talk about those inputs as we go along, we'll explain them all.

So first we have to start somewhere. So theta super something is going to tell us where we are in a particular step. So 0 is where we start and we have to tell the algorithm where we're going to start. That's a tough problem, initialization is a tough problem. You want to have a good initialization, it helps the more that you have it, and that is just a really, you know, difficult thing to do in general. Although we're going to see that for some types of problems, at least it doesn't matter in terms of whether you get to the right answer, it might matter in terms of how quickly you get to the right answer.

Okay then we're just gonna have a counter for what round we're on. So this just says t = 0, we haven't yet started, we're about to start. We're going to repeat these steps of trying to go lower.

So now we're going to start our first round, so t = t + 1. And then here's the meat of it. So what this is saying is let's take the gradient, that's the last thing in this line, that's the thing that tells us what's the direction to go for steepest descent, or let's take the steepest ascent which is the gradient and then when we take the negative we're now looking at steepest descent. Let's multiply it by eta. So eta tells us how far we're walking in this space and so it's called a step size parameter because it tells us how far we're going in this direction of steepest descent. And so what we do is we take our previous spot where we were before and we walk that far to a new spot theta^(t).

Okay, so in this plot we've done that once: we started from our big red dot, we followed this arrow to a new spot, but we can do it again. We can say, “okay now we're at our theta^(1), let's calculate the gradient there. Let's go in the direction of the negative gradient, let's do that by some step size eta, and let's end up at a new spot theta^(2).” Okay at theta^(2), we'll calculate the gradient. We'll go in the direction of the negative gradient, we'll do that by some step size eta and we'll get a theta^(3) and so on and so forth. And so then the question just becomes when do we stop? And there are a few different ways to stop. One is to say well, you know, think about what happens when we get to this valley at the bottom. Eventually the valley, you know, we hope that we're going to be making smaller and smaller and smaller step sizes as we get closer to this minimum and so eventually we'll stop just when the change in step sizes is sufficiently small. So epsilon is an input to this algorithm it just says, you know, when is too small to keep going. It could be something like machine precision, because what's the point of going beyond machine precision. Even easier would be if you're only going to report two decimal places, why bother keep going after that, so you could do something like that, choose epsilon that way, but basically it'll tell you when to stop. Now there are a few other ways that you could stop and return theta^(t). One really important one is to have a max number of iterations. I actually highly recommend that no matter what you do, no matter what algorithm you make, you always incorporate a max number of iterations. If something goes horribly wrong in that algorithm, if you run out of time, you just want to have stopped and so a really good idea here would be to say if I reach some other stopping criterion, like the change becomes really small, I can stop or if I run out of time because I'm running out of time in lab or homework or, you know, I'm just running out of time in my analysis, I will also stop. Now there are other things that you could do to stop besides the change in f, another one is to just say I'm not moving really much, that much in theta, so that would be a way to stop. Also, at this optimum we expect the gradient to be zero, you know, that's basically saying there's nowhere else to move to, you know, descend further and so that would be a way to stop when the gradient is very small. Okay so this is a very general algorithm, this is not specific to logistic regression, this is not specific to loss minimization, this is just a way to minimize or come very close to minimizing a general function f, but what's really cool about it is that it has guarantees on quality and so let's check out those guarantees.

Now before we describe those guarantees, it'll be helpful for us to have a particular definition which is: what is convexity? So we're going to say that a function is convex if we can look at its graph, pick out two points on that graph, make a line segment between those points, and if that lies above or on the graph, always, it's convex.

So here's a graph of a quadratic.

So I say that a function on R^m is convex. What I mean by that is a function with inputs in R^m. So here we're looking at a function, the horizontal axis is the input, the vertical axis is the output. So what is m in this function? What is m? Could you put that in the chat?

Great so I'm seeing a few different answers. So if you're writing 1, you are correct. So 1, so m here, is the input to the function. So we're saying: what is the dimensionality of the input? So that's what we mean by on R^m. So in this particular case, this function has dimension 1 for input, that's the horizontal axis, so that would be like z and then f(z) or theta and f(theta), f(theta) is the vertical axis. So here we're looking at a function on one dimension, but then the function value is what's providing the second dimension that we're seeing. Great. Okay so now, to ask if this is convex, we're gonna take any two points, we're gonna draw the line segment between them, and we're gonna ask: is that line segment above the graph? And here it is and it turns out that because this is quadratic, you can show that for any two points that you take, the line segment will always be above the graph. Okay now here's another question for you: here's a new function, is this function convex? This is one for the chat again, is this a convex function?

Okay lots of great answers including a no with tons of o's and a bunch of exclamation points, so well done, it is not convex. And so here's the thing: yes you can find two points for which the line is above the function, but it's not convex because there exists any two points for which the line is sometimes below the function and so just because I could find these two points, and you see in the middle that the black line is below the blue line, that's why it is not convex and somebody said but not overall. Indeed there's some locally convex looking things but the whole function is not convex. Okay but we can do this in higher dimensions too. So this happens to be a fourth degree polynomial that is convex: no matter which lines I find, it's always going to be above the function. And again here's a question for the chat: is this function convex? Is this a convex function?

You're all totally right. No it is not convex, frowny face. Indeed, again it's just a proof of concept. Could you find an example of a line that goes below it and if you took these two dips that it has and put a line between those dips, that would go below the function, so this is not convex. Okay, so why are we talking about convexity? Because convex functions are easier to minimize. So in particular, here's a theorem about gradient descent performance. If, now in my assumptions I'm going to choose some epsilon tilde, this is basically going to ask me how good am I, we'll see that again in the end. I'm going to assume f is sufficiently smooth and convex: I can differentiate it, it's continuous in the right ways—there's a little bit more advanced math than we're going to get into here—but it's a nice enough function and it's convex. If it has at least one global optimum, so what do I mean by global optimum? Let's look at this example up here: a global optimum is a point such that its f value is at least as low as every other f value.

A local optimum, which is the diamond here, is a point that locally that's true, that if I just look at a small area around it, it's at least as low as everything else, but it doesn't have to be the case that it's at least as low as absolutely everything else. So global optimum is an example of a local optimum but not every local optimum has to be a global optimum. An f has to have at least one global optimum for our theorem to hold.

Now eta has to be sufficiently small. You're going to explore in your lab how a big eta can have big problems, but if it's sufficiently small then this theorem will hold. That doesn't mean it'll hold quickly but it means it will hold.

Then if we run long enough, so if we don't cut off with our criteria too soon, because we do have this termination criteria so we have to make sure that we run long enough, you know, that we don't cut off with that too soon, then gradient descent will return a value as close as we want to the global optimum, so that's what this epsilon tilde is saying: I can choose however close I want to be and I can get that close again by running long enough.

So this is pretty cool. This is telling us that, you know, under some conditions that just so happen to hold for logistic regression, so that's nice, we can get this nice convergence property.

Okay so let's make this a little bit more explicit, let's come back to logistic regression here. Okay I normally I have the questions go to discourse, but there is this question about optimum equals minimum. So when we talk about an optimum, it can refer to a minimum, but it can also refer to a maximum and so here I am specifically thinking of a minimum, so that's a great clarification, yes this should be, we're thinking of minimum, because we're thinking of gradient descent performance. So yeah good point, I'll probably just correct that in the slides because that is a good point. Okay cool.

Okay so now let's look at gradient descent for logistic regression.

Okay our loss, first of all, is differentiable. You'll notice that in gradient descent, we take the gradient, we take the derivatives, and so you can't even apply it if you don't have derivatives. And so here our loss is differentiable, so we can apply gradient descent, so that's the first big thing. And then the second thing to notice which, again is a little bit beyond the scope here, but I'll just tell you: it's convex. And so that's really nice, because if all of our assumptions in the theorem hold, that means the theorem will hold. Now remember there were some other assumptions in the theorem and we'll come back to that. But this means we can apply gradient descent and we're going to get good performance a lot of the time. So let's take a look at this, let's run gradient descent for our problem, so what will happen there if we run gradient descent for our logistic regression problem, instead of just a generic f, we'll put in the loss for logistic regression, we'll put in its derivatives, it's gradient, and then everything else we have to choose. We still have to choose an initial value, we still have to choose a step size, we still have to choose an epsilon, but we can run gradient descent on our problem and so let's ask what that's going to look like. So in particular, we had this “wear coat” problem from before: ”am I going to wear a coat?”. This could be the data that I get: I run gradient descent and this is what I would get out. In this particular case, I'm going to get out this nice, you know, I'm going to get out theta and theta naught but that describes one of these sigmoid functions that says I have a high probability of wearing a coat in one part of the temperature spectrum, in particular the cold part, I have a low probability of wearing a coat in the hot part, and I'm not so sure in the middle. Okay, suppose I have this data, so I'm deciding whether I'm going to wear a swimsuit today. So in this case, I probably tend to wear a swimsuit when it's hotter and not wear one in the middle of winter and maybe there are some days where I'm willing to wear one but it's a little bit chilly. So how is the output of our learning algorithm going to look different here? This is a question for the chat: what's going to look different about this function that we return, this sigmoid relative to the one that we're seeing on the left?

Okay so people are definitely saying it's going to be flipped horizontally.

It's going to be offset, it's going to be one difference.

There's actually potentially one more difference. So let's take a look at this. So first of all, we see that it's flipped because I'm very likely to wear a swimsuit in hot temperatures and very unlikely in low temperatures. Great and the last one is steepness. Okay so we've got them all. Fantastic, so the first one we're seeing is that it's flipped, the next one we're seeing is that it's offset, so the temperature where I start wearing a coat, at least for me, is different than the temperature where I'm willing to wear a swimsuit, it's much warmer where I might be willing to wear a swimsuit and then we see that in this particular case it's a little steeper because there's not such a large range where I might do either.

Okay so this is after running the entirety of gradient descent, so this is where I've gone for a really, really long time. I've gotten to something where it's not changing too much, this is how I got this theta and theta naught and I got this on this curve out. Let's actually look at some of the steps along the way for a different problem.

Okay so here's yet another problem that is very Boston relevant. So when it gets really cold in the winter, I will often wear a base layer so, you know, wearing a layer of wool or fleece under your pants can be fantastic if you're walking outside and it's really cold, I think it really makes a difference. So something I might do is I might say when it's sufficiently cold, I'm going to wear a base layer, I'm going to wear a sort of a second pair of trousers essentially and when it's not that cold, I'm not going to bother with that. Now something you'll notice here is that, in this particular data set, I don't have data between the pluses and the minuses, so in the last two data sets we saw that there was an area where sometimes I was wearing the coat and sometimes I wasn't, and here I just happened to not collect data in that area and we said we wanted to express that we're just uncertain about that, so let's see what it looks like when I run gradient descent.

Okay so I'm going to initialize my gradient descent with a theta and a theta naught and maybe that theta and theta naught look like this, so I'm not plotting the theta and theta naught, I'm plotting what the sigmoid that depends on those theta and theta naught looks like, but every theta and theta naught describes one of these functions. And now I'm going to take a step of gradient descent and the thing to notice is, well, what's going to happen? Let's think this through. My pluses right now have high probability and my minuses have low probability but I could increase their probability even more by making this steeper, and the whole idea of our loss is that we want to increase that probability and all it's saying is that we want to increase the probability. And so we could do that by making this steeper. So a step of gradient descent will make this steeper. Okay well I could still increase the probability of this data set even more by making this steeper and so I take another step of gradient descent and this gets steeper.

And it's still the case that I could increase the probability of this data set even more by making this steeper and so gradient descent will continue to make this steeper. And it's still the case that I could increase this even more by making it steeper and I think somebody has asked exactly the right question: “what if it gets too steep?” This is definitely too steep, this doesn't express what we wanted to about this data, which is that we're uncertain in between. So let's see what went wrong and how could we correct this. And so in order to understand what went wrong, I'm going to draw the loss function.

So now technically remember this loss, is a function of two things: it's a function of theta and theta naught but let's just pretend that theta naught is always set to zero to make this an easier thing to draw. So this is the loss function as a function of theta. Now what's happening here on the left hand side makes sense: as theta gets more and more negative, our loss goes up as well it should, because we shouldn't have a negative theta here, that would be like flipping this around. But now, what we really want to say is that as theta gets way too positive, the loss should also go up, because that's making it too steep. But instead what's happening, it's a little bit hard to see because it's so small on the right there, but what's actually happening is that the loss continues to go down as theta gets larger, no matter how large theta gets. And again, that's for exactly the reason we just described: as theta gets larger and larger, that's just making all the probabilities higher and so absolutely that's going to make the loss, you know, get lower but we don't want that, we don't want that to happen. We kind of want to express that somehow a humongous theta is just too competent, like you would need a lot of data and information to get past that level of confidence that we're not comfortable just asserting that level of confidence.

And the way to do that, to assert that there's such a thing as being too confident and we need data, like a lot of data to overcome, that is a regularizer. Okay so let's see what we can do to get beyond this. There's an interesting question: “is this what epsilon is for?” There's actually something called early stopping, which can also play a form of regularization, but let's not get into that just now. Actually we're just going to focus on what happens when we add a regularizer to deal with this, but there are actually other forms of regularization than the one we're talking about and we'll probably talk about some of those later on. Okay but we have one idea right here, which is we want to say that too big theta is bad, and our way of saying that things are bad is our loss function, and so how can we modify our loss function to incorporate this idea that a theta that's too big is being too confident and it's bad? We can do that, again, by adding in a regularizer. So here was the logistic regression loss that I showed you so far and here's the full logistic regression loss with what's known as an L2 regularizer, it's a particular type of regularizer. It basically has lambda, which is some constant that's greater than equal to 0, times the size of theta. Now previously it's not like we weren't showing logistic regression loss, we absolutely were, we just showed it for lambda equals 0, so this is like a more general version of what we looked at before.

Okay so this is known as a regularizer, or sometimes a penalty. It's something we add to a loss and it's usually to express some information that we have that isn't purely expressed in the loss. In this particular case, it's to express that we think that a really large theta isn't natural, we shouldn't just naturally be that confident, that we need a lot of data to overcome that. And so here, that's exactly what we're saying. We're saying if we have a really large in magnitude theta, because magnitude's the relevant thing here, that we really need a lot of data to choose that, we can't just choose that for fun and so whenever we have a really large magnitude theta, that's going to increase our loss.

In this particular case, this exactly penalizes being overly certain, because if we just increase the magnitude of theta here without doing anything else, that's like saying we're more certain, that's increasing that steepness that we saw. Now something that you can check is that the subjective is still differentiable, so we can still run gradient descent, and it's still convex, so we're going to have nice properties of gradient descent. Now something you might ask is “why didn't our gradient descent theorem apply before, because it didn't, there was no global optimum that it was going to because there was no global optimum.” So in particular, when we looked at this lambda equals 0, because this is always decreasing, there is no global optimum and that was one of the assumptions of our theorem. But now, when we include our regularizer, there is a global optimum and so now our theorem will actually apply and we can say something about actually getting to that global optimum which is nice.

Now this seems preferable not only because the theorem applies, but also because we're going to have this optimum that we can get to and we're not going to just choose this sort of arbitrarily steep curve like we would have without it. Now, we can choose different lambdas and we'll get different losses as a result. So in particular, something that you'll notice is that, as we increase lambda (you can think of lambda sort of a trade-off between the loss and the penalty), as we increase lambda, eventually if lambda's big enough, the penalty is what's going to dominate, we're just going to have a quadratic. But if we decrease lambda, then the loss that's what's going to dominate and you can really see here that we start from this loss with lambda equals 0, we get something that looks more and more quadratic as we increase lambda. In fact, if we kept doing that we would get something that really looks even more quadratic.

Now a question that has come up a few different times now is how do we choose these hyper parameters? You know, now we have this lambda that we have to choose, how could we possibly choose it? And this certainly is not the only hyperparameter that we have encountered, we've encountered, you know, how do you choose the size of the polynomial basis if you're using something like polynomials for your features? You know, how do you choose, even in our random algorithm for the very first day, how do you choose, you know, how many different things that you look at? The k, that was the number of things that you look at. Hyperparameter selection, in general, is an interesting question just as parameter selection is an interesting question. You can, you know, think of it much like that, you know, it's called a hyperparameter for a reason, it's sort of like just the next level up parameter. But one idea and one that's worth thinking about is to consider a handful of possible values and compare them with cross-validation. I see there's a question.

You just answered it. The question was: “how do you usually, how big do you usually do that before the regularizer?” Awesome great, yeah and something that I would recommend too is, so this is one way, just as we are talking about many learning algorithm, it's not the only way, but it's certainly a very popular way is to like maybe try out a few orders of magnitude and compare them with cross validation. Another thing that is definitely worth doing is checking the values you get out of the end, like take a look at the loss function, see if it's a reasonable loss function. Like in this case, if your lambda was such that it totally dominated, you know, if your whole loss function was basically a big quadratic in the penalty and you lost all the interesting stuff that was going on in the classification loss, the negative log likelihood loss, you might be a little bit weirded out, you might think “hey am I really doing classification or is it just the penalty that's driving everything?” And so it's really worth, you know, once you've chosen a lambda, thinking about it and really, you know, plotting it, thinking about what it means relative to your loss, figuring out what it really means in your problem.

Okay so now we have all of the tools for logistic regression at our disposal: we have the loss, the full loss including penalty that describes logistic regression, we can do classification with it, we can run gradient descent applied to logistic regression, and we can put it all together and this is something that we can run on data. And so let's just write that out. I just want to emphasize how much this is literally just gradient descent with f given by the logistic regression objective and so if, you know, gradient descent and, you know, the logistic regression objective, you can just write this out and you should. You should check that you can derive that from just those two things because this isn't like its own special thing, this is just, you know, applying exactly that. So here is a logistic regression learning algorithm based on gradient descent. I do want to emphasize that gradient descent isn't the only optimization algorithm that you can use, in fact, we're going to see another one pretty shortly and have in our reading and so this is logistic regression with gradient descent, it's not the only thing you can do for the logistic regression objective. Okay so let's look at logistic regression with gradient descent, it's going to take initializations, it's going to take a step size parameter, and it's going to take a stopping parameter, hyperparameter epsilon and hyperparameter eta. It doesn't take in the function f now because that's defined by logistic regression.

So we initialize, we start at particular values of the parameters, the theta in it and the theta naught in it. So those are just initial values. We have our counter of what step that we're on,  that's our t = 0.

And we're going to repeat the steps of gradient descent just applied specifically to logistic regression. Okay so this looks pretty crazy. I think if you just saw this, you'd be like “whoa math, what happened?” But the reality is this is literally just you take the logistic regression objective and you take the gradients and you just stick them in. And so if this looks like a lot, literally the best thing you can do is just go home, sit for a moment, and derive what are the gradients in logistic regression and just make sure that this just pops out for you, that this is exactly what you get. So again it looks like a lot, but it really, it really isn't: it is just the gradients for logistic regression. And remember, we apply the penalty to theta to make sure that we weren't being super overconfident which was respected by theta, which are expressed by theta, and so we see that that penalty is showing up in our update for theta but not for theta naught.

Okay and then we keep going until we're not changing that much and again every single one of those stopping criteria that we considered was basically expressing not changing that much. There's a question: shouldn't this take lambda as an input? You could think of lambda as an input, either way. Yeah I think that's that's a totally reasonable thing. Here I happen to be thinking of it as part of the logistic regression objective and I'm not bothering to put that in, but yeah you could absolutely think of that as an input to this algorithm as well as a hyper parameter specifically for the algorithm. Okay so here we have our stopping criteria, this is a particular stopping criteria just based on the objective function not changing too much, and then finally we return the theta, the thetas, that we have learned along the way. Okay so again, just gradient descent applied to our logistic regression objective, that's all that's happening here. It again looks like a lot, but it's not when you go in and derive it, this should be just that application. But the reason that I've written it out, is that there is something that is interesting, that we want to understand here and that's the following: so let's look at one of these steps. Let's look at, for instance, the theta t step what's happening in the theta^(t) step. So in the theta^(t) step, what we do is we take all of our data points and we calculate something whatever's in that sum and we do it for every data point and then we sum up over all the data points, and once we have done that sum over absolutely all of the data points, we make an update to theta^(t). Now this is pretty different from what we did with the perceptron, for instance, where we did something to one of the data points and then we immediately made an update, and there's something to be said for that. Like let's say that you have a data set that's in the millions, that's in the billions, then this is just going to take a really long time, you're going to sum up over all of those millions or billions of data points and then finally you're allowed to move in gradient descent. And it feels like if you look at one data point or even like 10 or 20 or 100, you probably already have some information about a good direction to move, that you don't need all of the data points to figure out a roughly good direction to move, to go in the direction roughly of the gradient and to minimize your function. And so actually, there's a different learning algorithm. So we talked about gradient descent as an optimization algorithm, there's a different optimization algorithm called stochastic gradient descent, that's in your reading for this week, that takes advantage of that, that actually says we only need to look at a few data points before we make a move or one data point even. There's a question: so for the gradient to step why, isn't there a 2 lambda theta. Oh maybe that might actually just be a typo, yeah I think so.

Um I'll check that out, but yeah I agree with you, good taking of derivatives on the fly. Great, somebody was gonna find one at some point and so I think we've done well in going this far but that's fantastic. Okay great, so these are exactly these gradients, even with a two in here, my point holds about you have to make this sum up over all of the data and that's just going to take a really long time and we might be able to be more efficient, we might be able to take steps before we have to do all of that, and that's the idea of stochastic gradient descent, to be able to start making those steps before we've seen the whole data. Okay, so just to recap: what have we done today? We introduced logistic regression. If you are going to use a classification algorithm in person, in reality, this is really one that you will really use. This is going to come up and, you know, probably any data analysis that you do in your life, there's a good chance that this will be in it. We saw how gradient descent can let us use the logistic regression objective and optimize it and get very very close to an optimum. It turns out in order to have an optimum, we have to be careful and we might have to have a penalty. So we covered all that today and we're going to move on to, I believe, regression on next week, so I'll see you then.
