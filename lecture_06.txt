Good morning, let's get started on today's lecture. So in our previous lectures, we've been talking about all of these different hypothesis classes and how to learn them. In particular, we talked about linear classification and linear regression and we also talked about how you can get these really interesting boundaries and classification and really interesting regression curves by choosing different features, by choosing not just sort of our default x_1 and x_2 and x_3, but actually having different features. And we're going to sort of explore more of that today, we're going to look at a potential set of features that we could use based on step functions, and that'll lead us to discussing neural nets as a hypothesis class and learning in neural nets analogous to what we did for both linear classification and linear regression. Okay so let's just review briefly what we did with linear classification, with the default feature. So when I say default features, I mean just x_1, x_2, we haven't applied any additional transformations here. Okay so we have a bunch of data and what we saw in a previous lecture is that a way that we could approach classifying this data, is we could look at a certain set of functions in a slightly higher dimensional space. So here we have x_1 on the sort of east-west axis, we have x_2 on the north-south axis and this additional z on an up-down axis, and so we could make this curve, this theta transpose x plus theta naught which is linear in the thetas and also, in this case, happens to be linear in the x's, and we could ask ourselves: for any particular theta and theta naught, where is this curve below zero? And that will give us one of our labels, maybe it's -1. And where is this curve above 0? And that's where we'll apply a different label, maybe it's +1. And so, in general, any particular theta and theta naught defines such a curve, which happens to be a hyperplane, and therefore defines a set of predictions where we apply and predict +1 and where we predict -1 and then what we'll do is we'll say “okay well that's a huge class, with all kinds of different thetas and theta naughts, but we'll try to get as low as possible loss to figure out which one will apply to our particular data.” Okay so we saw this picture before and now we've also seen another picture, we saw in the features case, that we could do this and we can get these more complex non-linear boundaries here as well. So, in particular, one way that we could do that, we saw, oh and of course again just reiterating, we predict one particular label on the side where this is greater than 0 and a label on the side where this is less than 0. Okay so now we can get these more complex non-linear boundaries with, for instance, different types of features, like one example we saw was polynomial features. So with polynomial features we said “hey, we have these different features like phi_1 is now x_1, phi_2 is x_2, phi_3 is (x_1)^2 etc. And so maybe I have a bunch of data and maybe I don't think that there's a really good single linear boundary here, so I might use my polynomial features and so we saw we could do the same thing: we can make this curve that is still linear in theta, so we might still call this linear classification, it's just non-linear in the x. So we have this still theta transpose, but now times phi(x) instead of x plus theta naught and we're going to predict a label, say -1, where this curve is below 0 and a different label, say +1, where the curve is above 0. So it's the same thing, it's just now a more interesting, more complex curve. Okay so again, we get a curve like that for every possible theta and theta naught, but then what we do is learning. We say “hey, let's try to get as low of possible loss, so we'll use that to choose our theta and theta naught and that's what we'll apply to our data and then in particular used to predict at new points.” Okay and so basically, what we're going to be doing today is we're going to be thinking about other possible features we could use. There's nothing that says we have to use polynomial features and, in fact, maybe something that we might ask ourselves is: do we have to have a fixed set of features? Maybe we could actually learn good features, maybe the features themselves could have parameters that we learn. And so that's essentially what we're going to be going into a lot of depth on today. Now before we do that, let me just go back to this picture here and let's just notice that what's happening here is we're essentially applying a function, to apply this hypothesis, to apply this prediction. For a particular theta and theta naught, we're predicting one label above a line and another label below the line and so we could actually plot that function. So here's that function where we have one value on one side of the x_1, x_2 line and another value on the other side. So, for instance, if we were predicting 1 and 0, the upper value here in the up-down axis would be 1 and the lower value would be 0. Okay and so, in particular, another way that we could represent that which we saw in a previous class is that we could use this indicator function. Remember the idea here is that whatever's inside the parentheses, we're gonna return 1 if it's true and 0 if it's not. And so what we're doing here is we're actually plotting this indicator function applied to this linear boundary. So we see that it's 1 on one side where this is true, and it's 0 on the other side. Okay so this is called the step function where we have 1 on one side and 0 on the other side. We're used to using step functions to classify—we've been doing this for essentially many classes now—and the really big difference about what we're gonna do today is we're going to start using step functions as features. So we're still going to use them to classify, but we're going to use them as features as well with their own parameters.

Okay so let's go ahead and check that out and do that and see what happens.

Okay, so we have some new features, they’re step functions. So because these are going to be features, let's give them their own parameter names, like let's say maybe w transpose and w naught instead of theta transpose and theta naught, which is what we were using for our classifier. And so this is a feature, so I'm going to call it phi_1. It's going to be our first feature and it's going to be some function of x because we've seen that that's what features are: they're just functions of x, the original sort of default features that we got in the beginning of the data. Okay so let's draw this. Well depending on my choice of w and w naught, I'm going to get something that looks like this and it's just that breaking point, that line, that's going to be different depending on how I choose w and w naught, but this is a feature, it's a function of x, it's something that we can use as a feature, and it's got this step function property.

We can make another one of these and we need to give it its own new parameters, so maybe let's call them w tilde and w naught tilde for the moment. And again that's going to define where we break and on what side we get 1 and on what side we get 0. So this is just another feature, it's just another function of x. Any function of x could be our features, we happen to be choosing these at the moment, and so now we have two features, we have two functions of x that we can use as features, and so let's ask ourselves: what would classification look like with these features if I were to use these features for classification? Okay well, we've been doing linear classification, let's do linear classification again. And what I mean by that again is that we're doing something that's linear in the parameters, in the theta and theta naught. But now we have these interesting features, the phi's. In this case we have phi_1 and phi_2 and so this function that I've just written down, the z equals theta transpose phi of x plus theta naught, this is nothing new, this is what we've been doing with classification this whole time. We've considered polynomial features, we've considered other things, the thing that's new is: one, our new step function features. We haven't considered step function features before. And two, our features themselves have parameters, that's a little bit new, we have not done that before. Okay so let's see what is this going to look like. Well in our particular case, we're looking at two features and so if we write out what does this dot product look like, there's going to be two components: there's going to be our theta_1 times phi_1 or theta_2 times phi_2 plus theta naught. And then. just so I can plot this, I'm going to choose some particular values of the thetas. That'll be a particular hypothesis. Whenever I choose all of my parameters to have a value, that's one hypothesis. Later on, I'm going to try to learn those parameters, but here I'm just trying to plot what do the hypotheses even look like. And so here's just an example set of values that I could use to plot my hypothesis. Okay and so once I have those values, I can go ahead and plot a hypothesis and here we go. So this is my hypothesis, based on these two features and this particular linear combination of these features, and again, just as before, what's happening is we're having these labels. So we're applying a label of 1 somewhere and a label of 0 somewhere and we can find out what those are by looking down from above. So if we look down from above, along that z axis, we're gonna see where we apply the different labels. So in one region of the space, in fact on two different sides, but wherever you're seeing that check pattern, we're getting that we're gonna get a label of 1 in this case and in that in-between part, we're going to get a label of 0. So it's just like before where we say “hey we have this function, we're going to ask where is that function above 0 and where is it below 0,” that's where we're going to get our label of 1 and our label of 0 respectively. And what's different now is just we get a different type of function. This isn't a function that we saw when we did vanilla linear classification, this wasn't a function that we saw when we did polynomial linear classification with the polynomial features, this is a new function for a new set of features and it actually might be pretty useful. So just as an example of where you might want this kind of classifier, let's think about exoplanets. So exoplanets, again, are these stars that are outside of our solar system or sorry, they are these planets that are around stars that are not in our solar system. And so one thing that we might want to ask is: which exoplanets are habitable, which ones could could life actually exist on? And if you look at this, well, there are a lot of different sizes of stars that exist out in the world or out in the universe and for each of those stars, if as the star gets bigger, the habitable zone is going to be sort of farther out, because if you get too close to the star, everything's going to be too hot, if you get too far away from the star, everything's going to be too cold. You want to be in that perfect little goldilocks area and so this kind of hypothesis can express that, that as the size of the star increases, there's this nice little band of distance where things aren't too hot and aren't too cold and maybe life could exist on this star. And so from that perspective, this is a useful hypothesis to have. You know, we wouldn't want to just use a linear classifier for this because we wouldn't be able to say that there's that nice little band that's separate from everything else. Now we have a little bit more flexibility that we didn't have before with just a linear classifier. Now we have the ability to have two different divisions and to say something's happening on either side of them. Okay so this is one particular setting of these weights in the features and one particular setting of these theta and theta naught in the classifier but let's see what would happen if we changed those settings just a little bit. Remember the idea of hypothesis classes is that we can have a lot of different shapes that we can get by changing our parameters and so let's just change our parameters a little bit and see what else might happen. So in particular, let's look at this first feature: what would happen if I change, say the w and w naught? Well I'm gonna move around the line that defines where I'm labeling 0 and where I'm labeling 1 in this feature. Well if I do that, then that's gonna change my plot down here because it's still this linear combination of feature 1 and feature 2. Instead it's going to look like this. So I've just changed one of my features and that therefore changes my prediction here and now when I look at it from above and see what gets labeled 1 and what gets labeled 0, it's going to look different, I'm going to get a different kind of boundary.

Now we can do this with the other feature too. So here's another feature, here's our second feature. We could change the weights in the second feature. Remember we've seen this sort of thing before, that the normal and the offset defines a line, and so if we change those values, we're going to change where that line is and so we can do that too. So here I just happen to have changed it to a different set of values. And now if we keep the same weights in our final classifier, we're going to get something different. Now what happened here? In this case, you'll notice there was some overlap of the two features and because we're just adding them up, we're going to get a value of 2 instead of just a value of 1 or 0 and that's fine. I mean, in general, we expect that we could have any values here for this linear combination, but in the end of the day, if we take this step function perspective, we're still going to look down on this and we're going to ask “where is this function above 0 and where is it below 0?” to make our classification. And so this is what our final classifier is going to look like, our final hypothesis is going to look like: it's going to predict 1 in the sort of upper corner, it's going to predict zero in this lower corner. And again, this is a potentially useful classifier to have. Just as an example, maybe I'm trying to decide when do I go for a run. I want to look at the temperature, maybe the precipitation, and decide—and from here you can sort of see, maybe x_1 is precipitation on this horizontal axis—and I might say to myself: if it's snowing or raining too much, I'm just not going to go for a run and I don't go for a run. Then conversely, if it's way, way, way too hot outside, I'm not gonna go for a run and so we're seeing that as well here. And so this is again something that I couldn't get necessarily from a straight linear classifier with just the default features as we saw before, that I need something a little bit different and here I'm able to express this. Now what if I also don't go for a run if it's too cold? I don't go for a run if it's raining or snowing too much but I also don't go for a run if it's either too hot or too cold. I'd like to be able to express that, I'd like to be able to express that in my hypothesis. So how could I do that? Well I might add in a third feature. So here, so far, we have two features phi_1 and phi_2. They're both step functions, but there's nothing that stops us from going further and making yet another feature. So we have the two so far, let's go for a third. So here I'm just adding yet another step function. I'm running out of notation so I'm calling it w tilde tilde and w naught tilde tilde. This is clearly a problem that we're going to have to resolve and we're going to resolve it on the next slide but bear with me for the moment. It's just another step function, it's just another line with one value on one side and one value on the other side. And now I want to ask myself: how does this change once I have this new feature, how does this change the function that I create with linear combinations and then my final classifier that I can make from that function? Okay well let's start with this function I make with linear combinations. In order to look at this function, I'm going to need to change what I was doing over here because I only have two features over here. So let's look at this first line, the first line is generic. This is for any number of features, it's going to work out because it's just the features as a vector. The second line is the problem: it's only got two features in it so far, we're going to need to add a third feature and so this is really what that first line is going to look like when we have three features. So all I did was I added in that theta_3, phi_3 term that wasn't there before. So this is really just writing out that dot product from the line above but now with three features instead of two. Okay, so now let's choose some particular values just so we can plot this, because we're going to need some particular values in order to have a particular hypothesis. So here I had some values for two features, I'm going to add in another weight for a third feature. It's pretty arbitrary, I'm just choosing some weights here so we can see what do these hypotheses even look like. So if I do these weights now I can plot what does my function look like, the function defined right there on the left, it's just a function of x and here we go. So now we see that there's multiple values that we can get but again some of them are above 1 and some of them are below 1 and that's what we were doing with our classification for polynomials, that's what we were doing with our classification for just vanilla features, and that's what we're gonna do now, we're gonna ask what's above 1 and what's below 1 and so we're gonna get the following hypothesis for classification: I'm not going to go for a run if it's raining or snowing too much, I'm not going to go for a run if it's too hot or it's too cold, but if it's in that nice little middle place where it's not running or snowing too much and the temperature is just right, I'll go for a run. And so again, this is the kind of hypothesis that we couldn't have expressed if we just had our traditional no nonsense, not changing anything features, we just had our x_1 and our x_2, but now that we've created these more complex step function features, we're able to say this sort of complex statement about going for a run. Okay yes there's a question. Please. Could you say that it's more confidently predicting a 1 if the graph is higher above 1 or... Oh yeah that's a great intuition. So this idea that we kind of feel like it should be more confidently predicting a 1 if the graph is very much above 1, versus if not. It's sort of saying like “oh here are all the reasons I wouldn't go for a run, you know, it's raining too much and it's too cold” and so it's like, wow, if all of those things are true, I'm super not gonna go for a run and that's such a great intuition and that's the kind of thing that we can express if, instead of a straight stem function we had a sigmoid, because a sigmoid is basically telling us exactly that kind of thing. Remember like when are we most likely to go for a run and when are we least likely and so we're going to see that later basically. So right now we're just saying “hey am I going for a run or not” but absolutely that's great intuition and we're definitely gonna go there later in the lecture. Great. Okay cool. So right now, we have this set of hypotheses but we were running into an issue which was namely that I was running out of tildes, like the notation was just getting way out of hand. At some point, I can't just keep adding tildes to my weights, I'm gonna need to, you know, what if I want four features? What's going to happen then? What if I want four of these step functions? And so let's just take a step back now and try to develop a much more general set of notation for the idea that we've just been talking about, basically the idea again being that we create a whole bunch of step functions, we're going to use them as features and then we're going to put them into a classification problem. Okay so now we're just going to spend time on that notation. So let's get some new notation, again just for exactly the thing that we just did.

Okay so the first layer was constructing the features, there were kind of two layers that were going on here: we first constructed some features, then we put them all into a linear classifier. Okay so let's talk about constructing the features and let's develop some notation for that same thing we did, just with some new notation. Okay our input is going to be a data point, so just as before, we said we have these phi(x), they take a data point and they output our features, we're going to have our data point and we're going to output some features. So just as an example, maybe our data exists in two dimensions and this could be our data point that we would use an input, this little star, in the two dimensions.

Okay now something that we're going to do is we're going to have this general notation for each layer, so the superscripts on this page are going to represent the layers. So in this first layer, we're constructing the features. We're in the first layer so we're layer one and we're going to say that the number of inputs to layer one is m^(1), so that's just going to be a notation that we use. But in this particular example, in this drawing that I have of x right here, can somebody tell me what is m^(1) as a number? What would the size of the data point be? So this is for the private chat.

Great lots of great answers here.

Okay so the answer here in this particular drawing that I have on the right, this is a two-dimensional data point. So we're seeing that it has size—we've been thinking about these things as column vectors—so it has size 2 by 1 and so m^(1) here is 2 and the size of the data point is 2 by 1. Okay, great. Now we've called this something different in the past and so I'm just going to point that out: we've called this d in the past, so m^(1) is just equal to d, the d the dimension of the data that we talked about before. Okay now our output in this layer is going to be the feature values. So let's just go back to what we had on the previous slide only in miniature. So these were our three features on the previous slide, they're each a function of x_1 and x_2, although I've only labeled those axes in the third feature, but each one is a function of x_1 and x_2 and so what A^(1) is going to be is it's going to be those features evaluated at x. So, if I take my particular x—so this is just the same x that I have above here, it's got a pretty low value of x_1 and sort of a mid value of x_2—and I ask what's phi_1(x) at that x? Well it looks like it's 0 in this case. In this particular case, phi_2(x) also looks like it's 0 and phi_3(x) looks like it's 0, but it's basically what we're going to do is we're going to evaluate our features at this particular x and that's how we're going to get A^(1).

Okay so we get these particular values, we line them all up into a vector, that's going to be A^(1).

And so that's going to have size, we're going to say that the output size is a column vector of size n^(1). And so in this particular illustration where we have exactly these features, this is another question for the chat, what's the size of n^(1) or what exactly is n^(1), can you tell me what n^(1) is? Again just for this illustration that we're showing here.

Okay great we're getting a lot of threes. Just for this illustration n^(1) is 3, so basically the idea is we're going to take in data of whatever data dimension we have—in this particular illustration, the data dimension was 2—and then we're going to output a set of feature values. In this particular illustration, the number of feature values is 3. Now A^(1) itself will be those values, it just has size n^(1) by 1, it's a column vector. Okay so how do we get these feature values? Well we apply our feature and so you'll recall in the particular case of a step function, we had some function f^(1), that's basically the step function, and then we applied it to some linear function of the x. And so here, I've gotten around this issue of okay well I was running out of tildes with my w’s by just saying “hey, I'm gonna give a different w to each one of these problems.” Ah, I see that there is a small typo here, the w naught should also be indexed by i. I will fix that for the online slides when I show them later but basically we're going to have a set of w’s, so that's both the offsets and the normals here that we're used to, and we're going to have one of those for each of the features, so we're going to have a (w_naught)^(i) and a w^(i) as well. And then the superscript again, everywhere here the superscript just means we're in the first layer, we're constructing the features. So that's what that superscript 1 means. Okay so in particular, if we were looking at i equals 3, we're constructing the third feature. So here I'm focusing on the third feature, that's i equals 3, and the way that we would do that is exactly we would construct our function of x in the following way: So this is phi_3. This is the phi_3 function of x for this particular step function feature and then we would say “hey, let's evaluate at this particular x to get out our (A_i)^(1).

So in this particular case, if we're looking at i equals 3 and we evaluate our feature and we get out our (A_i)^(1), what is the value of (A_i)^(1)? Again, remembering that this phi_3 has two values: it's either 0 or 1, 0 is below and 1 is above.

So what is (A_i)^(1) here? This is a question for the private chat again.

Okay great, so the answer is 0 in this particular case and the reason that we know that is we look at that particular x, that's that little star, we evaluate it at phi_3(x) and we see that at that particular x here, the value is 0. There were two choices, 0, 1 and this one happened to be 0. Great, so lots of great answers on that one.

Okay so now, though we actually want to do this multiple times, we don't just want to do this for one particular i, we want to do this over all the features i, and so we're going to do all the features at once and put them into this vector A^(1).

And that's going to look like this. Now, let's talk carefully through this equation because there's some weird stuff going on here. So first, W^(1) here is collecting the (w_i)^(1)'s, so it's going to be an m^(1) by n^(1) matrix and it's just collecting all of the (w_i)^(1)'s that we had from those i features. Similarly, w naught is collecting all the w naughts. So I made this typo, it should be w naught i in that i feature area and so this would be collecting for each of those i’s, the w naught ones. Okay now here's where the weirdness comes in. This thing is going to be a vector.

So you should check using the kind of tools that we used in previous lectures what is the dimension of that vector. You can get it from the W^(1)s, the size of the W^(1)s, the size of x, the size of (W_0)^(1). I'll tell you, I mean you can kind of see just by looking at (W_0)^(1), if I'm going to add (W_0)^(1) to it, this has got to have the same size as (W_0)^(1). And so this whole thing is going to be n^(1) by 1 and what's weird about that is we've never really applied a function to a vector before, that's not something that we've been doing in this class, we've always applied it to a scalar and when you apply a function to a vector that could mean a lot of different things and so I want to be really clear what does this particular usage mean. It means we're going to apply it element-wise. That doesn't always have to mean that, there are different ways to apply functions to vectors, but here that's what we mean by this. So f^(1) is applied to each element in that vector to get A^(1) out so this is really, you know, this A^(1) equation with all the features at once is just a shorthand of writing what we did for the i-th feature i times or n^(1) times for every i.

And that's really the story of this slide and the notation of neural nets, in some sense, is that it's all this sort of shorthand that seems laborious at the time and is like kind of a headache but ultimately will make our lives easier because we're going to have too many things floating around to have the notation that we were using before.

Okay so that's the first layer where we constructed the features. Now let's go to the second layer where we take those features and we assign a label or potentially multiple labels. So this is really kind of like what we were doing before in previous classes, like just doing linear classification with different features and so we're just going to do that again only with like slightly different notation. Okay so let's do this. So first, in this layer, the input is going to be the features, just like if you were doing linear classification with polynomial features you would take in those polynomial features. So now we're going to call the size of any layer m super that layers index by one. So in the first layer we call the m^(1) by 1. In this layer, we're going to call it m^(2) by 1. Is m^(2) equal to anything else on this slide? Could you say in the private chat: is there something that m^(2) would be equal to? It's basically...

Great yes it's the number of features which we just said was the output of the previous layer which we just said was n^(1). Perfect, great observations here. Another good one is it's the dimension of A^(1). Totally another great way to phrase this it's the number of elements in the vector A^(1), so these are all correct. Good stuff. So I'm just going to write that explicitly here: m^(2) is the number of inputs to this layer, so it's got to be the number of outputs in the previous layer because we're taking the number of features and in this particular illustration that we have on the side, again that's going to be three because we have three features.

Okay, so now here, our output is going to be the labels. Now in everything else we've done in this class so far, we've had a single label. We've said, okay, are we classifying this as an exoplanet in the habitable zone or not? Are we classifying this as I'm gonna go for a run or not? Are we classifying this as one thing or the other? And so this is a little bit different potentially than that and that we could have multiple outputs. So in what we just saw in the illustration, that we saw in the previous slide, we were just having one output. We were doing this classification of am I going to go for a run or not, so that's either one or zero, and so in this particular case our vector of labels is actually just a 1 by 1 vector, there's nothing to it but you can imagine that maybe I want to say multiple things: I want to say am I going to go for a run based on the precipitation and the temperature and is my best friend gonna go for a run based on the precipitation and the temperature and is our neighbor gonna go for a run based on the precipitation and the temperature? And if I asked all of those questions using the same set of features, then I would have a vector of labels here. Incidentally, I'm going to call this whole function that starts from the data, goes through the first layer, and through the second layer “NN” for neural net because we're basically building up a neural net which hopefully is something that you have read about in the reading before today and and did your exercises and we'll talk much more about this wording of neural nets but basically we're just building up a complex function here and it depends on these parameters W. Okay so finally we're going to get that label and what that label is is we're just evaluating that complex function that we're building up at a particular point. So we put in that point to get our feature values, the A^(1)’s, now we put in the A^(1)’s to get our final value, A^(2), maybe final values but in this particular example it's just 1. And so this is gonna have a size, we'll call it n^(2) by 1. That's the number of outputs but again typically n^(2) is just going to be 1 because you're going to be doing one classification or later we'll see you might do one regression.

Okay and so again we have the same idea that we're going to apply our, in this case, classifier to get the i-th label. So for instance, if we have a single label, we apply the classifier in the following way and this is our typical classification, this is the kind of thing that we've been doing for a while now and the only thing that's changed essentially is how we express it in notation. So previously, we had theta and theta naught. Now, we have (W^(i))^(2) because this is the second layer and again unfortunately I made a typo. This is (W_0)^(i). It depends on which output you have and so you're going to have a (W_0)^(i) again super two because this is the second layer. So that's why we have that two superscript and then again we might do this fancy thing where we collect all of them together at once by applying this component-wise f, which in the case that we've been looking at so far, it's been the step function but we're going to see some alternatives. So we apply this component-wise f and we collect everything in these W^(2) and (W_0)^(2) matrices, very much analogous to what we did in the first layer. Okay so the whole idea here and I think the important thing to take home from this, is that all we've done is constructed a really complex hypothesis. So remember: what is a hypothesis? It's just a way to go from data to a label and it depends on some parameters and that's exactly what we have here. We have a function that goes from our original data, through to some features, and then finally to a label and so what I'm plotting here is that whole function, that function just takes in data and outputs a label. And here we're sort of not seeing the hidden calculations along the way, we're just seeing here's the final output. We can see that, as a result, this is a pretty complex, interesting function. And the other important thing to note is it depends on the choices of the parameters, the w and w naught, so just like in the past our hypotheses for, say, just linear classification with default parameters depended on the choice of our parameters here. We get the same thing if I choose different parameters, if I move the parameters around I get a different hypothesis and that's what's let us do learning. You know, ultimately what we want to do is we want to say “hey there's a lot of different hypotheses for a lot of different parameters” and when we get a particular set of data, we're going to ask: well which of these hypotheses has a particularly low loss? Can we minimize that loss? And so what we're doing here is really just building up what even are these hypotheses, what do these look like and in particular for neural nets.

Okay so this is a big old slide of notation which I think is the kind of thing, you know, all notation looks weird when you first see it and you you get used to it by working with it, by working with it a lot, and so that's what you're going to be doing in your labs, your exercises, everything that you're doing, you're going to be getting really familiar with this notation and it'll hopefully become second nature, it'll become something that you're more used to, but certainly it does feel like a lot in the beginning. Okay, so now what we're going to do, is we're going to find another way to express the same idea which can also be a very useful way to express the same idea and that's with a function graph.

Okay so again, let's just reiterate: so there's a couple of key things going on in this slide and I'm just going to extract them. This is basically the whole slide. We had the first layer where we constructed the features and we output the feature values and we had the second layer where we took those feature values and we turned them into labels and then we said this whole thing, we'll call this a neural net where NN, and it's just a big function that takes in our data and outputs some labels A^(2). Okay and now what we're going to do is we're going to express the exact same idea, this way to express our function as a function graph. Again, we're just sort of describing our hypotheses at this point, we're saying “what do they look like” and here's just another way to describe them. And I think it's useful to have these multiple descriptions because we're getting into these pretty complex hypotheses and so we want to be able to understand all the little components that go into them and to understand how does this work. So let's look at this idea of a function graph. Okay, so the first thing we're going to do to build up our function graph is we're going to look at what does a function graph look like if we're constructing one feature, let's say the i-th feature in the first layer.

Okay it's going to look like the following: so we have our inputs x_1, x_2 up to x_(m^(1)) because we said that that's our cool new way to describe the number of inputs, but m^(1) is just d, it's just the d that we saw from before. Now we see this circle. What does a circle mean in a function graph? A circle means that a function evaluation is happening. So this first circle, where all the x_1s are going, x_1, x_2, x_3, etc are going, into there's this little summation symbol and that essentially represents this dot product plus adding w naught, because the dot product's just a sum. If you think about it, you're really just summing over the elements of that vector and then we're adding in the W naught and so we're doing that whole dot product and we're getting out an output and we can call that Z, we could call that output Z. This is basically the thing that's going to go into our function f^(1).

Now why doesn't Z have a circle around it? Because there's no function evaluation going on there. So we have a circle around the sum because we're taking all those x_1’s and doing something to them and out making an output. With Z, that is a value and now we're going to take that and put it into a new function f^(1) and because with f^(1) we're doing a function evaluation getting an output, that's going to have a circle on it. Again and so now we do that function evaluation, we take this Z and we put it into f^(1). So again, Z is just this W^(1) transpose x plus W naught for the i-th component and then we get out this (A_i)^(1) by applying f^(1) and this is representing that component wise evaluation that we talked about from before. So, for particular, for the i-th value for the I feature, we have a particular value, a scalar (Z_i)^(1) that goes into f^(1) and then we evaluate (A_i)^(1).

Okay and now if we want to look at the whole first layer, it's just doing this a bunch of times. So this is exactly what I drew for one particular i, and now we have the same thing for i equals 1, i equals 2, i equals 3, up to i equals m^(1), because again there were n^(1) different features. Now something that's kind of interesting about this, though, is I didn't just repeat the same graph over and over, you'll see that the inputs are shared and so that's something that this graph is expressing is that it's not just a total repeat of the graph again and again, we have the x_1, x_2 up to x_(m^(1)) going to every single one of these summations because the inputs are shared by every feature.

There's a question in the chat: shouldn't there be a W naught going into the sum? That's a good point. So here I actually haven't really represented the W's, so something that you'll be familiar with from seeing some of your representations in the course and in the reading is W naughts and W's. Here I just haven't even shown them and so if I were showing the W’s, I would definitely want to make sure that I have the W's and the W naughts. I guess I'm kind of just avoiding even showing them on this, so they're implicit in this particular sum, but that's a good point. It's worth noting that really you are using the x's and the W's going into this and it's sort of, you know, here I'm kind of using it the same way that you know x's are the inputs and W's are the parameters. Like they're still inputs but we kind of treat them as like different inputs and so... But yeah, but they definitely are still inputs and they're definitely going into every one of these sums and it's a different W set of W's going into each sum.

Okay great. So now we have these features, A_1, A_2, up to A_n, and what I want to do now is I want to look at the second layer, how we're going to represent the second layer on this function graph representation. Well it's going to be a similar deal. Now our inputs into the second layer are going to be the features from the first layer. I'm going to write a second layer where there's only one output but there could be more outputs and if there were more outputs, then we would have the same features for each one of those summations.

Okay so we have our first layer, we have our second layer, and then the whole thing—you know, each one of these individually was a set of functions—the whole thing is one giant function: it takes in a bunch of x's and it spits out, in this particular case, one label at the end. It could do more labels, one label is pretty standard. Okay so now we're just going to make a few notes about this function graph representation. The first note is that it has directionality, there is a notion of going forward and a notion of going backward in it. Something you'll notice is that there's a direction defined by going from the inputs to the outputs and all the arrows accord with that direction and that direction is what we're going to call “forward.” So forward means going to the inputs, from the inputs, to the outputs. There is also, accordingly, a notion of going backward. Once we have a notion of forward, backward is the other direction so we can also draw backward. So backward, we'll be going from the outputs to the inputs and this is something that's going to come up when you're actually doing inference, when you're actually doing learning in these models. It turns out you're going to do a lot of chain rule and the easiest way to do that chain rule is often by going backwards and so that's why we have something called back propagation because you're going backwards in this graph.

Now this particular network that we've shown here is what's known as a feed forward neural network because all of the lines go forwards. It is possible to have a network that is not a feed-forward network where some of the lines go backwards. So, for instance, here is such a network: this is just an example of a network that would not be a feed-forward network because one of the farther along outputs then becomes an input, it has a line that goes backwards, an arrow that goes backwards, and this is actually something that we're going to see later in this course with recurrent neural nets. But it's not something we're doing right now. The networks that we're looking at right now, like this network before I added this extra bit, are feed forward neural networks. So now we're back to a feed-forward neural network now that I've gotten rid of that weird backwards line.

Okay let's also label some of the things that are going on here. So we have inputs. You can actually have input standing layers. In this particular case, in this first layer, our inputs are going to be the data. We have essentially this dot product that's going on where we're adding in the the W naughts too but you can think of that as part of a dot product, perhaps with a feature that is equal to 1. You know, we've seen that interpretation before. We have this thing which is the output of doing the dot product and adding W naught but before we apply this function f^(1) and so we're going to call that the pre-activation, it's what goes into the function f^(1) and a reason for that name pre-activation is, in some sense, because the function f^(1) will be called the activation function and so it's before you apply the activation function, so it's the pre-activation. And then finally what we get when we apply the activation function is, perhaps not surprisingly at this point, the activation. Now why might this be called the activation? Well if you think about these step functions that we've been talking about that have a 0 and a 1, you can think of them as being sort of activated when you hit a 1 and not active when you hit a 0. So this is sort of telling us: did we activate this particular, what we're going to find out, is a node? But did we activate this particular feature?

Okay so now a neuron or sometimes a unit or a node is basically the sequence of inputs through the dot product and the activation function to a particular activation.

So not going through any other activations, just getting to that nearest activation and taking all the inputs that go into that node, that's going to be a neuron or a node. Then a layer... Oh here's just another neuron or node, we can have them in any layer. And now implicitly, we've been saying the word layer a lot, let's just say a layer is this collection of neurons here at a certain distance relative to the inputs or the outputs. So for instance, this is a particular layer, this is sort of the first one. It's out of all of these neurons or a particular distance relative to the inputs and then we'll see we have another layer as well. Okay and then finally we go through all of this stuff and we eventually get a set of final predictions or guesses.

And which is what we get from anything that we do, any hypothesis that we do. Whenever we put in a particular data point, we get a final prediction or guess out of that hypothesis for each possible data point value and so this is no different. The one thing that is a little bit different is that we could have more than one. That's not something that we've talked about too much before. I mean implicitly I guess we could have done it, but here we're explicitly including it in our notation as a possibility. Again, in this particular function graph, I've only shown one output but I could have more.

Okay this also happens to be what's known as a fully connected network. So in particular, we say that a layer is fully connected if all of the inputs to every neuron in the layer are the same. And in this particular case, that's true of all of the layers so the whole network is fully connected. Now one more note: we can actually name some of these layers. You could say we have this output layer. So that's the layer that eventually we get that final prediction, that “final” gets out of, the thing that we actually report at the end of the day. And the stuff before it, we don't see, we don't get a final prediction or a final output out of and so we might call those the “hidden.” Well in this case, it's one hidden layer. Spoiler alert: we can have more than one layer that is hidden and in that case we will have hidden layers.

But it's basically, it's the stuff that's not the output layer, it's the stuff that makes the features.

Okay so now we have multiple ways to think about neural networks with our original step function idea, with all of the notation that we developed with this function graph representation, and so let's think about how we would slot that into the way that we think about hypotheses and the way that we do learning in this course.

Okay so our typical problem setup, this is just what we would do for any learning problem so far, is we choose a hypothesis class. We've done that every time, that hypothesis class could have been the linear classifiers. This time, we might choose, instead, the neural networks. They're defined by the layers, by the number of units in each layer, the number of nodes in each layer, and by these weights, the W's and W naughts, and so to really nail down a hypothesis class, we'll have to say we're going to have this many hidden layers and this many outputs and we're going to have this many nodes in each layer and that'll tell us a hypothesis class.

Okay in this particular case, we chose two layers with, let's say, one output and some number of hidden units.

And then the next thing we have to do always is choose a loss. And so, for instance, if we are interested in classification, there's a number of losses that we've talked about: we've talked about 0-1 loss, we've talked about asymmetric loss, we've talked about negative log likelihood loss, there's a lot of choices. We just choose one of those.

Then we learn the parameters. We have some general purpose methods to do that now, like gradient descent and stochastic gradient descent, so we could hope to apply those, although we're going to see there's a [unintelligible] in just a second.

And then once we have learned some set of parameters, that is to say we've found some set of parameters that seems to get the loss pretty low, and we try to minimize the loss, then we can use those parameters to choose our particular hypothesis, we don't just have a whole class anymore, we have a particular one and we can use that to predict unknown data. Okay so this is our general setup, I mean, this is how we approach these problems in this class, this is not specific to neural nets, it's just if we choose this hypothesis class and we're using neural nets. But there are some problems with applying this general setup right now to what we're doing and so let's go into that before we can actually apply it, because we're going to have to resolve those problems. Okay so here's the first issue, the first problem that arises. If we want to use gradient descent or stochastic gradient descent, we can't really use these step function activation functions that we were talking about before. The reason for that is that their derivatives are 0 basically everywhere and so if I apply gradient descent or stochastic gradient descent, whatever gradient I calculate, it's just immediately going to be 0 and I'm not gonna move and that's it, that's done, I'm done with stochastic gradient descent, I'm done with gradient descent and that's boring and that's not telling me anything and it's not useful, right? I really want to use stochastic gradient descent and gradient descent to get to some good set of parameters and so that's not going to happen if I have these step function activations and so I'm really going to have to do something else if I want to apply gradient descent or stochastic gradient descent. Another issue with our current setup is: what if I want to do regression? Here so far, we've been talking about f^(2) as a step function: it returns 1 or it returns 0. That's not very helpful for regression. We really want some value that could be continuous, that could take a range of values, certainly not something that is only 0 or 1, and so we're going to have to resolve that. And another one is that, actually, even if I wanted to use negative log likelihood loss, this loss that we've talked about that can be really useful for classification, that's also a problem because the step function returns 0 or 1, and as many people have been having a really nice set of discussions in Discourse about, I just can't put in a straight 0 or 1 into that loss, I really want something that's between 0 and 1, something that I can interpret as a probability. And so I'm going to need to do something else if I want to use that loss in this type of problem.

Okay so let's think about what can we do to solve all of these issues that I want to use some kind of gradient descent or stochastic gradient descent, I want to do regression, I want to use negative log likelihood loss. Luckily, it turns out there is basically a single solution to all of these issues and that's to use a different activation function, that we are not beholden to step functions. We don't have to use step functions, they provide some nice intuition, they're useful for plotting things and understanding what a neural network does, but we can use things that are like step functions and still have the properties that we want. And so that's why we're now going to turn to choosing a different activation function than just a step function. Okay let's look at different activation functions. So our hypotheses look like this right now: we're looking at neural networks with two layers. In the first layer, we create the features and in the second layer we get the labels from the features. So our typical linear classification or linear regression is going on in the second layer.

Okay so let's ask first: what if I want to do regression? So the problem that we said was that if f^(2) was a step function, if it only returned 0 or 1, then I'm not doing anything really interesting for regression, like I really want something that could be generally real valued, that I could get all kinds of different values for. I mean that's sort of the point of regression, is to be able to have a range of values. Like if I'm trying to predict what is my air conditioning bill going to be, I don't think it's just going to be 0 or 1, I think it's going to take on all kinds of values and I want to predict that. So instead of a particular step function activation for f^(2), here I could have the identity. So if the inputs to f^(2) can take on a range of values, then now my outputs will be able to take on a range of values, so I'll be able to get an interesting set of values for my regression.

Okay what if I want to use negative log likelihood loss? I want my output of f^(2) to be between 0 and 1. Well if I use f^(2) equal to the identity, that's not going to happen.

If I use f^(2) equal to a step function, that's not going to happen, I'm going to get 0 or 1. But luckily, we have a great function that we know returns values between 0 and 1 and that's our sigma function, so we can use the sigmoid instead here for f^(2).

Okay our third question was what if I want to use gradient descent or stochastic gradient descent? I have this nice, general way of optimizing losses now. If I want to use those, well I need to be able to take derivatives and I need them to be useful. So you can take derivatives of the step function, it's just not super useful: it just gives you 0 everywhere. So what do I want to do here? I want to get something that is actually non-zero, that tells me where to go and so let's think about how we might do that.

Well first of all, we need the f^(2)s to be differentiable. Luckily each of the f^(2) values that we just said is fine for that. Z, the identity, is differentiable, the sigmoid is differentiable, so we're all set on the f^(2)s on the output activations. Okay, but we need the f^(1)s to be differentiable too. So we have to think about what f^(1)s would we choose. Now f^(2) and f^(1) are accomplishing different things in this network. When you look at the output layer, you're getting the labels, you're making the labels. And so with f^(2), we're thinking about “hey, what are the choices that we've made before to do regression or classification or get these output labels.” With f^(1), we're thinking about creating the features or thinking about what features would be useful to have and so with f^(1) we want to think, well, what kind of features would be useful but also have useful derivatives? And so there are actually multiple choices here because I mean, in some sense, features aren't as immediately obvious, like what do you have to have in a feature? We sort of know with regression and with classification what we're looking for in our output, but what do you have to have in a feature? So there's a few different options: so one option is to just choose the sigmoid again. You know, we know the sigmoid, we're familiar with the sigmoid, it has derivatives everywhere and they're generally useful: they tell us where to go and they can tell us things about the sigmoid. Another option, so you can think of the sigmoid as sort of a relaxed 0-1 function, there's something called a hyperbolic tangent which is a relaxed -1, 1 step function. So, if for any reason, you want something that's a relaxation of -1, 1 then that might be more convenient. So you might use the tangent instead. Another option is what's known as a rectified linear unit or ReLU. So this is a function that's the max of z or zero. It does have an interesting non-zero derivative on one side. When you're on that side, it's extremely easy to take that derivative because it's just the derivative of z, so that's super easy: it's 1, it's great. And what's kind of nice about this too, is that if you're doing something like regression, you can't really get like a really big range of values out, you can get something that's like a linear function and so you might find that useful for what you're doing. But ultimately, at least from our perspective right now, these are all perfectly fine functions that we could use to create features and that's what we're trying to do with them, we're just trying to create some features that we could use. Okay so let's just do a quick little look at how this changes our setup that we saw before when we use these different functions, like what's going to be different about what we're doing.

So before, we said we were going to use a step function activation in our first layer. So we make our features by using the step function activation, and this is what our features look like as a result: we had our linear boundary and then there was this sharp cut off at that boundary.

Okay and then in our second layer, we took those features and we turned them into a final prediction and so here what we're going to show is the full function that we get out from x all the way to that final prediction. I mean this is just what we saw before and, again, it's going to have these very sharp boundaries but it's only going to have values of 0, 1 because we're using a step function activation and now let's just ask: how would this change if we changed our activation function?

Okay well let's go up here and change our activation function to, for instance, the sigmoid. Let's use the sigmoid instead and you can think to yourself: how do you think that these step functions are going to change? You might just take a moment and think what do you think that these are going to look like when we change to the step function activation? And in general like the idea that we've seen before about the sigmoid is that it's much like a step function but sort of smoother and so here I'm just showing the exact same linear boundaries that we had before and you can just see when we apply the sigmoid activation function instead of this strict step function activation that things kind of smooth out, you know, it's a little bit gentler. We see that usually they're inline, there isn't a clear dividing line, things are just more or less likely and that's what this is expressing.

Now we can go to our second layer down here and think about our outputs, you know, how does this all translate into taking input x all the way out to a final output.

And so one option is we might care about regression, so if we care about regression, our output activation will just be the identity or it's certainly a very reasonable choice that we might make, and in that case, we're going to get a very different looking function. So one thing that's happening here is that we're actually seeing multiple plateaus that are happening because we can actually see what's happening when we're adding in these different almost step function features and so we're seeing that here and then, of course, everything's smoother because not only are we choosing z as our output now, but we had those smoother inputs to go in, we had those smoother features to go in. Now something you can see here is that when we're doing regression but we have these sigmoid features, that you still kind of get like these different layers, there's different, there's sort of a different levels that you can get. If you don't want that, you can get away from that with a ReLU. If you were using a ReLU for your features, you wouldn't necessarily have to have those levels that could be useful, it could not be useful, but by changing the weights, you really can't get any value here for your output. Okay now we could also ask what happens if we use our sigmoid for our final output? So we're just going to basically take what we have on the left here and smush it down with the sigmoid. So with the sigmoid, we're just taking all those different values that we get because, you know, essentially the left is the pre-activation because it was the identity and we're smooshing them down to be between 0 and 1. So this comes back to the question from earlier: is it that when these values are higher, when we see these higher values in our pre-activation, does that represent that maybe some class is more likely? In this particular case, that's made sort of very concrete and made very precise. Yes exactly, when we have that higher pre-activation value that gets smushed to a higher value when we apply the sigmoid and so we have a higher probability of a particular class.

Okay so at this point, we have this ability to choose all kinds of different features, we have this ability to have different types of outputs, regression style output or classification style output, and we can finally ask: how do we learn these parameters because we can apply these methods that we know from before? Okay so before we can apply stochastic gradient descent or gradient descent, we have to know what are we applying it to, what is the objective that we're going to try to minimize with stochastic gradient descent or gradient descent? And in some sense, what I'm writing here is just a completely generic objective: I'm just saying “hey, we said from the beginning that the notion of training error is one over the number of data points, a sum over the data points of the loss between our guess and our actual for each data point.” Our guess is the hypothesis applied to that data point and our actual is our actual label y^(i) when we're doing supervised learning like this.

Now in this particular case, our hypotheses we're thinking of as neural networks and so they're going to have these big W and W naught multiple matrices that we're trying to solve for and so those are going to be the parameters here. Now again, what's really interesting about this problem and very different from our problems before is that we have parameters not just for that final classification or regression step, but also for all the features that we've built along the way and so all of those are in here, all of those are in our hypothesis, the parameters for that final layer which is doing the classification of the regression, but also the parameters for all the hidden layers which are building up those features and so that's different from what we've done before but ultimately, at the end of the day, what we have is a function of all those parameters and it's a differentiable function and those derivatives are not just zero and so we can apply gradient descent or stochastic gradient descent. Now, the one slight thing that is untrue about that, is there is this one point in the ReLU that is not differentiable, but you basically almost never go there, I mean, or you essentially never go there if you're running around in stochastic gradient descent or gradient descent, you pretty much will never be at that point because it's just a single point and you can also even just define what happens at that point. Like maybe you define the gradient to be 0 there as well and so it's not as big of a concern as it might see on the face of it.

The bigger concern is that when you're at any other point, which you pretty much always are, that you have a non-zero derivative for most of those points and you do still get that with the ReLU that there's that whole side of it that's non-zero and with the other ones we see that you really do get a non-zero value for the derivative everywhere, for the tangent, for the sigmoid. Okay so let's just review what goes on with gradient descent and stochastic gradient descent before we apply them for our particular problem. So let's think about a different problem with some different parameters. Let's call them theta and we want to apply gradient descent.

And so with gradient descent, remember the idea of gradient descent is essentially: hey, I've got this objective function, it's over some parameters (in this particular case, let's say theta_1 and theta_2) and I would like to find the minimum and so I’ll start at some particular point, I'll calculate the gradient, I'll move in the direction of the gradient times eta, say, and I'll end up at a new point. Now here's the thing: so we have previously said that the gradient is a vector that's equal to the size of theta. So in this case, the gradient's a two-dimensional vector because my theta is two-dimensional, but it looks like I just plotted a vector in three dimensions. So can anybody tell me what's the third dimension here? Why is there a value in this up-down axis? So this is another one for the chat: what do you think is the third dimension? What is the value of the up-down axis, the not theta_1 or theta_2 axis?

Okay there's some answers that are on point and also I think some answers that are going the other way. So this is a good, I think this is a good thing for us to be discussing. So what we're seeing here, so what this vector that I'm plotting here is, it's a three-dimensional vector, it's eta times the gradient in the theta_1, theta_2 plane and then its value in the up-down direction is the actual objective function. So the starting point of that vector is the objective function at our initial point and the ending point of that vector is our objective function after we have moved in the direction of the gradient and so this vector isn't just the gradient and that's a really important thing to notice. If we were drawing just the gradient, it would just be in the theta_1, theta_2 plane. Here we're drawing the gradient with a little extra information. We're also showing where it came from and where it moved to in the objective. Great.

Okay so once we've done that, we've moved to a new point in the theta space and also implicitly in the J(theta) space, because we can evaluate the objective function of that particular theta and that's what we've done here in this gradient, and so we can do it again: we can say, “hey, let's take another gradient descent step, let's calculate the gradient, let's calculate where we move to in the theta space and then let's calculate the J(theta) at that point and then let's do it again and then let's do it again.” And there's a question about the magnitude of this vector and yes so what's happening is the size of the vector in the theta plane, the theta_1, theta_2, is the gradient times the step size and then the total magnitude of the vector in that direction is the magnitude of the gradient times the step size and then the magnitude or the difference in the J direction is the difference in the J values.

Okay so that's gradient descent and then we've also learned now about stochastic gradient descent and you can think of stochastic gradient descent as like drunk gradient descent. It's like you're taking steps but you don't have a lot of information and things are feeling pretty cloudy and so you start somewhere and you go somewhere off to the side because you only have the information from one data point, you don't have all the data points like if you actually looked at J, J is defined by all of the data points. You know, the optimization objective when it's this loss, is defined by all the data points and so with stochastic gradient descent you don't have access to this beautiful objective function that we're plotting here, you only have access to information from one data point. That one data point isn't very reliable so it just sends you off in all kinds of crazy directions and so who knows where you're going next, it's maybe over here whatever, and you just keep moving around like that. Now again, let's just make sure we review why would we ever do this, why wouldn't we just go with gradient descent when it has all this great information at every step and again the difference is that the cost of one of these steps is just a huge difference. So to make even one gradient descent step, we have to wait to go through absolutely every single data point, so if we have a ton of data points, that's going to take forever, whereas with stochastic gradient descent, yeah sure it's like a drunken meandering step, but it took us so little time and by the time we've taken a lot of these steps that are just kind of in the right direction, we'll probably, in many cases, especially with a lot of data, be doing better than if we had waited that whole time just to take one step.

Okay so we have these theorems from our previous, you know, talking about gradient descent and stochastic gradient descent that I'm just going to state very roughly and in very plain English that roughly, say, okay, if our objective is nice and convex, then gradient descent, stochastic gradient descent should perform well, in the sense that if you run them long enough they should get near the unique global optimum when that unique global optimum exists, that unique global minimum in the case where we're minimizing things, we're minimizing the objective.

Okay so this is the challenge of neural nets, this is why we're spending a few weeks on them. You know, we're gonna have a couple of weeks with lots of different explorations and if you are just using them in practice, this is something that you should absolutely be aware of, this is why there are so many different parts to this chapter, there are so many different things to read about, because the neural net objective is not just not convex, it's like super not convex, it's got so many different local optimum. It is something that, to this day, especially in the kinds of neural nets that people are really looking at and trying to use in practice, people spend tons of time trying to understand what these objectives even look like. In this particular cartoon, and again it's always worth reiterating and reminding ourselves what we're doing with cartoons, in this cartoon, our parameter space is two-dimensional. Even for the simple two-dimensional neural net, or sorry, two-layer neural net that we've talked about today, with maybe just a couple of hidden units or three hidden units, we're in so much higher dimensions than two already for our parameter space. Our parameter space is going to be hugely high dimensional in real life, it's going to get so big and so it's so hard to visualize what's going on and to get a sense of what's going on, especially when we as humans can only see in two dimensions, we can only really understand what's going on in two dimensions and so there's a ton of research that goes in just trying to understand these landscapes and to try and understand what to do with these landscapes and, in particular, a lot of that's going to mean that we're going to have to do a lot of special things to optimize and to regularize. So in general, we just can't expect, because this is so non-convex, that we're going to be getting to a global optimum when we optimize and so what can we do about that? You're going to see in the reading, you've probably already seen in the reading, that there's a huge bag of tricks to deal with this and, again, I can't emphasize enough this is a area of active research to think about what are good ways to do optimization in neural nets and a really tricky thing about this problem is that it's not clear that you actually want the global optimum and this relates to regularization. Now we've talked in the past about these polynomial bases with just a few different basis functions, these polynomial features as being super complex and that you might overfit and so it's probably not going to be a surprise that if we have these super expressive many different step function like features that it seems like we could easily overfit, especially if we just keep adding more and more of them and so we have these conflicting things that we want to do: we want to minimize the loss, we want to do really well in terms of loss, but we also maybe don't want to do it too much, we want to regularize somehow, we don't want to necessarily overfit to all of that data that we have and so this is, again, just a huge challenge of neural nets and something that's very different from what we have looked at before in this class in terms of linear classification, linear regression where everything was nice and convex so you could have all these nice guarantees about how you're going to perform. Now there's so much more work that has to go into optimizing and understanding what's the right way to regularize given that you don't necessarily want to perfectly optimize this objective and so these are the kind of trade-offs that you see in this bag of tricks and partly I also want to emphasize that you see all of these awesome ideas that people have come up with, actually pretty recently in some cases in the reading, and that's not the end of the story because this is such a major area that people are working in right now. New ideas are getting developed pretty recently and so it might well be the case that by the time in a few years we're using this in some other application, that there might be new tricks to be using, there might be even better things to be using, and so I think that's why you really want to take away this very high level idea about what's going on with neural nets and what's going on with their optimization and learning in neural nets, because any particular method might become obsolete in a few years, it's hard to know, and so you want to be just aware of the challenges and what these are trying to address. Okay, so that all being said, I just want to spend a few moments at the end saying we don't have to just have two layers. So we've talked this whole time about having two layers in our neural nets, the one where we create the features, and the one where we do the regression or the classification, but why stop there? And we're gonna get into this a lot more when we talk about convolutional neural nets and there they definitely don't stop at two layers, but let's just think about why we would want to have more than two layers. So here this is just another representation as a function graph. So here, I'm taking the function that takes the x's, takes all the W's, and turns it just into the output at any particular layer. So I've sort of compressed a few of the functions into one function from what we saw earlier, but that's fine. I mean a function graph, the circle, is just a function, it can be whatever function I want and here there's actually three layers to this net. We first come up with a set of A^(1)’s, then we come up with A^(2)’s, and then finally some A^(3)’s.

And so that final layer, in general, is where we do the linear classification or regression. That was true in the two layer net and that's also true in our three layer net here and in higher layer nets and the rest of this is where we're making features. Now before, we just made features out of our data but something we could do is recursively make features: we could make features and then we can make features out of those features and that's what you can do when you have multiple hidden layers and again we're going to see a really concrete example of this when we get into convolutional neural nets.

Now a corollary of this observation is that, in general, when I'm adding or subtracting layers, what I'm doing is I'm taking it from these hidden layers, these layers that are making the features, and so if I subtracted enough layers that I only got down to one layer, all that would be left in my neural net is that linear classification and regression layer at the end and so just a single layer of a neural net with just one output is linear classification and regression again with the default features, it's just what we've been seeing this whole time. You can think of it as a special case of this broader idea now. Okay great, so that's neural nets. We have defined neural nets, we've seen some examples of feed forward neural nets, fully connected neural nets. You're gonna be exploring neural nets and using them and getting into all these issues of, you know, how do we actually optimize them in your labs and homeworks and everything like that. There is no live lecture next week, although there is a pre-recorded lecture from previous years so you can check that out if you're interested. I will note, however, that the week after that, we'll be back and again we'll be talking about more layers, basically what happens with more layers: why might we use more layers, how does that help us out? It turns out to be pretty important so I'll catch you then. Thanks.
